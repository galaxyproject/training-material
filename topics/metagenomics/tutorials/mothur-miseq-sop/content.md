{% if include.short %}
  {% assign other_tutorial = "../mothur-miseq-sop/tutorial.html" %}
  {% assign other_tutorial_name = "extended" %}
{% else %}
  {% assign other_tutorial = "../mothur-miseq-sop-short/tutorial.html" %}
  {% assign other_tutorial_name = "short"%}
{% endif %}


# Overview
{:.no_toc}

In this tutorial we will perform an analysis based on the
[Standard Operating Procedure (SOP) for MiSeq data](https://www.mothur.org/wiki/MiSeq_SOP), developed by the [Schloss lab](http://www.schlosslab.org/), the creators of the mothur software package {% cite Schloss2009 %}.


> ### {% icon comment %} Note: Two versions of this tutorial
>
> Because this tutorial consists of many steps, we have made two versions of it, one long and one short.
>
> {% if include.short %}
> This is the **shortened version**. Instead of running each tool individually, we will employ
> workflows to run groups of analysis steps (e.g. data cleaning) at once. If you would like
> more in-depth discussion of each step, please see the [longer version of tutorial]({% link topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.md %})
> {% else %}
> This is the **extended version**. We will run every tool manually and discuss the results in detail.
> If you would like to run through the tutorial a bit quicker and use workflows to run groups of
> analysis steps (e.g. data cleaning) at once, please see the [shorter version of this tutorial]({% link topics/metagenomics/tutorials/mothur-miseq-sop-short/tutorial.md %})
> {% endif %}
> You can also **switch** between the long and short version at the start of any section.
{: .comment}


> ### Agenda
>
> In this tutorial, we will cover:
>
> 1. TOC
> {:toc}
>
{: .agenda}


{% snippet faqs/galaxy/analysis_results_may_vary.md %}

# Obtaining and preparing data

In this tutorial we use 16S rRNA data, but similar pipelines can be used for WGS data.

> ### {% icon comment %} Background: The 16S ribosomal RNA gene
> ![The 16S ribosomal RNA gene](../../images/16S_gene.png) <br><br>
>
> The 16S rRNA gene has several properties that make it ideally suited for our purposes
>
> 1. Present in all prokaryotes
> 2. Highly conserved + highly variable regions
> 3. Huge reference databases
>
> ![16S Variable regions](../../images/16S_variableregions.jpg)
>
> The highly conserved regions make it easy to target the gene across different organisms,
> while the highly variable regions allow us to distinguish between different species.
>
> (slide credit [https://www.slideshare.net/beiko/ccbc-tutorial-beiko](https://www.slideshare.net/beiko/ccbc-tutorial-beiko ))
{: .comment}

## Understanding our input data

In this tutorial we use the [dataset generated by the Schloss lab](https://mothur.org/wiki/Analysis_examples) to illustrate
[their MiSeq SOP](https://www.mothur.org/wiki/MiSeq_SOP).

They [describe the experiment](https://www.mothur.org/wiki/MiSeq_SOP#Logistics) as follows:

*"The Schloss lab is interested in understanding the effect of normal variation in the gut microbiome on host health. To that end,
we collected fresh feces from mice on a daily basis for 365 days post weaning. During the first 150 days post weaning (dpw),
nothing was done to our mice except allow them to eat, get fat, and be merry. We were curious whether the rapid change in
weight observed during the first 10 dpw affected the stability microbiome compared to the microbiome observed between days
140 and 150."*

![Experiment setup](../../images/experiment_setup.png)

To speed up analysis for this tutorial, we will use only a subset of this data. We will look at a single mouse at 10 different
time points (5 early, 5 late). In order to assess the error rate of the analysis pipeline and experimental setup, the Schloss lab
additionally sequenced a mock community with a known composition (genomic DNA from 21 bacterial strains). The sequences used
for this mock sample are contained in the file `HMP_MOCK.v35.fasta`

> ### {% icon comment %} Dataset naming scheme
> For this tutorial, you are given 10 pairs of files. For example, the following pair of files:<br />
>  `F3D0_S188_L001_R1_001.fastq`<br />
>  `F3D0_S188_L001_R2_001.fastq`
>
> The first part of the file name indicates the sample; `F3D0` here signifies that this sample was obtained from Female 3 on Day 0.
> The rest of the file name is identical, except for `_R1` and `_R2`, this is used to indicate the forward and reverse reads
> respectively.
{: .comment}

<!-- note: mothur does not include day 4 in their SOP example data, therefore this description and results
in this document differ slightly from the description on their website -->


## Importing the data into Galaxy

Now that we know what our input data is, let's get it into our Galaxy history:

All data required for this tutorial has been made available from Zenodo [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.800651.svg)](https://doi.org/10.5281/zenodo.800651)

> ### {% icon hands_on %} Hands-on: Obtaining our data
>
> 1. Make sure you have an empty analysis history. Give it a name.
>
>    {% snippet faqs/galaxy/histories_create_new.md %}
>
> 2. **Import Sample Data.**
>    - Import the sample FASTQ files to your history, either from a shared data library (if available), or from Zenodo
>      using the URLs listed in the box below (click {% icon param-repeat %} to expand):
>
>      > ### {% icon solution %} List of Zenodo URLs
>      > ```
>      > https://zenodo.org/record/800651/files/F3D0_R1.fastq
>      > https://zenodo.org/record/800651/files/F3D0_R2.fastq
>      > https://zenodo.org/record/800651/files/F3D141_R1.fastq
>      > https://zenodo.org/record/800651/files/F3D141_R2.fastq
>      > https://zenodo.org/record/800651/files/F3D142_R1.fastq
>      > https://zenodo.org/record/800651/files/F3D142_R2.fastq
>      > https://zenodo.org/record/800651/files/F3D143_R1.fastq
>      > https://zenodo.org/record/800651/files/F3D143_R2.fastq
>      > https://zenodo.org/record/800651/files/F3D144_R1.fastq
>      > https://zenodo.org/record/800651/files/F3D144_R2.fastq
>      > https://zenodo.org/record/800651/files/F3D145_R1.fastq
>      > https://zenodo.org/record/800651/files/F3D145_R2.fastq
>      > https://zenodo.org/record/800651/files/F3D146_R1.fastq
>      > https://zenodo.org/record/800651/files/F3D146_R2.fastq
>      > https://zenodo.org/record/800651/files/F3D147_R1.fastq
>      > https://zenodo.org/record/800651/files/F3D147_R2.fastq
>      > https://zenodo.org/record/800651/files/F3D148_R1.fastq
>      > https://zenodo.org/record/800651/files/F3D148_R2.fastq
>      > https://zenodo.org/record/800651/files/F3D149_R1.fastq
>      > https://zenodo.org/record/800651/files/F3D149_R2.fastq
>      > https://zenodo.org/record/800651/files/F3D150_R1.fastq
>      > https://zenodo.org/record/800651/files/F3D150_R2.fastq
>      > https://zenodo.org/record/800651/files/F3D1_R1.fastq
>      > https://zenodo.org/record/800651/files/F3D1_R2.fastq
>      > https://zenodo.org/record/800651/files/F3D2_R1.fastq
>      > https://zenodo.org/record/800651/files/F3D2_R2.fastq
>      > https://zenodo.org/record/800651/files/F3D3_R1.fastq
>      > https://zenodo.org/record/800651/files/F3D3_R2.fastq
>      > https://zenodo.org/record/800651/files/F3D5_R1.fastq
>      > https://zenodo.org/record/800651/files/F3D5_R2.fastq
>      > https://zenodo.org/record/800651/files/F3D6_R1.fastq
>      > https://zenodo.org/record/800651/files/F3D6_R2.fastq
>      > https://zenodo.org/record/800651/files/F3D7_R1.fastq
>      > https://zenodo.org/record/800651/files/F3D7_R2.fastq
>      > https://zenodo.org/record/800651/files/F3D8_R1.fastq
>      > https://zenodo.org/record/800651/files/F3D8_R2.fastq
>      > https://zenodo.org/record/800651/files/F3D9_R1.fastq
>      > https://zenodo.org/record/800651/files/F3D9_R2.fastq
>      > https://zenodo.org/record/800651/files/Mock_R1.fastq
>      > https://zenodo.org/record/800651/files/Mock_R2.fastq
>      > ```
>      {: .solution }
>
>      {% snippet faqs/galaxy/datasets_import_via_link.md %}
>
>      {% snippet faqs/galaxy/datasets_import_from_data_library.md %}
>
> 3. **Import Reference Data**
>    - Import the following reference datasets
>      - `silva.v4.fasta`
>      - `HMP_MOCK.v35.fasta`
>      - `trainset9_032012.pds.fasta`
>      - `trainset9_032012.pds.tax`
>
>
>    > ### {% icon solution %} List of Zenodo URLs
>    > ```
>    > https://zenodo.org/record/800651/files/HMP_MOCK.v35.fasta
>    > https://zenodo.org/record/800651/files/silva.v4.fasta
>    > https://zenodo.org/record/800651/files/trainset9_032012.pds.fasta
>    > https://zenodo.org/record/800651/files/trainset9_032012.pds.tax
>    > https://zenodo.org/record/800651/files/mouse.dpw.metadata
>    > ```
>    {: .solution }
{: .hands_on}

Now that's a lot of files to manage. Luckily Galaxy can make life a bit easier by allowing us to create
**dataset collections**. This enables us to easily run tools on multiple datasets at once.

Since we have **paired-end** data, each sample consist of two separate fastq files, one containing the
forward reads, and one containing the reverse reads. We can recognize the pairing from the file names,
which will differ only by `_R1` or `_R2` in the filename. We can tell Galaxy about this paired naming
convention, so that our tools will know which files belong together. We do this by building a **List of Dataset Pairs**


> ### {% icon hands_on %} Hands-on: Organizing our data into a paired collection
>
> 1. Click on the **checkmark icon** {% icon param-check %} at top of your history.
>
> 2. Select all the FASTQ files (40 in total)
>    - **Tip:** type `fastq` in the search bar at the top of your history to filter only the FASTQ files; you can now use the `All` button at the top instead of having to individually select all 40 input files.
>    - Click on **for all selected..**
>    - Select **Build List of Dataset Pairs** from the dropdown menu
>
>    In the next dialog window you can create the list of pairs. By default Galaxy will look for pairs
>    of files that differ only by a `_1` and `_2` part in their names. In our case however, these
>    should be `_R1` and `_R2`.
>
> 3. Change these values accordingly
>    - Change `_1` to `_R1` in the text field on the top left
>    - Change `_2` to `_R2` om the text field on the top right
>
>    You should now see a list of pairs suggested by Galaxy:
>    ![List of suggested paired datasets](../../images/create_collection.png)
>
> 4. Click on **Auto-pair** to create the suggested pairs.
>   - Or click on "Pair these datasets" manually for every pair that looks correct.
>
> 5. **Name the pairs**
>    - The middle segment is the name for each pair.
>    - These names will be used as sample names in the downstream analysis, so always make sure they are informative!
>    - Make sure that {% icon param-check %} `Remove file extensions` is checked
>    - **Check** that the pairs are named `F3D0`-`F3D9`, `F3D141`-`F3D150` and `Mock`.
>      - Note: The names should **not** have the .fastq extension
>      - If needed, the names can be edited manually by clicking on them
>
>    ![The result of pairing](../../images/create_collection2.png)
>
> 6. **Name your collection** at the bottom right of the screen
>   - You can pick whatever name makes sense to you
> 7. Click the **Create Collection** button.
>    - A new dataset collection item will now appear in your history
{: .hands_on}


# Quality Control
{% include topics/metagenomics/tutorials/mothur-miseq-sop/switch_tutorial.md section="quality-control" %}

For more information on the topic of quality control, please see our training materials
[here]({% link topics/sequence-analysis/index.md %}).

Before starting any analysis, it is always a good idea to assess the quality of your input data and improve it
where possible by trimming and filtering reads. The mothur toolsuite contains several tools to assist with this task.
We will begin by merging our reads into contigs, followed by filtering and trimming of reads based on quality score
and several other metrics.

## Create contigs from paired-end reads

In this experiment, paired-end sequencing of the ~253 bp V4 region of the 16S rRNA gene was performed.
The sequencing was done from either end of each fragment. Because the reads are about 250 bp in length, this results in a
significant overlap between the forward and reverse reads in each pair. We will combine these pairs of reads into *contigs*.

![Merging into contigs](../../images/16S_merge_contigs.png)

The **Make.contigs** tool creates the contigs, and uses the paired collection as input. **Make.contigs**
will look at each pair, take the reverse complement reverse read, and then determine the overlap between the
two sequences. Where an overlapping base call differs between the two reads, the quality score is used to determine
the consensus base call. A new quality score is derived by combining the two original quality scores in both of
the reads for all the overlapping positions.


> ### {% icon hands_on %} Hands-on: Combine forward and reverse reads into contigs
>
> -  {% tool [Make.contigs](toolshed.g2.bx.psu.edu/repos/iuc/mothur_make_contigs/mothur_make_contigs/1.39.5.1) %} with the following parameters
>   - {% icon param-select %} *"Way to provide files"*: `Multiple pairs - Combo mode`
>   - {% icon param-collection %} *"Fastq pairs"*: the collection you just created
>   - Leave all other parameters to the default settings
>
{: .hands_on}


This step combined the forward and reverse reads for each sample, and also combined
the resulting contigs from all samples into a single file. So we have gone from a paired
collection of 20x2 FASTQ files, to a single FASTA file. In order to retain information about
which reads originated from which samples, the tool also output a *group file*. View that
file now, it should look something like this:

```
M00967_43_000000000-A3JHG_1_1101_10011_3881     F3D0
M00967_43_000000000-A3JHG_1_1101_10050_15564    F3D0
M00967_43_000000000-A3JHG_1_1101_10051_26098    F3D0
[..]
```

Here the first column contains the read name, and the second column contains the sample name.


## Data Cleaning

{% if include.short %}

Next, we want to improve the quality of our data. To this end we will run a workflow that performs the following steps:

1. **Filter by length** \\
   We know that the V4 region of the 16S gene is around 250 bp long. Anything significantly longer
   was likely a poorly assembled contig. We will remove any contigs longer than 275 base pairs using the **Screen.seqs** {% icon tool %} tool.
2. **Remove low quality contigs** \\
   We will also remove any contigs containing too many ambiguous base calls.
3. **Deduplicate sequences** \\
   Since we are sequencing many of the same organisms, there will likely be many identical contigs. To speed up downstream analysis we will determine the set of unique contigs using **Unique.seqs** {% icon tool %}.


> ### {% icon hands_on %} Hands-on: Perform data cleaning
>
> 1. **Import the workflow** into Galaxy
>    - Copy the URL (e.g. via right-click) of [this workflow]({{ site.baseurl }}{{ page.dir }}workflows/workflow1_quality_control.ga) or download it to your computer.
>    - Import the workflow into Galaxy
>
>    {% snippet faqs/galaxy/workflows_import.md %}
>
> 2. Run **Workflow 1: Quality Control** {% icon workflow %} using the following parameters:
>    - *"Send results to a new history"*: `No`
>    - {% icon param-file %} *"1: Contigs"*: the `trim.contigs.fasta` output from **Make.contigs** {% icon tool %}
>    - {% icon param-file %} *"2: Groups"*: the `group file` from **Make.contigs** {% icon tool%}
>
>    {% snippet faqs/galaxy/workflows_run.md %}
>
> > ### {% icon question %} Questions
> >
> > 1. How many sequences were removed in the screening step?
> > 2. How many *unique* sequences are there in our cleaned dataset?
> >
> > > ### {% icon solution %} Solutions
> > > 1. The screening removed **23,488** sequences.
> > >
> > >    This can be determined by looking at the number of lines in `bad.accnos` output of **Screen.seqs** {% icon tool %}
> > >    or by comparing the total number of sequences before and after this screening step in the output of **Summary.seqs** {% icon tool %}
> > >
> > > 2. There are **16,426** unique sequences.
> > >
> > >    This can be determined by expanding one of the outputs of **Unique.seqs** {% icon tool %} and looking at the number of lines in the file.
> > {: .solution }
> {: .question}
>
{: .hands_on}

{% else %}
As the next step, we want to improve the quality of our data. But first, let's get a feel of our dataset:

> ### {% icon hands_on %} Hands-on: Summarize data
>
> - {% tool [Summary.seqs](toolshed.g2.bx.psu.edu/repos/iuc/mothur_summary_seqs/mothur_summary_seqs/1.39.5.0) %} with the following parameters
>   - {% icon param-file %} *"fasta"*: the `trim.contigs.fasta` file created by **Make.contigs** {% icon tool%}
>   - *"Output logfile?"*: `yes`
>
{: .hands_on}

The `summary` output files give information per read. The `logfile` outputs also contain some summary
statistics:

```
             Start    End        NBases     Ambigs   Polymer  NumSeqs
Minimum:     1        248        248        0        3        1
2.5%-tile:   1        252        252        0        3        3810
25%-tile:    1        252        252        0        4        38091
Median:      1        252        252        0        4        76181
75%-tile:    1        253        253        0        5        114271
97.5%-tile:  1        253        253        6        6        148552
Maximum:     1        502        502        249      243      152360
Mean:        1        252.811    252.811    0.70063  4.44854
 # of Seqs:   152360
```

In this dataset:

* Almost all of the reads are between 248 and 253 bases long.
* 2,5% or more of our reads had ambiguous base calls (`Ambigs` column).
* The longest read in the dataset is 502 bases.
* There are 152,360 sequences.

Our region of interest, the V4 region of the 16S gene, is only around 250 bases long. Any reads significantly longer than this expected value likely did not
assemble well in the **Make.contigs** step. Furthermore, we see that 2,5% of our reads had between 6 and 249
ambiguous base calls (`Ambigs` column). In the next steps we will clean up our data by removing these
problematic reads.

We do this data cleaning using the `Screen.seqs` tool, which removes

1. sequences with ambiguous bases (`maxambig`) and
2. contigs longer than a given threshold (`maxlength`).

> ### {% icon hands_on %} Hands-on: Filter reads based on quality and length
>
> - {% tool [Screen.seqs](toolshed.g2.bx.psu.edu/repos/iuc/mothur_screen_seqs/mothur_screen_seqs/1.39.5.1) %} with the following parameters
>   - {% icon param-file %} *"fasta"*: the `trim.contigs.fasta` file created by **Make.contigs** {% icon tool %}
>   - {% icon param-file %} *"group"*: the group file created in the **Make.contigs** {% icon tool %} step
>   - *"maxlength"*: `275`
>   - *"maxambig"*: `0`
>
> > ### {% icon question %} Question
> >
> > How many reads were removed in this screening step? (Hint: run the summary.seqs tool again)
> >
> > > ### {% icon solution %} Solution
> > > 23,488.
> > >
> > > This can be determined by looking at the number of lines in bad.accnos output of screen.seqs
> > > or by comparing the total number of seqs between of the summary log before and after this screening
> > > step
> > {: .solution }
> {: .question}
{: .hands_on}


## Optimize files for computation

Microbiome samples typically contain a large numbers of the same organism, and therefore we expect
to find many identical sequences in our data. In order to speed up computation, we first determine
the unique reads, and then record how many times each of these different reads was observed in
the original dataset. We do this by using the **Unique.seqs** tool.


> ### {% icon hands_on %} Hands-on: Remove duplicate sequences
>
> - {% tool [Unique.seqs](toolshed.g2.bx.psu.edu/repos/iuc/mothur_unique_seqs/mothur_unique_seqs/1.39.5.0) %} with the following parameters
>   - {% icon param-file %} *"fasta"*: the `good.fasta` output from **Screen.seqs** {% icon tool %}
>   - *"output format"*: `Name File`
>
> > ### {% icon question %} Question
> >
> > How many sequences were unique? How many duplicates were removed?
> >
> > > ### {% icon solution %} Solution
> > > 16,426 unique sequences and 112,446 duplicates.
> > >
> > > This can be determined from the number of lines in the fasta (or names) output, compared to the
> > > number of lines in the fasta file before this step.
> > {: .solution}
> {: .question}
{: .hands_on}

Here we see that this step has greatly reduced the size of our sequence file; not only will this speed up further computational
steps, it will also greatly reduce the amount of disk space (and your Galaxy quota) needed to store all the intermediate files generated during
this analysis. This **Unique.seqs** tool created two files, one is a FASTA file containing only the unique sequences,
and the second is a so-called *names file*. This names file consists of two columns, the first contains the sequence names
for each of the unique sequences, and the second column contains all other sequence names that are identical to the representative
sequence in the first column.

```
name          representatives
read_name1    read_name2,read_name,read_name5,read_name11
read_name4    read_name6,read_name,read_name10
read_name7    read_name8
...
```

To recap, we now have the following files:

- a FASTA file containing every distinct sequence in our dataset (the *representative* sequences)
- a *names file* containing the list of duplicate sequences
- a *group file* containing information about the samples each read originated from


To further reduce file sizes and streamline analysis, we can use the **Count.seqs** tool to combine
the *group file* and the *names file* into a single *count table*.


> ### {% icon hands_on %} Hands-on: Generate count table
>
> - {% tool [Count.seqs](toolshed.g2.bx.psu.edu/repos/iuc/mothur_count_seqs/mothur_count_seqs/1.39.5.0) %} with the following parameters
>   - {% icon param-file %} *"name"*: the `names` output from **Unique.seqs** {% icon tool %}
>   - *"Use a Group file"*: `yes`
>   - {% icon param-file %} *"group"*: the `group file` we created using the **Screen.seqs** {% icon tool %}
{: .hands_on}

{% endif %}

Have a look at the *count_table* output from the **Count.seqs** {% icon tool %}, it summarizes the number of times each unique sequence was observed across each of the samples. It will look something like this:

```
Representative_Sequence                      total   F3D0   F3D1  F3D141  F3D142  ...
M00967_43_000000000-A3JHG_1_1101_14069_1827  4402    370    29    257     142
M00967_43_000000000-A3JHG_1_1101_18044_1900  28      1      0     1       0
M00967_43_000000000-A3JHG_1_1101_13234_1983  10522   425    281   340     205
...
```

The first column contains the read names of the representative sequences, and the subsequent columns contain
the number of duplicates of this sequence observed in each sample.

> ### {% icon comment %} Representative sequences vs Total sequences
> From now on, we will only work with the set of *unique sequences*, but it's important to remember that these represent a larger
> number of *total sequences*, which we keep track of in the *count table*.
>
> The **Summary.seqs** {% icon tool %} tool will
> report both the number of unique *representative sequences* as well as the *total sequences* they represent.
{: .comment}



# Sequence Alignment
{% include topics/metagenomics/tutorials/mothur-miseq-sop/switch_tutorial.md section="sequence-alignment" %}

For more information on the topic of alignment, please see our training materials
[here]({% link topics/sequence-analysis/index.md %})

We are now ready to align our sequences to the reference. This is an important
step to improve the clustering of your OTUs {% cite Schloss2012 %}.


> ### {% icon hands_on %} Hands-on: Align sequences
>
> 1. {% tool [Align.seqs](toolshed.g2.bx.psu.edu/repos/iuc/mothur_align_seqs/mothur_align_seqs/1.39.5.0) %} with the following parameters
>   - {% icon param-file %} *"fasta"*: the `fasta` output from **Unique.seqs** {% icon tool %}
>   - {% icon param-file %} *"reference"*: `silva.v4.fasta` reference file from your history
> <br><br>
>
>     > ### {% icon question %} Question
>     >
>     > Have a look at the alignment output, what do you see?
>     >
>     > > ### {% icon solution %} Solution
>     > > At first glance, it might look like there is not much information there. We see our read names, but only period `.` characters below it.
>     > > ```
>     > > >M00967_43_000000000-A3JHG_1_1101_14069_1827
>     > > ............................................................................
>     > > >M00967_43_000000000-A3JHG_1_1101_18044_1900
>     > > ............................................................................
>     > > ```
>     > > This is because the V4 region is located further down our reference database and nothing aligns to the start of it. If you scroll to right you will start seeing some more informative bits:
>     > > ```
>     > > .....T-------AC---GG-AG-GAT------------
>     > > ```
>     > > Here we start seeing how our sequences align to the reference database.
>     > > There are different alignment characters in this output:
>     > >   - `.`: terminal gap character (before the first or after the last base in our query sequence)
>     > >   - `-`: gap character within the query sequence
>     > >
>     > > We will cut out only the V4 region in a later step (**Filter.seqs** {% icon tool %})
>     > {: .solution }
>     {: .question}
>
> 2. {% tool [Summary.seqs](toolshed.g2.bx.psu.edu/repos/iuc/mothur_summary_seqs/mothur_summary_seqs/1.39.5.0) %} with the following parameters:
>   - {% icon param-file %} *"fasta"*: the `align` output from **Align.seqs** {% icon tool %}
>   - {% icon param-file %} *"count"*: `count_table` output from **Count.seqs** {% icon tool %}
>   - *"Output logfile?"*: `yes`
>
{: .hands_on}

Have a look at the summary output (log file):

```
            Start    End      NBases  Ambigs   Polymer  NumSeqs
Minimum:    1250     10693    250     0        3        1
2.5%-tile:  1968     11550    252     0        3        3222
25%-tile:   1968     11550    252     0        4        32219
Median:     1968     11550    252     0        4        64437
75%-tile:   1968     11550    253     0        5        96655
97.5%-tile: 1968     11550    253     0        6        125651
Maximum:    1982     13400    270     0        12       128872
Mean:       1967.99  11550    252.462 0        4.36693
# of unique seqs:   16426
total # of seqs:    128872
```


The `Start` and `End` columns tell us that the majority of reads aligned between positions 1968 and 11550,
which is what we expect to find given the reference file we used. However, some reads align to very different positions,
which could indicate insertions or deletions at the terminal ends of the alignments or other complicating factors.

Also notice the `Polymer` column in the output table. This indicates the average homopolymer length. Since we know that
our reference database does not contain any homopolymer stretches longer than 8 reads, any reads containing such
long stretches are likely the result of PCR errors and we would be wise to remove them.

Next we will clean our data further by removing poorly aligned sequences and any sequences with long
homopolymer stretches.


## More Data Cleaning

To ensure that all our reads overlap our region of interest, we will:

1. Remove any reads not overlapping the region V4 region {% unless include.short %}(position 1968 to 11550){% endunless %} using **Screen.seqs** {% icon tool %}.
2. Remove any overhang on either end of the V4 region to ensure our sequences overlap *only* the V4 region, using **Filter.seqs** {% icon tool %}.
3. Clean our alignment file by removing any columns that have a gap character (`-`, or `.` for terminal gaps) at that position in every sequence (also using **Filter.seqs** {% icon tool %}).
4. Group near-identical sequences together with **Pre.cluster** {% icon tool %}. Sequences that only differ by one or two bases at this point are likely to represent sequencing errors rather than true biological variation, so we will cluster such sequences together.
{% if include.short %} 5. Remove Sequencing artefacts known as *chimeras* (discussed in next section). {% endif %}

{% unless include.short %}

> ### {% icon hands_on %} Hands-on: Remove poorly aligned sequences
>
> 1. {% tool [Screen.seqs](toolshed.g2.bx.psu.edu/repos/iuc/mothur_screen_seqs/mothur_screen_seqs/1.39.5.1) %} with the following parameters
>   - {% icon param-file %} *"fasta"*: the aligned fasta file from **Align.seqs** {% icon tool %}
>   - *"start"*: `1968`
>   - *"end"*: `11550`
>   - *"maxhomop"*: `8`
>   - {% icon param-file %} *"count"*: the `count table` file from **Count.seqs** {% icon tool %}
>
>     **Note:** we supply the count table so that it can be updated for the sequences we're removing.
>
>     > ### {% icon question %} Question
>     >
>     >  How many sequences were removed in this step?
>     > > ### {% icon solution %} Solution
>     > > 128 sequences were removed. This is the number of lines in the bad.accnos output.
>     > {: .solution }
>     {: .question}
>
>     Next, we will remove any overhang on either side of the V4 region, and
>
> 2. {% tool [Filter.seqs](toolshed.g2.bx.psu.edu/repos/iuc/mothur_filter_seqs/mothur_filter_seqs/1.39.5.0) %} with the following parameters
>   - {% icon param-file %} *"fasta"*: `good.fasta` output from the latest **Screen.seqs** {% icon tool %}
>   - *"vertical"*: `yes`
>   - *"trump"*: `.`
>   - *"Output logfile"*: `yes`
>
{: .hands_on}


Your resulting alignment (`filtered fasta` output) should look something like this:

```
>M00967_43_000000000-A3JHG_1_1101_14069_1827
TAC--GG-AG-GAT--GCG-A-G-C-G-T-T--AT-C-CGG-AT--TT-A-T-T--GG-GT--TT-A-AA-GG-GT-GC-G-TA-GGC-G-G-C-CT-G-C-C-AA-G-T-C-A-G-C-G-G--TA-A-AA-TT-G-C-GG-GG--CT-C-AA-C-C-C-C-G-T-A--CA-G-C-CGTT-GAAAC-TG-C-CGGGC-TCGA-GT-GG-GC-GA-G-A---AG-T-A-TGCGGAATGCGTGGTGT-AGCGGT-GAAATGCATAG-AT-A-TC-AC-GC-AG-AACCCCGAT-TGCGAAGGCA------GC-ATA-CCG-G-CG-CC-C-T-ACTGACG-CTGA-GGCA-CGAAA-GTG-CGGGG-ATC-AAACAGG
>M00967_43_000000000-A3JHG_1_1101_18044_1900
TAC--GG-AG-GAT--GCG-A-G-C-G-T-T--GT-C-CGG-AA--TC-A-C-T--GG-GC--GT-A-AA-GG-GC-GC-G-TA-GGC-G-G-T-TT-A-A-T-AA-G-T-C-A-G-T-G-G--TG-A-AA-AC-T-G-AG-GG--CT-C-AA-C-C-C-T-C-A-G-CCT-G-C-CACT-GATAC-TG-T-TAGAC-TTGA-GT-AT-GG-AA-G-A---GG-A-G-AATGGAATTCCTAGTGT-AGCGGT-GAAATGCGTAG-AT-A-TT-AG-GA-GG-AACACCAGT-GGCGAAGGCG------AT-TCT-CTG-G-GC-CA-A-G-ACTGACG-CTGA-GGCG-CGAAA-GCG-TGGGG-AGC-AAACAGG
>M00967_43_000000000-A3JHG_1_1101_13234_1983
TAC--GG-AG-GAT--GCG-A-G-C-G-T-T--AT-C-CGG-AT--TT-A-T-T--GG-GT--TT-A-AA-GG-GT-GC-G-CA-GGC-G-G-A-AG-A-T-C-AA-G-T-C-A-G-C-G-G--TA-A-AA-TT-G-A-GA-GG--CT-C-AA-C-C-T-C-T-T-C--GA-G-C-CGTT-GAAAC-TG-G-TTTTC-TTGA-GT-GA-GC-GA-G-A---AG-T-A-TGCGGAATGCGTGGTGT-AGCGGT-GAAATGCATAG-AT-A-TC-AC-GC-AG-AACTCCGAT-TGCGAAGGCA------GC-ATA-CCG-G-CG-CT-C-A-ACTGACG-CTCA-TGCA-CGAAA-GTG-TGGGT-ATC-GAACAGG
```

These are all our representative reads again, now with additional alignment information.

In the log file of the **Filter.seqs** step we see the following additional information:

```
Length of filtered alignment: 376
Number of columns removed: 13049
Length of the original alignment: 13425
Number of sequences used to construct filter: 16298
```

From this log file we see that while our initial alignment was 13425 positions wide, after filtering the overhangs
(`trump` parameter) and removing positions that had a gap in every aligned read (`vertical` parameter), we have
trimmed our alignment down to a length of 376.

Because any filtering step we perform might lead to sequences no longer being unique, we deduplicate our data by re-running
the **Unique.seqs** tool:

> ### {% icon hands_on %} Hands-on: Re-obtain unique sequences
>
> - {% tool [Unique.seqs](toolshed.g2.bx.psu.edu/repos/iuc/mothur_unique_seqs/mothur_unique_seqs/1.39.5.0) %} with the following parameters
>   - {% icon param-file %} *"fasta"*: the `filtered fasta` output from **Filter.seqs** {% icon tool %}
>   - {% icon param-file %} *"name file or count table"*: the `count table` from the last **Screen.seqs** {% icon tool %}
>
> > ### {% icon question %} Question
> >
> >  How many duplicate sequences did our filter step produce?
> >
> > > ### {% icon solution %} Solution
> > > 3: The number of unique sequences was reduced from 16298 to 16295
> > {: .solution }
> {: .question}
{: .hands_on}


### Pre-clustering

The next step in cleaning our data, is to merge near-identical sequences together. Sequences that only differ
by around 1 in every 100 bases are likely to represent sequencing errors, not true biological variation. Because
our contigs are ~250 bp long, we will set the threshold to 2 mismatches.

> ### {% icon hands_on %} Hands-on: Perform preliminary clustering of sequences
>
> - {% tool [Pre.cluster](toolshed.g2.bx.psu.edu/repos/iuc/mothur_pre_cluster/mothur_pre_cluster/1.39.5.0) %} with the following parameters
>   - {% icon param-file %} *"fasta"*: the `fasta` output from the last **Unique.seqs** {% icon tool %} run
>   - {% icon param-file %} *"name file or count table"*: the `count table` from the last **Unique.seqs** {% icon tool %}
>   - *"diffs"*: `2`
>
> > ### {% icon question %} Question
> >
> >  How many unique sequences are we left with after this clustering of highly similar sequences?
> > > ### {% icon solution %} Solution
> > > 5720: This is the number of lines in the fasta output
> > {: .solution }
> {: .question}
{: .hands_on}

{% endunless %}


## Chimera Removal

{% unless include.short %}
We have now thoroughly cleaned our data and removed as much sequencing error as we can. Next, we will look
at a class of sequencing artefacts known as *chimeras*.
{% endunless %}

During PCR amplification, it is possible that two unrelated templates are combined to form a sort of hybrid sequence,
also called a *chimera*. Needless to say, we do not want such sequencing artefacts confounding our results. We'll do
this chimera removal using the `VSEARCH` algorithm {% cite Rognes2016 %} that is called within mothur, using the
**Chimera.vsearch** {% icon tool %} tool.

{% include topics/metagenomics/tutorials/mothur-miseq-sop/background_chimeras.md %}

{% if include.short %}


> ### {% icon hands_on %} Hands-on: Clean Aligned sequences and Chimera Removal
>
> 1. **Import the workflow** into Galaxy
>    - Copy the URL (e.g. via right-click) of [this workflow]({{ site.baseurl }}{{ page.dir }}workflows/workflow2_data_cleaning.ga) or download it to your computer.
>    - Import the workflow into Galaxy
>
>    {% snippet faqs/galaxy/workflows_import.md %}
>
> 2. Run **Workflow 2: Data Cleaning and Chimera Removal** {% icon workflow %} using the following parameters:
>    - *"Send results to a new history"*: `No`
>    - {% icon param-file %} *"1: Aligned Sequences"*: the `align` output from **Align.seqs** {% icon tool %}
>    - {% icon param-file %} *"2: Count Table"*: the `count table` from **Count.seqs** {% icon tool%}
>
>    {% snippet faqs/galaxy/workflows_run.md %}
>
> > ### {% icon question %} Question
> >
> > 1. How many chimeric sequences were detected?
> > 2. How many sequences remain after these cleaning steps?
> >
> > > ### {% icon solution %} Solution
> > >
> > > 1. There were **3,439 representative sequences** flagged as chimeric. These represent a total of **10,564 total sequences**
> > >
> > >    This can be determined by looking at the number of sequences in the `vsearch.accnos` file (3439). To determine how many total sequences these represent, compare the Summary.seqs log output files before and after the chimera filtering step (128,655-118,091=10,564).
> > >
> > > 2. There are 2,281 remaining sequences after filtering, clustering of highly similar sequences, and chimera removal.
> > >
> > >    This can be determined by looking at the number of sequences in the fasta output of **Remove.seqs** {% icon tool %}
> > {: .solution }
> {: .question}
>
{: .hands_on}

Have a look at the FASTA output from **Pre.cluster**, it should looks something like this:

```
>M00967_43_000000000-A3JHG_1_1101_13234_1983
TAC--GG-AG-GAT--GCG-A-G-C-G-T-T--AT-C-CGG-AT--TT-A-T-T--GG-GT--TT-A-AA-GG-GT-GC-G-CA-GGC-G-G-A-AG-A-T-C-AA-G-T-C-A-G-C-G-G--TA-A-AA-TT-G-A-GA-GG--CT-C-AA-C-C-T-C-T-T-C--GA-G-C-CGTT-GAAAC-TG-G-TTTTC-TTGA-GT-GA-GC-GA-G-A---AG-T-A-TGCGGAATGCGTGGTGT-AGCGGT-GAAATGCATAG-AT-A-TC-AC-GC-AG-AACTCCGAT-TGCGAAGGCA------GC-ATA-CCG-G-CG-CT-C-A-ACTGACG-CTCA-TGCA-CGAAA-GTG-TGGGT-ATC-GAACAGG
>M00967_43_000000000-A3JHG_1_1101_14069_1827
TAC--GG-AG-GAT--GCG-A-G-C-G-T-T--AT-C-CGG-AT--TT-A-T-T--GG-GT--TT-A-AA-GG-GT-GC-G-TA-GGC-G-G-C-CT-G-C-C-AA-G-T-C-A-G-C-G-G--TA-A-AA-TT-G-C-GG-GG--CT-C-AA-C-C-C-C-G-T-A--CA-G-C-CGTT-GAAAC-TG-C-CGGGC-TCGA-GT-GG-GC-GA-G-A---AG-T-A-TGCGGAATGCGTGGTGT-AGCGGT-GAAATGCATAG-AT-A-TC-AC-GC-AG-AACCCCGAT-TGCGAAGGCA------GC-ATA-CCG-G-CG-CC-C-T-ACTGACG-CTGA-GGCA-CGAAA-GTG-CGGGG-ATC-AAACAGG
```

We see that these are our contigs, but with extra alignment information. The filtering steps have removed any positions which had a gap symbol in all reads of the dataset.


{% else %}

This command will split the data by sample and check for chimeras. The recommended
way of doing this is to use the abundant sequences as our reference.

> ### {% icon hands_on %} Hands-on: Remove chimeric sequences
>
> 1. {% tool [Chimera.vsearch](toolshed.g2.bx.psu.edu/repos/iuc/mothur_chimera_vsearch/mothur_chimera_vsearch/1.39.5.1) %} with the following parameters
>   - {% icon param-file %} *"fasta"*: the `fasta` output from **Pre.cluster** {% icon tool %}
>   - {% icon param-select %} *"Select Reference Template from"*: `Self`
>   - {% icon param-file %} *"count"*: the `count table` from the last **Pre.cluster** {% icon tool %}
>   - {% icon param-check %} "dereplicate" to Yes
>
>     Running **Chimera.vsearch** with the count file will remove the chimeric sequences from the count table, but we
>     still need to remove those sequences from the fasta file as well. We do this using **Remove.seqs**:
>
> 2. {% tool [Remove.seqs](toolshed.g2.bx.psu.edu/repos/iuc/mothur_remove_seqs/mothur_remove_seqs/1.39.5.0) %} with the following parameters
>   - {% icon param-file %} *"accnos"*: the `vsearch.accnos` file from **Chimera.vsearch** {% icon tool %}
>   - {% icon param-file %} *"fasta"*: the `fasta` output from **Pre.cluster** {% icon tool %}
>   - {% icon param-file %} *"count"*: the `count table` from **Chimera.vsearch** {% icon tool %}
>
> > ### {% icon question %} Question
> >
> >  How many sequences were flagged as chimeric? what is the percentage? (Hint: summary.seqs)
> >
> > > ### {% icon solution %} Solution
> > > Looking at the chimera.vsearch `accnos` output, we see that **3,439 representative sequences** were flagged as chimeric. If we run summary.seqs on the resulting fasta file and count table, we see that we went from 128,655
> > > sequences down to 118,091 total sequences in this step, for a reduction of **10,564 total sequences**, or 8.2%. This is a reasonable number of
> > > sequences to be flagged as chimeric.
> > {: .solution }
> {: .question}
{: .hands_on}

{% endif %}


# Taxonomic Classification
{% include topics/metagenomics/tutorials/mothur-miseq-sop/switch_tutorial.md section="taxonomic-classification" %}

Now that we have thoroughly cleaned our data, we are finally ready to assign a taxonomy to our sequences.

We will do this using a Bayesian classifier (via the **Classify.seqs** {% icon tool %} tool) and a mothur-formatted [training
set provided by the Schloss lab](https://www.mothur.org/wiki/RDP_reference_files) based on the RDP (Ribosomal Database Project, {% cite Cole2013 %}) reference taxonomy.

{% include topics/metagenomics/tutorials/mothur-miseq-sop/background_reference_taxonomies.md %}


## Removal of non-bacterial sequences

Despite all we have done to improve data quality, there may still be more to do:
there may be 18S rRNA gene fragments or 16S rRNA from Archaea, chloroplasts, and mitochondria
that have survived all the cleaning steps up to this point. We are generally not interested in these sequences
and want to remove them from our dataset.


{% if include.short %}


> ### {% icon hands_on %} Hands-on: Taxonomic Classification and removal of non-bacterial sequences
>
> 1. **Import the workflow** into Galaxy
>    - Copy the URL (e.g. via right-click) of [this workflow]({{ site.baseurl }}{{ page.dir }}workflows/workflow3_classification.ga) or download it to your computer.
>    - Import the workflow into Galaxy
>
>    {% snippet faqs/galaxy/workflows_import.md %}
>
> 2. Run **Workflow 3: Classification** {% icon workflow %} using the following parameters:
>    - *"Send results to a new history"*: `No`
>    - {% icon param-file %} *"1: Cleaned sequences"*: the `fasta` output from **Remove.seqs** {% icon tool %}
>    - {% icon param-file %} *"2: Count Table"*: the `count table` from **Remove.seqs** {% icon tool%}
>    - {% icon param-file %} *"3: Training set Taxonomy"*: `trainset9_032012.pds.tax` file you imported from Zenodo
>    - {% icon param-file %} *"4: Training set FASTA"*: `trainset9_032012.pds.fasta` file from Zenodo
>
>    {% snippet faqs/galaxy/workflows_run.md %}
>
> > ### {% icon question %} Questions
> >
> > How many non-bacterial sequences were removed? Determine both the number of *representative sequences* and *total sequences* removed.
> >
> > > ### {% icon solution %} Solution
> > > There were **20 representative sequences** removed, representing **162 total sequences**.
> > > This can be determined by looking at the summary.seqs log outputs before the **Remove.lineage** step (after chimera removal), and after.
> > {: .solution }
> {: .question}
>
{: .hands_on}


{% else %}

> ### {% icon hands_on %} Hands-on: Taxonomic Classification and Removal of undesired sequences
>
> 1. {% tool [Classify.seqs](toolshed.g2.bx.psu.edu/repos/iuc/mothur_classify_seqs/mothur_classify_seqs/1.39.5.0) %} with the following parameters
>   - {% icon param-file %} *"fasta"*: the `fasta` output from **Remove.seqs** {% icon tool %}
>   - {% icon param-file %} *"reference"*: `trainset9032012.pds.fasta` from your history
>   - {% icon param-file %} *"taxonomy"*: `trainset9032012.pds.tax` from your history
>   - {% icon param-file %} *"count"*: the `count table` file from **Remove.seqs** {% icon tool %}
>
>     Have a look at the taxonomy output. You will see that every read now has a classification.
>
>     Now that everything is classified we want to remove our undesirables. We do this with the **Remove.lineage**
>     tool:
>
> 2. {% tool [Remove.lineage](toolshed.g2.bx.psu.edu/repos/iuc/mothur_remove_lineage/mothur_remove_lineage/1.39.5.0) %} with the following parameters
>   - {% icon param-file %} *"taxonomy"*: the taxonomy output from **Classify.seqs** {% icon tool %}
>   - {% icon param-text %} *"taxon - Manually select taxons for filtering"*: `Chloroplast-Mitochondria-unknown-Archaea-Eukaryota`
>   - {% icon param-file %} *"fasta"*: the `fasta` output from **Remove.seqs** {% icon tool %}
>   - {% icon param-file %} *"count"*: the `count table` from **Remove.seqs** {% icon tool %}
>
> > ### {% icon question %} Questions
> >
> > 1. How many unique (representative) sequences were removed in this step?
> > 2. How many sequences in total?
> >
> > > ### {% icon solution %} Solution
> > > 1. 20 representative sequences were removed.
> > >    The fasta file output from Remove.seqs had 2281 sequences while the fasta output from Remove.lineages
> > >    contained 2261 sequences.
> > >
> > > 2. 162 total sequences were removed.
> > >    If you run summary.seqs with the count table, you will see that we now have 2261 unique sequences
> > >    representing a total of 117,929 total sequences (down from 118,091 before). This means 162 of our
> > >    sequences were in represented by these 20 representative sequences.
> > {: .solution }
> {: .question}
{: .hands_on}

{% endif %}

The data is now as clean as we can get it. In the next section we will use the Mock sample to assess how accurate
our sequencing and bioinformatics pipeline is.


# Optional: Calculate error rates based on our mock community
{% include topics/metagenomics/tutorials/mothur-miseq-sop/switch_tutorial.md section="optional-calculate-error-rates-based-on-our-mock-community" %}


> ### {% icon comment %} Skipping the mock community analysis
>
> The mock community analysis is optional. If you are low on time or want to skip ahead, you can jump straight to [the next section](#otu-clustering)
> where we will cluster our sequences into OTUs, classify them and perform some visualisations.
> <br><br>
> Click [here](#otu-clustering) to skip this section and continue with the analysis.
>
{: .comment}


The following step is only possible if you have co-sequenced a mock community with your samples. A mock community is a sample
of which you know the exact composition and is something we recommend to do, because it will give you an idea of how
accurate your sequencing and analysis protocol is.


> ### {% icon comment %} Background: Mock communities
>
> **What is a mock community?**
>
> A mock community is an artificially constructed sample; a defined mixture of microbial cells and/or
> viruses or nucleic acid molecules created *in vitro* to simulate the composition of a microbiome
> sample or the nucleic acid isolated therefrom.
>
> **Why sequence a mock community?**
>
> In a mock community, we know exactly which sequences/organisms we expect to find, and at which proportions.
> Therefore, we can use such an artificial sample to assess the error rates of our sequencing and
> analysis pipeline.
>   - Did we miss any of the sequences we know to be present in the sample (false negatives)?
>   - Do we find any sequences that were not present in the sample (false positives)?
>   - Were we able to accurately detect their relative abundances?
>
> If our workflow performed well on the mock sample, we have more confidence in the accuracy of the
> results on the rest of our samples.
>
> **Example**
>
> As an example, consider the following image from Fouhy et al {% cite Fouhy2016 %}.
> A mock community sample was sequenced for different combinations of sequencer and primer sets (V-regions).
> Since we know the expected outcome, we can assess the accuracy of each pipeline. A similar approach can be used to
> assess different parameter settings of the *in-silico*  analysis pipeline.
>
> ![example results of mock community sequencing to assess error rates](../../images/mock_community.jpg "Example of usage of a mock community to assess accuracy. On the left is the expected result given that we know the exact composition of the mock sample. This was then used to assess the accuracy of different combinations of sequencing platform and primer set (choice of V-region)")
>
>
> **Further reading**
>
> - Next generation sequencing data of a defined microbial mock community {% cite Singer2016 %}
> - 16S rRNA gene sequencing of mock microbial populations- impact of DNA extraction method, primer choice and sequencing platform {% cite Fouhy2016 %}
>
{: .comment}

The mock community in this experiment was composed of genomic DNA from 21 bacterial strains. So in a perfect world, this is
exactly what we would expect the analysis to produce as a result.

First, let's extract the sequences belonging to our mock samples from our data:

> ### {% icon hands_on %} Hands-on: extract mock sample from our dataset
>
> - {% tool [Get.groups](toolshed.g2.bx.psu.edu/repos/iuc/mothur_get_groups/mothur_get_groups/1.39.5.0) %} with the following parameters
>   - {% icon param-file %} *"group file or count table"*: the `count table` from **Remove.lineage** {% icon tool %}
>   - {% icon param-select %} *"groups"*: `Mock`
>   - {% icon param-file %} *"fasta"*: `fasta` output from **Remove.lineage** {% icon tool %}
>   - {% icon param-check %} *"output logfile?"*: `yes`
>
{: .hands_on}

In the log file we see the following:

```
Selected 58 sequences from your fasta file.
Selected 4046 sequences from your count file
```

The Mock sample has 58 unique sequences, representing a total of 4,046 total sequences.

The **Seq.error** tool measures the error rates using our mock reference. Here we align
the reads from our mock sample back to their known sequences, to see how many fail to match.

> ### {% icon hands_on %} Hands-on: Assess error rates based on a mock community
> - {% tool [Seq.error](toolshed.g2.bx.psu.edu/repos/iuc/mothur_seq_error/mothur_seq_error/1.39.5.0) %} with the following parameters
>   - {% icon param-file %} *"pick.fasta"*: the `fasta` output from **Get.groups** {% icon tool %}
>   - {% icon param-file %} *"reference"*: `HMP_MOCK.v35.fasta` file from your history
>   - {% icon param-file %} *"count"*: the `count table` from **Get.groups** {% icon tool %}
>   - {% icon param-check %} *"output log?"*: `yes`
>
{: .hands_on}

In the log file we see something like this:

```
Overall error rate:    6.5108e-05
Errors    Sequences
0    3998
1    3
2    0
3    2
4    1
[..]
```

That is pretty good! The error rate is only 0.0065%! This gives us confidence that the rest of the samples
are also of high quality, and we can continue with our analysis.


### Cluster mock sequences into OTUs

We will now estimate the accuracy of our sequencing and analysis pipeline by clustering the Mock sequences into OTUs,
and comparing the results with the expected outcome.

{% include topics/metagenomics/tutorials/mothur-miseq-sop/background_otus.md %}

> ### {% icon hands_on %} Hands-on: Cluster mock sequences into OTUs
>
> {% if include.short %}
>
> 1. **Import the workflow** into Galaxy
>    - Copy the URL (e.g. via right-click) of [this workflow]({{ site.baseurl }}{{ page.dir }}workflows/workflow4_mock_otu_clustering.ga) or download it to your computer.
>    - Import the workflow into Galaxy
>
>    {% snippet faqs/galaxy/workflows_import.md %}
>
> 2. Run **Workflow 4: Mock OTU Clustering** {% icon workflow %} using the following parameters:
>    - *"Send results to a new history"*: `No`
>    - {% icon param-file %} *"1: Mock Count Table"*: the `count table` output from **Get.groups** {% icon tool %}
>    - {% icon param-file %} *"2: Mock Sequences"*: the `fasta` output from **Get.groups** {% icon tool%}
>
>    {% snippet faqs/galaxy/workflows_run.md %}
>
> {% else %}
>
> First we calculate the pairwise distances between our sequences
>
> 1. {% tool [Dist.seqs](toolshed.g2.bx.psu.edu/repos/iuc/mothur_dist_seqs/mothur_dist_seqs/1.39.5.0) %} with the following parameters
>   - {% icon param-file %} *"fasta"*: the `fasta` from **Get.groups** {% icon tool %}
>   - *"cutoff"*: `0.20`
>
>     Next we group sequences into OTUs
>
> 2. {% tool [Cluster - Assign sequences to OTUs](toolshed.g2.bx.psu.edu/repos/iuc/mothur_cluster/mothur_cluster/1.39.5.0) %} with the following parameters
>   - {% icon param-file %} *"column"*: the `dist` output from **Dist.seqs** {% icon tool %}
>   - {% icon param-file %} *"count"*: the `count table` from **Get.groups** {% icon tool %}
>
>     Now we make a *shared* file that summarizes all our data into one handy table
>
> 3. {% tool [Make.shared](toolshed.g2.bx.psu.edu/repos/iuc/mothur_make_shared/mothur_make_shared/1.39.5.0) %} with the following parameters
>   - {% icon param-file %} *"list"*: the `OTU list` from **Cluster** {% icon tool %}
>   - {% icon param-file %} *"count"*: the `count table` from **Get.groups** {% icon tool %}
>   - *"label"*: `0.03` (this indicates we are interested in the clustering at a 97% identity threshold)
>
>     And now we generate intra-sample rarefaction curves
>
> 4. {% tool [Rarefaction.single](toolshed.g2.bx.psu.edu/repos/iuc/mothur_rarefaction_single/mothur_rarefaction_single/1.39.5.0) %} with the following parameters
>   - {% icon param-file %} *"shared"*: the `shared` file from **Make.shared** {% icon tool %}
>
> {% endif %}
>
> > ### {% icon question %} Question
> >
> >  How many OTUs were identified in our mock community?
> > > ### {% icon solution %} Solution
> > > Answer: **34**
> > >
> > > This can be determined by opening the shared file or OTU list and looking at the header line. You will see a column for each OTU
> > {: .solution }
> {: .question}
{: .hands_on}


Open the rarefaction output (dataset named `sobs` inside the `rarefaction curves` output collection), it should look
something like this:

```
numsampled	0.03-	lci-	hci-
1	1.0000	1.0000	1.0000
100	18.0240	16.0000	20.0000
200	19.2160	17.0000	22.0000
300	19.8800	18.0000	22.0000
400	20.3600	19.0000	22.0000

[..]

3000	30.4320	28.0000	33.0000
3100	30.8800	28.0000	34.0000
3200	31.3200	29.0000	34.0000
3300	31.6320	29.0000	34.0000
3400	31.9920	30.0000	34.0000
3500	32.3440	30.0000	34.0000
3600	32.6560	31.0000	34.0000
3700	32.9920	31.0000	34.0000
3800	33.2880	32.0000	34.0000
3900	33.5920	32.0000	34.0000
4000	33.8560	33.0000	34.0000
4046	34.0000	34.0000	34.0000

```


When we use the full set of 4060 sequences, we find 34 OTUs from the Mock community; and with
3000 sequences, we find about 31 OTUs. In an ideal world, we would find exactly 21 OTUs. Despite our
best efforts, some chimeras or other contaminations may have slipped through our filtering steps.

{% include topics/metagenomics/tutorials/mothur-miseq-sop/background_rarefaction.md %}

Now that we have assessed our error rates we are ready for some real analysis.


# OTU Clustering
{% include topics/metagenomics/tutorials/mothur-miseq-sop/switch_tutorial.md section="otu-clustering" %}


In this tutorial we will continue with an OTU-based approach, for the phylotype and phylogenic
approaches, please refer to the [mothur wiki page](https://www.mothur.org/wiki/MiSeq_SOP).


{% include topics/metagenomics/tutorials/mothur-miseq-sop/background_otus.md %}


{% if include.short %}
We will now repeat the OTU clustering we performed on our mock community for our real datasets. We use a slightly different workflow because these tools are faster for larger datasets. We will also normalize our data by subsampling to the level of the sample with the lowest number of sequences in it.

{% else %}

### Remove Mock Sample

Now that we have cleaned up our data set as best we can, and assured ourselves of the quality of our sequencing
pipeline by considering a mock sample, we are almost ready to cluster and classify our real data. But
before we start, we should first remove the Mock dataset from our data, as we no longer need it. We do this using
the **Remove.groups** tool:

> ### {% icon hands_on %} Hands-on: Remove Mock community from our dataset
>
> - {% tool [Remove.groups](toolshed.g2.bx.psu.edu/repos/iuc/mothur_remove_groups/mothur_remove_groups/1.39.5.0) %} with the following parameters
>   - {% icon param-select %} *"Select input type"*: `fasta , name, taxonomy, or list with a group file or count table`
>   - {% icon param-file %} *"group or count table"*: the `pick.count_table` output from **Remove.lineage** {% icon tool %}
>   - {% icon param-select %} *"groups"*: `Mock`
>   - {% icon param-file %} *"fasta"*: the `pick.fasta` output from **Remove.lineage** {% icon tool %}
>   - {% icon param-file %} *"taxonomy"*: the `pick.taxonomy` output from **Remove.lineage** {% icon tool %}
>
{: .hands_on}

### Cluster sequences into OTUs

There are several ways we can perform clustering. For the Mock community, we used the traditional approach of
using the **Dist.seqs** and **Cluster** tools. Alternatively, we can also use the **Cluster.split** tool. With
this approach, the sequences are split into bins, and then clustered with each bin.  Taxonomic information is used to guide this process.
The Schloss lab have [published results](https://www.mothur.org/wiki/MiSeq_SOP#OTUs) showing that if you split at the level of Order or Family, and cluster to a 0.03
cutoff, you'll get just as good of clustering as you would with the "traditional" approach. In addition, this approach
is less computationally expensive and can be parallelized, which is especially advantageous when you have large
datasets.

We'll now use the **Cluster** tool, with `taxlevel` set to `4`, requesting that clustering be done at the *Order* level.
{% endif %}

> ### {% icon hands_on %} Hands-on: Cluster our data into OTUs
>
> {% if include.short %}
>
> 1. **Import the workflow** into Galaxy
>    - Copy the URL (e.g. via right-click) of [this workflow]({{ site.baseurl }}{{ page.dir }}workflows/workflow5_otu_clustering.ga) or download it to your computer.
>    - Import the workflow into Galaxy
>
>    {% snippet faqs/galaxy/workflows_import.md %}
>
> 2. Run **Workflow 5: OTU Clustering** {% icon workflow %} using the following parameters:
>    - *"Send results to a new history"*: `No`
>    - {% icon param-file %} *"1: Sequences"*: the `fasta` output from **Remove.lineage** {% icon tool %}
>    - {% icon param-file %} *"2: Count table"*: the `count table` output from **Remove.lineage** {% icon tool%}
>    - {% icon param-file %} *"3: Taxonomy"*: the `taxonomy` output from **Remove.lineage** {% icon tool%}
>
>    {% snippet faqs/galaxy/workflows_run.md %}
>
> {% else %}
>
> 1. {% tool [Cluster.split](toolshed.g2.bx.psu.edu/repos/iuc/mothur_cluster_split/mothur_cluster_split/1.39.5.0) %} with the following parameters
>   - *"Split by"*: `Classification using fasta`
>   - {% icon param-file %} *"fasta"*: the `fasta` output from **Remove.groups** {% icon tool %}
>   - {% icon param-file %} *"taxonomy"*: the `taxonomy` output from **Remove.groups** {% icon tool %}
>   - {% icon param-file %} *"name file or count table"*: the `count table` output from **Remove.groups** {% icon tool %}
>   - *"taxlevel"*: `4`
>   - *"cutoff"*: `0.03`
>
>    Next we want to know how many sequences are in each OTU from each group and we can do this using the
>    **Make.shared** tool. Here we tell mothur that we're really only interested in the 0.03 cutoff level:
>
> 2. {% tool [Make.shared](toolshed.g2.bx.psu.edu/repos/iuc/mothur_make_shared/mothur_make_shared/1.39.5.0) %} with the following parameters
>   - {% icon param-file %} *"list"*: the `list` output from **Cluster.split** {% icon tool %}
>   - {% icon param-file %} *"count"*: the `count table` from **Remove.groups** {% icon tool %}
>   - *"label"*: `0.03`
>
>    We probably also want to know the taxonomy for each of our OTUs. We can get the consensus taxonomy for each
>    OTU using the **Classify.otu** tool:
>
> 3. {% tool [Classify.otu](toolshed.g2.bx.psu.edu/repos/iuc/mothur_classify_otu/mothur_classify_otu/1.39.5.0) %} with the following parameters
>   - {% icon param-file %} *"list"*: the `list` output from **Cluster.split** {% icon tool %}
>   - {% icon param-file %} *"count"*: the `count table` from **Remove.groups** {% icon tool %}
>   - {% icon param-file %} *"taxonomy"*: the `taxonomy` output from **Remove.groups** {% icon tool %}
>   - *"label"*: `0.03`
>
> {% endif %}
{: .hands_on}

Examine {% icon galaxy-eye %} the `taxonomy` output of **Classify.otu** {% icon tool %}. This is a collection, and the different levels of taxonomy are shown in the names of the collection elements. In this example we only calculated one level, 0.03. This means we used a 97% similarity threshold. This threshold is commonly used to differentiate at *species level*.

Opening the taxonomy output for level 0.03 (meaning 97% similarity, or *species* level) shows a file structured like the following:

```
OTU       Size    Taxonomy
..
Otu0008	5260	Bacteria(100);"Bacteroidetes"(100);"Bacteroidia"(100);"Bacteroidales"(100);"Rikenellaceae"(100);Alistipes(100);
Otu0009	3613	Bacteria(100);"Bacteroidetes"(100);"Bacteroidia"(100);"Bacteroidales"(100);"Porphyromonadaceae"(100);"Porphyromonadaceae"_unclassified(100);
Otu0010	3058	Bacteria(100);Firmicutes(100);Bacilli(100);Lactobacillales(100);Lactobacillaceae(100);Lactobacillus(100);
Otu0011	2958	Bacteria(100);"Bacteroidetes"(100);"Bacteroidia"(100);"Bacteroidales"(100);"Porphyromonadaceae"(100);"Porphyromonadaceae"_unclassified(100);
Otu0012	2134	Bacteria(100);"Bacteroidetes"(100);"Bacteroidia"(100);"Bacteroidales"(100);"Porphyromonadaceae"(100);"Porphyromonadaceae"_unclassified(100);
Otu0013	1856	Bacteria(100);Firmicutes(100);Bacilli(100);Lactobacillales(100);Lactobacillaceae(100);Lactobacillus(100);
..
```

The first line shown in the snippet above indicates that Otu008 occurred 5260 times, and that all of the
sequences (100%) were binned in the genus *[Alistipes](https://en.wikipedia.org/wiki/Alistipes)*.


> ### {% icon question %} Question
>
> Which samples contained sequences belonging to an OTU classified as Staphylococcus?
>
> > ### {% icon solution %} Solution
> > Examine the `tax.summary` file output by **Classify.otu** {% icon tool %}.
> >
> > Samples F3D141, F3D142,  F3D144, F3D145, F3D2. This answer can be found by
> > examining the tax.summary output and finding the columns with nonzero
> > values for the line of Staphylococcus
> {: .solution }
{: .question}


Before we continue, let's remind ourselves what we set out to do. Our original question was about the stability of
the microbiome and whether we could observe any change in community structure between the early and late samples.

{% unless include.short %}
Because some of our sample may contain more sequences than others, it is generally a good idea to normalize the
dataset by subsampling.


> ### {% icon hands_on %} Hands-on: Subsampling
>
> First we want to see how many sequences we have in each sample. We'll do this with the
> **Count.groups** tool:
>
> 1. {% tool [Count.groups](toolshed.g2.bx.psu.edu/repos/iuc/mothur_count_groups/mothur_count_groups/1.39.5.0) %} with the following parameters
>   - {% icon param-file %} *"shared"*: the `shared` file from **Make.shared** {% icon tool %}
>
>    > ### {% icon question %} Question
>    > How many sequences did the smallest sample consist of?
>    > > ### {% icon solution %} Solution
>    > > The smallest sample is `F3D143`, and consists of 2389 sequences. This is a reasonable number, so we will now subsample all the other samples down to this level.
>    > {: .solution}
>    {: .question}
>
> 2. {% tool [Sub.sample](toolshed.g2.bx.psu.edu/repos/iuc/mothur_sub_sample/mothur_sub_sample/1.39.5.0) %} with the following parameters
>   - *"Select type of data to subsample"*: `OTU Shared`
>   - {% icon param-file %} *"shared"*: the `shared` file from **Make.shared** {% icon tool %}
>   - *"size"*: `2389`
>
>    > ### {% icon question %} Question
>    >
>    >  What would you expect the result of `count.groups` on this new shared output collection to be? Check if you are correct.
>    > > ### {% icon solution %} Solution
>    > > all groups (samples) should now have 2389 sequences. Run count.groups again on the shared output collection by the sub.sample
>    > > tool to confirm that this is indeed what happened.
>    > {: .solution }
>    {: .question}
>
> **Note:** since subsampling is a stochastic process, your results from any tools using this subsampled data
> will deviate from the ones presented here.
{: .hands_on}

{% endunless %}


# Diversity Analysis
{% include topics/metagenomics/tutorials/mothur-miseq-sop/switch_tutorial.md section="diversity-analysis" %}

Species diversity is a valuable tool for describing the ecological complexity of a single sample (alpha diversity)
or between samples (beta diversity). However, diversity is not a physical quantity that can be measured directly,
and many different metrics have been proposed to quantify diversity by {% cite Finotello2016 %}.


> ### {% icon comment %} Background: Species Diversity
>
> Species diversity consists of three components: species richness, taxonomic or phylogenetic diversity and species evenness.
>
> - **Species richness** = the number of different species in a community.
> - **Species evenness** = how even in numbers each species in a community is.
> - **Phylogenetic diversity** = how closely related the species in a community are.
>
> <br><br>
>
> Each of these factors play a role in diversity, but how to combine them into a single measure of diversity is nontrivial.
> Many different metrics have been proposed for this, for example: *shannon, chao, pd, ace, simpson, sobs, jack, npshannon,
> smithwilson, heip bergerparker, boney, efron, shen, solow, bootstrap, qstat, coverage, anderberg, hamming, jclass, jest,
> ochiai, canberra, thetayc, invsimpson*, just to name a few ;). A comparison of several different diversity metrics is discussed in {% cite BonillaRosso2012 %}
>
>
> > ### {% icon question %} Question
> > To understand the difference between richness and evenness, consider the following example:
> >
> > ![illustration of richness and evenness](../../images/richness-evenness-blank.png)
> >
> > 1. Which of these communities has the highest richness?
> > 2. Which of these communities has the highest evenness?
> >
> > > ### {% icon solution %} Solution
> > > 1. Both communities have 4 different species, so they have same richness.
> > > 2. Community B is more even, because each species has the same abundance.
> > >
> > > ![illustration of richness and evenness](../../images/richness-evenness.png)
> > {: .solution }
> >
> {: .question}
> <br><br>
>
> Even when two samples have identical richness and evenness, we still may conclude that one is more diverse than
> the other if the species are very dissimilar in one of the samples (have high phylogenetic distance), but very
> closely related to each other in the second sample.
>
> ![illustration of phylogenetic distance](../../images/phylogenetic-distance.png){: width="50%"}
>
> Now, you do not need to know what all these different metrics are, but just remember that there is not a single
> definition of diversity and as always, the metric you choose to use may influence your results.
>
{: .comment}

## Alpha diversity


In order to estimate alpha diversity of the samples, we first generate the *rarefaction curves*. Recall that
rarefaction measures the number of observed OTUs as a function of the subsampling size.

{% include topics/metagenomics/tutorials/mothur-miseq-sop/background_rarefaction.md %}


{% if include.short %}

We will use a plotting tool to visualize the rarefaction curves, and use **Summary.single** {% icon tool %} to calculate a number of different alpha diversity metrics on all our samples.

> ### {% icon hands_on %} Hands-on: Alpha Diversity
>
> 1. **Import the workflow** into Galaxy
>    - Copy the URL (e.g. via right-click) of [this workflow]({{ site.baseurl }}{{ page.dir }}workflows/workflow6_alpha_diversity.ga) or download it to your computer.
>    - Import the workflow into Galaxy
>
>    {% snippet faqs/galaxy/workflows_import.md %}
>
> 2. Run **Workflow 6: Alpha Diversity** {% icon workflow %} using the following parameters:
>    - *"Send results to a new history"*: `No`
>    - {% icon param-file %} *"1: Shared File"*: the `Shared file` output from **Make.shared** {% icon tool %}
>
>    {% snippet faqs/galaxy/workflows_run.md %}
>
{: .hands_on}

{% else %}


We calculate rarefaction curves with the **Rarefaction.single** {% icon tool %} tool:

> ### {% icon hands_on %} Hands-on: Calculate Rarefaction
> - {% tool [Rarefaction.single](toolshed.g2.bx.psu.edu/repos/iuc/mothur_rarefaction_single/mothur_rarefaction_single/1.39.5.0) %} with the following parameters
>   - {% icon param-file %} *"shared"*: the `shared file` from **Make.shared** {% icon tool %}
{: .hands_on}

Note that we used the default diversity measure here (*sobs*; observed species richness), but there are many
more options available under the *calc* parameter. The mothur wiki describes some of these calculators
[here](https://mothur.org/wiki/Calculators).

Examine the rarefaction curve output.

```
numsampled    0.03-F3D0    lci-F3D0    hci-F3D0    0.03-F3D1   ...
1              1.0000       1.0000      1.0000      1.0000
100           41.6560      35.0000     48.0000     45.0560
200           59.0330      51.0000     67.0000     61.5740
300           70.5640      62.0000     79.0000     71.4700
400           78.8320      71.0000     87.0000     78.4730
500           85.3650      77.0000     94.0000     83.9990
...
```

This file displays the number of OTUs identified per amount of sequences used (`numsampled`). What we would like
to see is the number of additional OTUs identified when adding more sequences reaching a plateau. Then we know
we have covered our full diversity. This information would be easier to interpret in the form of a graph.
Let's plot the rarefaction curve for a couple of our sequences:


> ### {% icon hands_on %} Hands-on: Plot Rarefaction
>
> - {% tool [Plotting tool - for multiple series and graph types](toolshed.g2.bx.psu.edu/repos/devteam/xy_plot/XY_Plot_1/1.0.2) %} with the following parameters
>   - *"Plot Title"*: `Rarefaction`
>   - *"Label for x axis"*: `Number of Sequences`
>   - *"Label for y axis"*: `Number of OTUs`
>   - *"Output File Type"*: `PNG`
>   - {% icon param-repeat %} Click on **Insert Series**,
>     - {% icon param-collection %} *"Dataset"*: rarefaction curve collection
>     - *"Header in first line?"*: `Yes`
>     - *"Column for x axis"*: `Column 1`
>     - *"Column for y-axis"*: `Column 2` and `Column 5` and every third column until the end (we are
>       skipping the low confidence and high confidence interval columns)
>
{: .hands_on}

{% endif %}

View the rarefaction plot output. From this image can see that the rarefaction curves for all samples have started to level
off so we are confident we cover a large part of our sample diversity:

![Rarefaction curves](../../images/rarefaction_curves.png)

{% unless include.short %}

Finally, let's use the **Summary.single** tool to generate a summary report.  The following step
will randomly subsample down to 2389 sequences, repeat this process 1000 times, and report several metrics:

> ### {% icon hands_on %} Hands-on: Summary.single
>
> - {% tool [Summary.single](toolshed.g2.bx.psu.edu/repos/iuc/mothur_summary_single/mothur_summary_single/1.39.5.2) %} with the following parameters
>   - {% icon param-file %} *"share"*: the `shared` file from **Make.shared** {% icon tool %}
>   - *"calc"*: `nseqs,coverage,sobs,invsimpson`
>   - *"size"*: `2389`
{: .hands_on}

{% endunless %}


View the `summary` output from **Summary.single** {% icon tool %}. This shows several alpha diversity metrics:
- [sobs](https://www.mothur.org/wiki/Sobs): observed richness (number of OTUs)
- [coverage](https://mothur.org/wiki/Coverage): Good's coverage index
- [invsimpson](https://en.wikipedia.org/wiki/Diversity_index#Simpson_index): Inverse Simpson Index
- [nseqs](https://www.mothur.org/wiki/Nseqs): number of sequences



```
label   group   sobs          coverage    invsimpson   invsimpson_lci   invsimpson_hci  nseqs
0.03    F3D0    167.000000    0.994697    25.686387    24.648040        26.816067       6223.000000
0.03    F3D1    145.000000    0.994030    34.598470    33.062155        36.284520       4690.000000
0.03    F3D141  154.000000    0.991060    19.571632    18.839994        20.362390       4698.000000
0.03    F3D142  141.000000    0.978367    17.029921    16.196090        17.954269       2450.000000
0.03    F3D143  135.000000    0.980738    18.643635    17.593785        19.826728       2440.000000
0.03    F3D144  161.000000    0.980841    15.296728    14.669208        15.980336       3497.000000
0.03    F3D145  169.000000    0.991222    14.927279    14.494740        15.386427       5582.000000
0.03    F3D146  161.000000    0.989167    22.266620    21.201364        23.444586       3877.000000
0.03    F3D147  210.000000    0.995645    15.894802    15.535594        16.271013       12628.000000
0.03    F3D148  176.000000    0.995725    17.788205    17.303206        18.301177       9590.000000
0.03    F3D149  194.000000    0.994957    21.841083    21.280343        22.432174       10114.000000
0.03    F3D150  164.000000    0.989446    23.553161    22.462533        24.755101       4169.000000
0.03    F3D2    179.000000    0.998162    15.186238    14.703161        15.702137       15774.000000
0.03    F3D3    127.000000    0.994167    14.730640    14.180453        15.325243       5315.000000
0.03    F3D5    138.000000    0.990523    29.415378    28.004777        30.975621       3482.000000
0.03    F3D6    155.000000    0.995339    17.732145    17.056822        18.463148       6437.000000
0.03    F3D7    126.000000    0.991916    13.343631    12.831289        13.898588       4082.000000
0.03    F3D8    158.000000    0.992536    23.063894    21.843396        24.428855       4287.000000
0.03    F3D9    162.000000    0.994803    24.120541    23.105499        25.228865       5773.000000
```

There are a couple of things to note here:
* The differences in diversity and richness between early and late time points is small.
* All sample coverage is above 97%.

There are many more diversity metrics, and for more information about the different calculators available in mothur, see [the mothur wiki page](https://mothur.org/wiki/Calculators)

We could perform additional statistical tests (e.g. ANOVA) to confirm our feeling that there is no significant difference based on sex or early vs. late, but this is beyond the scope of this tutorial.

## Beta diversity

Beta diversity is a measure of the similarity of the membership and structure found between *different* samples.
The default calculator in the following section is *thetaYC*, which is the [Yue & Clayton theta similarity
coefficient](http://csyue.nccu.edu.tw/2005communicationindex.pdf). We will also calculate the [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index) (termed `jclass` in mothur).

{% if include.short %}

In the following workflow we will:
- Calculate pairwise distances between samples using the *thetaYC* calculator (**Dist.shared** {% icon tool %})
- Create a **Venn diagram** to show the number of overlapping OTUs between 4 of our samples
- Create a **heatmap** of the intersample similarities (**Heatmap.sim** {% icon tool %})
- Create **pylogenetic tree** showing the relatedness of samples (**Newick Display** {% icon tool %})


> ### {% icon hands_on %} Hands-on: Beta Diversity
>
> 1. **Import the workflow** into Galaxy
>    - Copy the URL (e.g. via right-click) of [this workflow]({{ site.baseurl }}{{ page.dir }}workflows/workflow7_beta_diversity.ga) or download it to your computer.
>    - Import the workflow into Galaxy
>
>    {% snippet faqs/galaxy/workflows_import.md %}
>
> 2. Run **Workflow 7: Beta Diversity** {% icon workflow %} using the following parameters:
>    - *"Send results to a new history"*: `No`
>    - {% icon param-file %} *"1: Shared File"*: the `Shared file` output from **Make.shared** {% icon tool %}
>    - {% icon param-collection %} *"2: Subsample shared"*: the `shared` output from **Sub.sample** {% icon tool %}
>
>    {% snippet faqs/galaxy/workflows_run.md %}
>
{: .hands_on}


{% else %}
We calculate this with the **Dist.shared** tool, which will rarefy our data.

> ### {% icon hands_on %} Hands-on: Beta diversity
>
> 1. {% tool [Dist.shared](toolshed.g2.bx.psu.edu/repos/iuc/mothur_dist_shared/mothur_dist_shared/1.39.5.0) %} with the following parameters
>   - {% icon param-file %} *"shared"*: to the `shared` file from **Make.shared** {% icon tool %}
>   - *"calc"*: `thetayc,jclass`
>   - *"subsample"*: `2389`
>
>    Let's visualize our data in a Heatmap:
>
> 2. {% tool [Heatmap.sim](toolshed.g2.bx.psu.edu/repos/iuc/mothur_heatmap_sim/mothur_heatmap_sim/1.39.5.0) %} with the following parameters
>   - *"Generate Heatmap for"*: `phylip`
>   - {% icon param-collection %} *"phylip"*: the output of **Dist.shared** {% icon tool %} (this is a collection input)
>
> <!-- TODO: add image conversion tool for Galaxy instances that do not support viewing of SVGs files? -->
{: .hands_on}

{% endif %}

Look at some of the resulting heatmaps (you may have to download the SVG images first). In all of these
heatmaps the red colors indicate communities that are more similar than those with black colors.

For example this is the heatmap for the `thetayc` calculator (output `thetayc.0.03.lt.ave`):

![Heatmap for the thetayc calculator](../../images/heatmap.sim_thetayc.png)

and the jclass calulator (output `jclass.0.03.lt.ave`):

![Heatmap for the jclass calculator](../../images/heatmap.sim_jclass.png)


{% unless include.short %}

When generating Venn diagrams we are limited by the number of samples that we can analyze simultaneously.
Let's take a look at the Venn diagrams for the first 4 time points of female 3 using the **Venn** tool:

> ### {% icon hands_on %} Hands-on: Venn diagram
>
> - {% tool [Venn](toolshed.g2.bx.psu.edu/repos/iuc/mothur_venn/mothur_venn/1.39.5.0) %} with the following parameters
>   - {% icon param-collection %} *"OTU Shared"*: output from **Sub.sample** {% icon tool %} (collection)
>   - *"groups"*: `F3D0,F3D1,F3D2,F3D3`
{: .hands_on}

This generates a 4-way Venn diagram and a table listing the shared OTUs.

{% endunless %}

Examine the Venn diagram:

![Venn diagram and table with shared OTUs](../../images/venn.png)

This shows that there were a total of 180 OTUs observed between the 4 time points. Only 76 of those OTUs were
shared by all four time points. We could look deeper at the shared file to see whether those OTUs were
numerically rare or just had a low incidence.

{% unless include.short %}
Next, let's generate a dendrogram to describe the similarity of the samples to each other. We will generate a
dendrogram using the `jclass` and `thetayc` calculators within the **Tree.shared** tool:

> ### {% icon hands_on %} Tree
>
> 1. {% tool [Tree.shared](toolshed.g2.bx.psu.edu/repos/iuc/mothur_tree_shared/mothur_tree_shared/1.39.5.0) %} with the following parameters
>   - *"Select input format"*: `Phylip Distance Matrix`
>   - {% icon param-collection %} *"phylip"*: the `distance files` output from **Dist.shared** {% icon tool%}
>
> 2. {% tool [Newick display](toolshed.g2.bx.psu.edu/repos/iuc/newick_utils/newick_display/1.6+galaxy1) %} with the following parameters
>  - {% icon param-collection %} *"Newick file"*: output from **Tree.shared** {% icon tool %}
{: .hands_on}

{% endunless %}

Inspection of the the tree shows that the early and late communities cluster with themselves to the exclusion
of the others.


`thetayc.0.03.lt.ave`:

![Thetayc tree](../../images/tree.thetayc.png)

`jclass.0.03.lt.ave`:

![Jclass tree](../../images/tree.jclass.png)


# Visualisations

## Krona

A tool we can use to visualize the composition of our community, is [Krona](https://github.com/marbl/Krona/wiki)

> ### {% icon hands_on %} Hands-on: Krona
>
>  First we convert our mothur taxonomy file to a format compatible with Krona
>
> 1. {% tool [Taxonomy-to-Krona](toolshed.g2.bx.psu.edu/repos/iuc/mothur_taxonomy_to_krona/mothur_taxonomy_to_krona/1.0) %} with the following parameters
>   - {% icon param-collection %} *"Taxonomy file"*: the `taxonomy` output from **Classify.otu**
>
> 2. {% tool [Krona pie chart](toolshed.g2.bx.psu.edu/repos/crs4/taxonomy_krona_chart/taxonomy_krona_chart/2.7.1+galaxy0) %} with the following parameters
>   - *"Type of input"*: `Tabular`
>   - {% icon param-collection %} *"Input file"*: the `taxonomy` output from **Taxonomy-to-Krona** {% icon tool %}
{: .hands_on}

The resulting file is an HTML file containing an interactive visualization. For instance try double-clicking the
innermost ring labeled "Bacteroidetes" below:

<iframe id="krona" src="krona_all.html" frameBorder="0" width="100%" height="900px"> ![Krona](../../images/krona.png) </iframe>

> ### {% icon question %} Question
>
>  What percentage of your sample was labelled `Lactobacillus`?
>
> > ### {% icon solution %} Solution
> > Explore the Krona plot, double click on Firmicutes, here you should see Lactobacillus
> > clearly (16% in our case), click on this segment and the right-hand side will show you the percentages at
> > any point in the hierarchy (here 5% of all)
> >
> > ![image showing view with Lactobacillus highlighted](../../images/krona_lacto.png)
> {: .solution }
{: .question}

### Exercise: generating per-sample Krona plots (Optional)

You may have noticed that this plot shows the results for all samples together. In many
cases however, you would like to be able to compare results for different samples.

In order to save computation time, mothur pools all reads into a single file, and uses
the `count table` file to keep track of which samples the reads came from. However, Krona
does not understand the mothur count table format, so we cannot use that to supply information
about the groups. But luckily we can get **Classify.otu** {% icon tool %} to output per-sample
taxonomy files. In the following exercise, we will create a Krona plot with per-sample subplots.


> ### {% icon question %} Exercise: per-sample plots
>
> Try to create per-sample Krona plots. An few hints are given below, and the full answer
> is given in the solution box. <br>
>
> 1. Re-run {% icon galaxy-refresh %} the **Classify.otu** {% icon tool %} tool we ran earlier
>    - See if you can find a parameter to output a taxonomy file per sample (group)
> 2. Run **Taxonomy-to-Krona** {% icon tool %} again on the per-sample taxonomy files (collection)
> 3. Run **Krona** {% icon tool %}
>
> > ### {% icon solution %} Full Solution
> >
> > 1. Find the previous run of **Classify.otu** {% icon tool %} in your history
> >    - Hit the **rerun** button {% icon galaxy-refresh %} to load the parameters you used before:
> >      - {% icon param-file %} *"list"*: the `list` output from **Cluster.split** {% icon tool %}
> >      - {% icon param-file %} *"count"*: the `count table` from **Remove.groups** {% icon tool %}
> >      - {% icon param-file %} *"taxonomy"*: the `taxonomy` output from **Remove.groups** {% icon tool %}
> >      - *"label"*: `0.03`
> >    - Add new parameter setting:
> >      - *"persample - allows you to find a consensus taxonomy for each group"*: `Yes`
> >
> >     <br> You should now have a collection with per-sample files <br><br>
> >
> > 2. {% tool [Taxonomy-to-Krona](toolshed.g2.bx.psu.edu/repos/iuc/mothur_taxonomy_to_krona/mothur_taxonomy_to_krona/1.0) %} with the following parameters
> >     - {% icon param-collection %} *"Taxonomy file"*: the `taxonomy` collection from **Classify.otu** {% icon tool%}
> >
> > 3. {% tool [Krona pie chart](toolshed.g2.bx.psu.edu/repos/crs4/taxonomy_krona_chart/taxonomy_krona_chart/2.7.1+galaxy0) %} with the following parameters
> >   - *"Type of input"*: `Tabular`
> >   - {% icon param-collection %} *"Input file"*: the collection from **Taxonomy-to-Krona** {% icon tool %}
> >   - *"Combine data from multiple datasets?"*: `No`
> >
> > <br> The final result should look something like this (switch between samples via the list on the left): <br><br>
> >
> > <iframe id="krona" src="krona_multisample.html" frameBorder="0" width="100%" height="900px"> ![Krona](../../images/krona_multisample.png) </iframe>
> >
> {: .solution }
{: .question}



## Phinch

We may now wish to further visualize our results. We can convert our *shared* file to the more widely used `biom` format and
view it in a platform like [Phinch](http://www.phinch.org/).

> ### {% icon hands_on %} Hands-on: Phinch
>
> 1. {% tool [Make.biom](toolshed.g2.bx.psu.edu/repos/iuc/mothur_make_biom/mothur_make_biom/1.39.5.0) %} with the following parameters
>   - {% icon param-collection %} *"shared"*: the output from **Sub.sample** {% icon tool %}
>   - {% icon param-collection %} *"constaxonomy"*: the `taxonomy` output from **Classify.otu** {% icon tool %}
>   - {% icon param-file %} *"metadata"*: the `mouse.dpw.metadata` file you uploaded at the start of this tutorial
>
> 2. **View** the file in Phinch
>   - If you expand the the output biom dataset, you will see a link to view the file at Phinch
>
>       ![Icon to view at Phinch](../../../../shared/images/viewatphinch.png)
>
>    - Click on this link ("view biom at Phinch")
>
>  This link will lead you to a Phinch server (hosted by Galaxy), which will automatically load your file, and
>  where you can several interactive visualisations:
>  ![Phinch overview](../../../../shared/images/phinch_overviewpage.png)
>
> > ### {% icon comment %} No link to Phinch on your dataset?
> >
> > If this visualisation link is not present on your Galaxy dataset, you can download the generated BIOM file from Galaxy,
> > and upload it directly to the Phinch server at [https://usegalaxy.eu/phinch/](https://usegalaxy.eu/phinch/index.html). <br><br>
> >
> > **Important:** After downloading, please change the file extension from `.biom1` to `.biom` before uploading to Phinch. <br><br>
> >
> > **Note:** This link will visualize your data in Phinch version 1. Recently, Phinch have released version 2 as a desktop application.
> > This file can also be visualized in Phinch2, but requires installation of Phinch to your local machine
> {: .comment}
{: .hands_on}


# Conclusion
{:.no_toc}

Well done! {% icon trophy %} You have completed the basics of the Schloss lab's [Standard Operating Procedure for Illumina MiSeq data](https://www.mothur.org/wiki/MiSeq_SOP). You have worked your way through the following pipeline:

![mothur sop tutorial pipeline](../../images/mothur_sop_pipeline.jpg){:width="50%"}

