---
layout: tutorial_hands_on

title: Viral variation
zenodo_link: 'https://doi.org/10.5281/zenodo.3519268'
questions:
- How to identify variants in viral genomes?
- How to separate reliable calls from spurious?
- How to analyze dozens of samples simultaneously?
objectives:
- Highlight analysis of real life data containing many samples
- Demonstrate dynamic downsampling using Galaxy's Expression Tools
- Show how to annotate drug resistance mutations
- Highlight the importance of Jupyter notebooks
time_estimation: 3H
key_points:
- Real datasets contain many samples - Galaxy can handle this with ease
- High coverage in viral re-sequencing data may cause problems
- Strand bias estimation can help to eliminate spurious calls
contributors:
- nekrut

---


# Introduction
{:.no_toc}

<!-- This is a comment. -->

Identifying sequence variants in pathogens such as viruses and bacteria is important for obvious reasons. It facilitates understanding of infectious agent evolution, localization of mutations conferring drug resistance, identification of founding populations, tracking infection spread and so on. 

In this tutorial we will use datasets generated by {% cite Jair2019-hw %}. It is a re-sequencing dataset in which the authors amplified several segments of the *pol* gene (for HIV-I genome structure see Fig. [1](#figure-1) below) and sequenced resulting amplicons from 68 individuals. 

![HIV genome map](https://www.hiv.lanl.gov/content/sequence/HIV/IMAGES/hxb2genome.gif "Landmarks of the HIV-1 genome, HXB2 (from <a href='https://https://www.hiv.lanl.gov/content/sequence/HIV/MAP/landmark.html'>LANL HIV Genome Database</a>)")

Our main goal will be to analyze multiple datasets simultaneously to identify variants corresponding to known drug resistance mutations as well as potential new genome alterations.


> ### Agenda
>
> In this tutorial, we will cover:
>
> 1. TOC
> {:toc}
>
{: .agenda}

# Loading multiple datasets from the web

## Datasets

For this tutorial we downsampled all datasets produced by {% cite Jair2019-hw %} ([PRJNA517147](https://www.ncbi.nlm.nih.gov/bioproject/?term=PRJNA517147))) to approximately 10% of the original size. This was done to ensure that analyses can be performed quickly. The downsampled datasets are available from Zenodo [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3519268.svg)](https://doi.org/10.5281/zenodo.3519268). 

**PAIRED END DATA EXPLANATION**

> ### {% icon warning %} Important: Datasets for this tutorial
>
> Here we based all examples of just four datasets listed below. We recommend starting with these. You can of course analyze all 68, but starts with these four to get the feel for how things work.
{: .comment}

```
SRR8525909
SRR8525905
SRR8525896
SRR8525889
```

## Loading datasets using rule builder

Above, we listed the four datasets we are going to play with. But these are just IDs. In order to download actual data (fastq files) associated with these we need to construct URLs to let Galaxy know where to download data from. 

The full URL can be constructed by adding the following suffix:

```
https://zenodo.org/record/3519268/files/
```

and the following prefix:

```
.fq.gz
```

so that the full URL looks something like this:

```
https://zenodo.org/record/3519268/files/SRR8525909.fq.gz
```

But we need to do this for all four datasets (or all 68, or a few thousand if you analyze other data). To help with this we use **Rule builder** - a part of Galaxy's dataset upload functionality.

To understand how Rule Builder operates it is best to watch the following video

```
VIDEO PLACEHOLDER
```

Now that you've watched the video, do it yourself:

> ### {% icon hands_on %} Upload with Rule Builder 
>
> 1. Click the upload icon {% icon galaxy-upload %} in the upper left corner of the Galaxy interface
> 2. Select **Rule-based** tab
> 3. In **Upload data as** dropdown select *Collection(s)*
> 4. Copy and paste the four dataset IDs listed above into the large text box
> 5. Click **Build** button
> 6. Follow step from the video to create a collection
>
> > ### {% icon tip %} Tip: JSON rules
> >
> > Alternatively you import the following JSON block as shown here **TODO**:
> >
> > ```
> > {
  "rules": [
    {
      "type": "add_column_value",
      "value": "https://zenodo.org/record/3519268/files/"
    },
    {
      "type": "add_column_concatenate",
      "target_column_0": 1,
      "target_column_1": 0
    },
    {
      "type": "remove_columns",
      "target_columns": [
        1
      ]
    },
    {
      "type": "add_column_value",
      "value": ".fq.gz"
    },
    {
      "type": "add_column_concatenate",
      "target_column_0": 1,
      "target_column_1": 2
    },
    {
      "type": "remove_columns",
      "target_columns": [
        1,
        2
      ]
    }
  ],
  "mapping": [
    {
      "type": "list_identifiers",
      "columns": [
        0
      ],
      "editing": false
    },
    {
      "type": "url",
      "columns": [
        1
      ]
    }
  ],
  "extension": "fastqsanger.gz"
}
> > ```
> {: .tip}
{: .hands_on}

Once the collection is created and uploaded, you will see a new green box in the history pane as shown in Fig. [2](#figure-2) below:

![Dataset collection](../../images/hiv_collection.png "HIV datasets uploaded as a collection. In this case the collection contains four datasets"){: width="100px"}

# Quality control

## Assessing read quality

Once the data is we will need to assess its quality. There is a number of tutorials here already covering this [subject]({{ site.baseurl }}{% link topics/sequence-analysis/index.md %}), but for continuity we go through QC steps anyway.

> ### {% icon hands_on %} QC: Assessing read quality
>
> 1. **FastQC** {% icon tool %} with the following parameters:
>    - {% icon param-collection %} *"Short read data from your current history"*: `Jair data` (or whatever other name you gave that collection; red arrow in Fig. [3](#figure-3))
>    - Click **Execute** button
>
>    > ### {% icon comment %} Comment
>    >
>    > Note that **FastQC** produces two output collections:
>    >
>    > 1. "Webpage", and
>    > 2. "Raw Data"   
>    >
>    > "Raw data" is what we will be using next.
>    {: .comment}
>
{: .hands_on}

![Fastqc interface](../../images/hiv_fastqc1.png "FastqQC using collection as an input. Note that a collections button ({% icon param-collection %}) was pressed to reveal the collection in the history").

**FastQC** produces an individual report for each dataset. This means that in order to get an idea about quality of the data one needs to click report corresponding to each element of the collection. This may be possible for four datasets but will be very difficult for more than that. Fortunately, there is a tool, **MultiQC** that summarize data for all datasets at once. It takes collection produced by **FastQC** as input and produces a single report. Let apply it our example:

> ### {% icon hands_on %} QC: Summarizing FastQC results
>
> 1. **MultiQC** {% icon tool %} with the following parameters:
>    - *"Which tool was used generate logs?"* : `FastQC` (red arrow in  Fig. [4](#figure-4))
>    - {% icon param-collection %} *"FastQC output"*: `FastQC on collection 1: Raw data`; blue arrow in Fig. [4](#figure-4))
>    - Click **Execute** button
>
{: .hands_on}

![MultiQC interface](../../images/hiv_multiqc1.png "Running MultiQC on FastQC output. Because FastQC was run on a collection, it produced collections as outputs. Here one of these collections, 'Raw data', is used as an input").

MultiQC produces a variety of graphical summaries including distribution of quality values across reads shown in Fig. [5](#figure-5):

![MultiQC report](../../images/hiv_multiqc2.png "Distribution of quality values across four datasets in our example.")

## Trimming adapters

The original data produced by {% cite Jair2019-hw %} was contaminated with [Nextera adapter sequences](https://www.nature.com/protocolexchange/system/uploads/6661/original/SupplementaryDocument2-illumina-adapter-sequences-Feb2018.pdf?1530635414). We removed these adapters during downsampling (because they interfered with mapping). However, for the same of this presentation, let assume that our reads are in fact contaminated with adapters. 

A great tool for automatic detection and removal of adapters in **Trim Galore!**, which, in turn, is based on another widely used tools called **CutAdapt**. Let use **trim-galore!** to remove adapters:

> ### {% icon hands_on %} QC: Trimming adapters
>
> 1. **Trim Galore!** {% icon tool %} with the following parameters:
>    - *"Is this library paired- or single-end?"* : `Single-end` (green arrow in Fig. [6](#figure-6))
>    - *"Reads in FASTQ format"* : `Jair data` (or whatever other name you gave that collection; grey arrow in Fig. [6](#figure-6))
>    - *"Trim Galore! advanced settings"* : `Full parameter list` (blue arrow in Fig. [6](#figure-6))
>    - *"Discard reads that became shorter than length N"* : `0` (red arrow in Fig. [6](#figure-6))
>    - Click **Execute** button
>
>    > ### {% icon comment %} Comment
>    >
>    > Why do we use `Single-end` setting while our data is actually paired-end?
>    > <hr>
>    > Our data is in interleaved paired-end format. It is a single file that contains forward and reverse reads in an alternating order. Here we are fooling **Trim Galore!** into processing this file as a single end. In order to do this we set *"Discard reads that became shorter than length N"* to `0`. This guarantees that no reads will be thrown away and the alternating order of forward and reverse reads in our file will not be disrupted. 
>   >
>    {: .comment}
>
{: .hands_on}

> ### {% icon tip %} Tip: What did Trim Galore! do?
>
> If you scroll down interface of the **Trim Galore!** tool, you will see *"Generate a report file"* selector. Setting this to `Yes` will produce a report dataset that can be processed with **MultiQC**. This will provide information on what types and how many adapters have been removed from the reads. When running **MultiQC** simply set *"Which tool was used generate logs?"* to `CutAdapt / Trim Galore!`.
>
{: .tip}
 
![Trim Galore interface](../../images/hiv_tg1.png "Trim Galore! interface. Note that <em>Discard reads that became shorter than length N</em> is set to <tt>0</tt>(red arrow)")

# Mapping and estimating coverage

## Get the HIV genome 

Now it is time to map the reads against the HIV reference. We need to download that reference first. The sequence can be obtained directly from [NCBI](https://www.ncbi.nlm.nih.gov/) using the following URL:

```
https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=nucleotide&id=K03455.1&rettype=fasta
```

This URL can be pasted directly into **Upload** utility:

> ### {% icon hands_on %} Upload with Rule Builder 
>
> 1. Click the upload icon {% icon galaxy-upload %} in the upper left corner of the Galaxy interface
> 2. Select **Regular** tab
> 3. Click **Paste/Fetch data** button
> 4. A text box *"Download data from the web by entering URLs (one per line) or directly paste content"* will appear
> 5. Paste the URL we just mentioned into this box
> 6. Set **Type** to `fasta`
> 7. Click **Start**
> 8. A dataset with a long ugly name will appear in history
> 9. Rename dataset as `hxb2` (to learn hot to rename datasets see Tip below)
> {% include snippets/rename_dataset.md %}
{: .hands_on}

## Map against HIV genome using **BWA-MEM**

Next, we will map reads trimmed at the previous step against the newly downloaded genome, which just appeared as a new history item:

> ### {% icon hands_on %} Mapping with BWA-MEM
>
> Use **BWA-MEM** {% icon tool %} with the following parameters:
>
>    1. *"Will you select a reference genome from your history or use a built-in index?"* : `Use a genome from history and build index` (red arrow in Fig. [7](#figure-7))
>    2. *"Use the following dataset as the reference sequence"* : `hxb2` or however you named just uploaded HIV genome (blue arrow in Fig. [7](#figure-7))
>    3. *"Single or Paired-end reads"* : `Paired Interleaved` (green arrow in Fig. [7](#figure-7))
>    4. *"Select fastq dataset"* : Click {% icon param-collection %} and select collection produced by **Trim Galore!** (orange arrow in Fig. [7](#figure-7))
>    5. *"Set read groups information?"* : `Automatically assign ID`
>    6. Click **Execute**
{: .hands_on}


![bwa mem interface](../../images/hiv_bwa1.png "BWA MEM interface.")

## Estimate coverage

To proceed with variant calling we need to estimate coverage or reads across the HIV genome. The first step in this analysis is to compute coverage using **bedtools Genome Coverage** tool:

> ### {% icon hands_on %} Calculating coverage with BEDtools
>
> Use **bedtools Genome Coverage** {% icon tool %} with the following parameters:
>
>    1. *"Input type"* : Set to `BAM` (red arrow in Fig. [8](#figure-8))
>    2. *"BAM file"* : Click {% icon param-collection %} and select collection produced by **BWA MEM** during the previous step (blue arrow in Fig. [8](#figure-8))
>    3. Click **Execute**
{: .hands_on}

![bamtools genome coverage interface](../../images/hiv_cvrg1.png "bedtools Genome Coverage interface.")

**bedtools Genome Coverage** will produce output that would look like this

```
K03455.1	2016	2017	185
K03455.1	2017	2019	187
K03455.1	2019	2020	188
K03455.1	2020	2022	186
K03455.1	2022	2023	187
K03455.1	2023	2026	188
```

where the first column is the genome id, second and third are start and end of a bin, respectively, and the last one is coverage. Such data is produced for all four datasets in our collection. We need to aggregate these data by computing mean and median across the four datasets. This can be done with **Datamash** tool:

> ### {% icon hands_on %} Computing mean and median with Datamash
>
> Use **Datamash** {% icon tool %} with the following parameters:
>
>    1. *"Input tabular dataset"* : Click {% icon param-collection %} and select collection produced by **bedtools Genome Coverage** during the previous step (red arrow in Fig. [9](#figure-9))
>    2. *"Operation to perform on each group"* : set **Type** to `mean` and **On column** to `4` (blue arrow in Fig. [9](#figure-9))
>    3. Click *"Insert operation to perform on each group*" (orange arrow in Fig. [9](#figure-9))
>    4. Repeat step 2 with one exception: set **Type** to `median` (green arrow in Fig. [9](#figure-9))
>    5. Click **Execute**
{: .hands_on}

![datamash interface](../../images/hiv_datamash1.png "Datamash interface.")

for each dataset **Datamash** will produce two numbers like this:

```
918.05347593583	880
```

where the first value is mean and the last is median.

## Collapsing collection into a single dataset

Now we want to aggregate these into a single dataset and generate a histogram. To do this we will use a special class of Galaxy tools called collection operations. Specifically, we will use **Collapse Collection** tool:

> ### {% icon hands_on %} Collapsing a dataset collection
>
> Use **Collapse Collection** {% icon tool %} with the following parameters:
>
>    1. *"Collection of files to collapse into single dataset"* : Click {% icon param-collection %} and select collection produced by **Datamash** during the previous step (red arrow in Fig. [10](#figure-10))
>    2. *"Prepend File name"* :`Yes` (blue arrow in Fig. [10](#figure-10))
>    3. *"Where to add dataset name*" : `Same line and each line in dataset` (green arrow in Fig. [10](#figure-10))
>
{: .hands_on}

![collapse collection](../../images/hiv_collapse1.png "Collapse collection interface.")

**Collapse Collection** will produce the following output:

```
SRR8525909	918.05347593583	880
SRR8525905	269.6483909416	274
SRR8525896	85.444444444444	4
SRR8525889	11922.120564344	12696
```

you can see that selecting options highlighted with blue and green arrows (see Fig. [10](#figure-10)) instructed **Collapse Collection** to pre-pend dataset names to each corresponding line. We can now use these data to draw a simple barplot using Galaxy's built-in visualization engine:

> ### {% icon hands_on %} Building a histogram
> 1. Locate **Visualize** at the very top of Galaxy interface
> 2. Click and select **Create visualization**
> 3. Click on **Bar Horizontal (NVD3)**
> 4. A dropdown **Select a dataset to visualize:** will appear. From this dropdown select the result of **Collapse Collection** step (red arrow in Fig. [11](#figure-11))
> 5. A bar chart will appear. Use {% icon galaxy-chart-select-data %} to modify chart parameters. 
> 6. Change **Data point labels** to `Column 1`
> 7. Change **Values for x-axis** to `Column 1`
>
{: .hands_on}

![Selecting dataset for bar chart](../../images/hiv_chart1.png "Selecting dataset to produce bar chart.")

The resulting chart will look like this:

![Selecting dataset for bar chart](../../images/hiv_chart2.png "Selecting dataset to produce bar chart.")

Here you can see that coverage varies dramatically between datasets.

# Dynamic downsampling

The chart above shown that one of the datasets, `SRR8525889`, has a very high coverage. Normally this should not be a problem. However, because we will performing variant calling using **Freebayes** this may cause problems. Specifically, **Freebayes** tends to "stall" on the datasets that have high coverage variation - it never finished. To alleviate this problem we can simply downsample the BAM files to approximately 1,000 $$\times$$ coverage. 

Yet there is one difficulty. If we were dealing with a single dataset, that would be easy - just run a downsampling tools (such as **DownsampleSam** from the Picard package). But we have multiple datasets. These datasets also have diffent coverage, so each needs to be downsampled differently. Thus we need some way for doing this automatically (just imagine having hundreds or trousands of datasets). 

Galaxy has a new functionality for addressing problems of this kind. This functionality is called *expression tools*. (To learn more about expression tools see the following [tutorial]({{ site.baseurl }}{% link topics/galaxy-ui/tutorials/workflow-parameters/tutorial.md %}).

The basic idea is this:

 - we compute the mean coverage for each dataset
 - we feed this value to a special **expression tool**
 - this **expression tool** puts this value (or *expression*) as a **parameter** to downsampling tool
 - because this value will be calculated for each dataset individually, they will be downsampled dynamically: each according to its own coverage mean.

> ### {% icon comment %} The point of expression tools
>
> Normal Galaxy tools take files as inputs and produce files as outputs. The expression tools take files as inputs but instead of producing files as outputs they output parameters that can be used in other tools.
>
{: .comment}

Let's do this by example. Look at the following workflow:

![Downsampling workflow](../../images/hiv_ds_workflow1.png "Downsampling workflow from 10,000 feet.")

It has takes two inputs ("BAMs" and "Datamash results") and four steps. Before discussing this workflow in details let's describe the overall logic.

## The logic

For dataset `SRR8525889` mean and median of coverage look like this:

```
11922.120564344	12696
```

To downsample a dataset with mean coverage of $$11922$$ to about $$1000\times$$ we need to downsample it by:

$$ \frac{1000}{11922} \approx 0.083 $$

the means that we need to keep approximately 8.3% of the original reads. At the same time other datasets in our example have relatively low coverage. For example `SRR8525905` has the following stats:

```
269.6483909416	274
```

for this dataset we do not need to do anything. However, because we will be computing expression

$$ \frac{1000}{\bar{C}} $$

(where $$\bar{C}$$ is the mean coverage) for all samples for `SRR8525905` we will get $$\approx3.7$$, which is larger than 1 and therefore impossible. To avoid such situations we will be taking a *minimum* of $$\frac{1000}{\bar{C}}$$ and 1.


## Workflow Inputs

1. The first input in BAM datasets. These, in the case of our tutorial, are generated with **BWA-MEM**. It is just a collection of BAM datasets. It is exactly what we've produced at [mapping step](#map-against-hiv-genome-using-bwa-mem).
2. The second input is **Datamash** output produced by us at [estimate coverage](#estimate-coverage) step. 

## Workflow Steps

### Compute

The first step, `Compute` takes input of **Datamash**. Again, it looks like this:

```
269.6483909416	274
```

The **Compute** tool is designed to perform simple computations on tab delimited files. In this case we will set *"Add expression"* parameter if this tool to `min(1000/c1,1)`. This means that the tools will compute the value of 1,000 divided by the content of the first column `c1`. If this value is less than 1, it will add a column to the input dataset containing this value. If the values is greater than 1, it will add `1` as a new column to the input dataset. The following table contains an example of such calculation for two datasets:

|                  |  SRR8525905 | SRR8525889 |
|------------------|:-------------:|:------------:|
| Input to **Compute** tool       | `269.64 274` | `11922.12 12696` |
| Output of **Compute** tool       | `269.64 274 1` | `11922.12 12696 0.083` |

### Cut

The step cuts out the column produced by **Compute** tool from its output. This step is executed using **Cut** tools. To illustrate what is happening let's use the table from previous step:

|                  |  SRR8525905 | SRR8525889 |
|------------------|:-------------:|:------------:|
| Input to **Cut** tool       | `269.64 274 1` | `11922.12 12696 0.083` |
| Output of **Cut** tool       | `1` | `0.083` |

### Parse parameter value

This is the **Expression tool** step. In Fig. [13](#figure-13) you can see that the *"Float param"* output of **Parse parameter value** expression tool is connected with *"Probability"* parameter of **Downsample SAM/BAM** tool. Thus it supplies this tool with the *"Probability"* value appropriate for a given dataset. Thus this tools get different probability for each input dataset. 

### Downsample SAM/BAM

This step randomly samples reads from the BAM value according to supplied *"Probability"* value. If it is, for example, `0.083` it will sample approximately 8% of the reads. If it is `1` is will sample all the reads.


## Recreating workflow

To run the workflow you can either construct it from scratch or import prebuilt copy from the following URL:

```
https://workshop.usegalaxy.org/u/anton/w/downsample
```

{% include snippets/create_new_workflow.md %}
{% include snippets/import_workflow.md %}

Let's create workflow from scratch as shown here:

![Downsampling workflow](../../images/create_ds_workflow.gif "Creating downsampling workflow with expression tools. It extremely important to save your workflow after you have built it by clicking on the save ({% icon galaxy-save %}) icon. ")

## Running workflow

Once the workflow is created we can run it on the collection that was produced by the running **BWA-MEM** on HIV-1 genome. 

> ### {% icon hands_on %} Running downsampling workflow
> 1. Locate **Workflow** at the very top of Galaxy interface
> 2. Select the workflow you've just created (it is named `ds` in Fig. [14](#figure-14), but you could have named it any way you want). 
> 3. Click on the down arrow ({% icon galaxy-dropdown %}) adjacent to the workflow name and select **Run**.
> 4. In the interface that would appear you need to change three things:
>    - *"BAMs"* : set to the output of **BWA-MEM** from the [mapping step](#map-against-hiv-genome-using-bwa-mem) (red arrow in Fig. [15](#figure-15))
>    - *"Datamash"* : set to the [output of **Datamash**]](#hands_on-computing-mean-and-median-with-datamash) (blue arrow in Fig. [15](#figure-15))
>    - *"Add expression"* : set to `min(1000/c1,1)`. This expression will output minimal of two values: `1000/c1` or `1` (orange arrow in Fig. [15](#figure-15))
> 5. Click **Run workflow**
>
{: .hands_on}

![Running downsampling workflow](../../images/hiv_run_wf.png "Running downsampling workflow")

# Calling variants

We are now ready to identify sequence variants in our data. For this purpose we will use **Freebayes** - a versatile variant calling suitable for mixed haploid samples. 

> ### {% icon hands_on %} Calling variants with Freebayes
> 1. *"Choose the source for the reference genome"* : `History` (red arrow in Fig. [16](#figure-16))
> 2. *"BAM dataset"* : Set this to the collection produced by the downsampling workflow (blue arrow in Fig. [16](#figure-16))
> 3. *"Use the following dataset as the reference sequence"* : Set this to HIV genome sequence [uploaded previously](#get-the-hiv-genome) (orange arrow in Fig. [16](#figure-16))
> 4. *"Choose parameter selection level"* : `Full list of options`
> 5. Locate *"Population model options"* dropdown and set it to `Set population model options`. Within this section set the following:
>    - *"Set ploidy for the analysis"* : `1` (red arrow in Fig. [17](#figure-17))
>    - *"Output all alleles which pass input filters, regardless of genotyping outcome or model"* : `Yes` (blue arrow in Fig. [17](#figure-17))
> 6. Locate *"Input filters"* dropdown and set it to `Set input filters`. Within this section set the following:
>    - *"Use stringent input base and mapping quality filters"* : `Yes` (red arrow in Fig. [18](#figure-18))
>    - *"Require at least this fraction of observations supporting an alternate allele within a single individual in the in order to evaluate the position"* : `0.001` (blue arrow in Fig. [18](#figure-18))
>    - *"Require at least this count of observations supporting an alternate allele within a single individual in order to evaluate the position"* : `10` (orange arrow in Fig. [18](#figure-18))
>  7. Click **Execute**
{: .hands_on}

![Freebayes datasets](../../images/hiv_freebayes1.png "Setting inputs for Freebayes")
![Freebayes population settings](../../images/hiv_freebayes2.png "Setting population options")
![Freebayes input filters](../../images/hiv_freebayes3.png "Setting input filters")




> 2. Select the workflow you've just created (it is named `ds` in Fig. [14](#figure-14), but you could have named it any way you want). 
> 3. Click on the down arrow ({% icon galaxy-dropdown %}) adjacent to the workflow name and select **Run**.
> 4. In the interface that would appear you need to change three things:
>    - *"BAMs"* : set to the output of **BWA-MEM** from the [mapping step](#map-against-hiv-genome-using-bwa-mem) (red arrow in Fig. [15](#figure-15))
>    - *"Datamash"* : set to the [output of **Datamash**]](#hands_on-computing-mean-and-median-with-datamash) (blue arrow in Fig. [15](#figure-15))
>    - *"Add expression"* : set to `min(1000/c1,1)`. This expression will output minimal of two values: `1000/c1` or `1` (orange arrow in Fig. [15](#figure-15))
> 5. Click **Run workflow**
>
{: .hands_on}


# Title of the section usually corresponding to a big step in the analysis

It comes first a description of the step: some background and some theory.
Some image can be added there to support the theory explanation:

![Alternative text](../../images/image_name "Legend of the image")

The idea is to keep the theory description before quite simple to focus more on the practical part.

***TODO***: *Consider adding a detail box to expand the theory*

> ### {% icon details %} More details about the theory
>
> But to describe more details, it is possible to use the detail boxes which are expandable
>
{: .details}

A big step can have several subsections or sub steps:

## Sub-step with **My Tool**

> ### {% icon hands_on %} Hands-on: Task description
>
> 1. **My Tool** {% icon tool %} with the following parameters:
>    - {% icon param-file %} *"Input file"*: File
>    - *"Parameter"*: `a value`
>
>    ***TODO***: *Check parameter descriptions*
>
>    ***TODO***: *Consider adding a comment or tip box*
>
>    > ### {% icon comment %} Comment
>    >
>    > A comment about the tool or something else. This box can also be in the main text
>    {: .comment}
>
{: .hands_on}

***TODO***: *Consider adding a question to test the learners understanding of the previous exercise*

> ### {% icon question %} Questions
>
> 1. Question1?
> 2. Question2?
>
> > ### {% icon solution %} Solution
> >
> > 1. Answer for question1
> > 2. Answer for question2
> >
> {: .solution}
>
{: .question}


## Re-arrange

To create the template, each step of the workflow had its own subsection.

***TODO***: *Re-arrange the generated subsections into sections or other subsections.
Consider merging some hands-on boxes to have a meaningful flow of the analyses*

# Conclusion
{:.no_toc}

Sum up the tutorial and the key takeaways here. We encourage adding an overview image of the
pipeline used.