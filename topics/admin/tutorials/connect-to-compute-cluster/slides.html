---
layout: tutorial_slides
logo: "GTN"

title: "Connecting Galaxy to a compute cluster"
questions:
  - How to connect Galaxy to a compute cluster?
  - How can I configure job dependent resources, like cores, memory for my DRM?
objectives:
  - Understand all components of the Galaxy job running stack
  - Understand how the `job_conf.xml` file controls Galaxy's jobs subsystem
  - Know how to map tools to job destinations
  - The various ways in which tools can be mapped to destinations, both statically and dynamically
key_points:
  - Galaxy supports a variety of different DRMs.
  - Dynamic Tool Destinations are a convenient way to map
  - Job resource parameters can allow you to give your users control over job resource requirements, if they are knowledgeable about the tools and compute resources available to them.
contributors:
  - natefoo
  - bgruening
  - nsoranzo
  - hexylena
tags:
  - features
---

# Galaxy Job Configuration

- Configured in `config/job_conf.xml`
- XML format with macro support
- Major components:
  - **Plugins**: distributed resource manager (DRM) modules to load
  - **Handlers**: job handler processes managing the lifecycle of jobs
  - **Destinations**: where to send jobs, and what parameters to run those jobs with
  - **Tool** to destination/handler mappings
  - **Resource** selection mappings: give users job execution options on the tool form
  - **Limits**: job runtime limits, e.g. the max number of concurrent jobs

---

# Why cluster?

Running jobs on the Galaxy server negatively impacts Galaxy UI performance

Even adding one other host helps

Can restart Galaxy without interrupting jobs

---

# Plugins

Correspond to job runner plugins in [lib/galaxy/jobs/runners](https://github.com/galaxyproject/galaxy/tree/dev/lib/galaxy/jobs/runners)

.left[Plugins for:]
- local
- Slurm (DRMAA subclass)
- DRMAA: SGE, PBS Pro, LSF, Torque
- HTCondor
- Torque: Using the `pbs_python` library
- Pulsar: Galaxy's own remote job management system
- Command Line Interface (CLI) via SSH
- Kubernetes
- Go-Docker
- Chronos

---

# Cluster library stack (DRMAA)

![Cluster library stack](../../images/cluster_lib_stack.png)

---

# Handlers

Define which Galaxy processes are job handlers

- `id` attribute should match the `--server-name` param value of a process
- Dedicated handlers can be reserved, e.g. for small, high throughput jobs
- The list of plugins that are loaded by a job handler can be limited using `<plugin>` subelements (e.g. when the DRMAA plugin needs to be loaded with different library paths)
- Not defined in `job_conf.xml` when using *job handler mules*

---

# Destinations

Define *how* jobs should be run

- Which plugin?
- In a Docker container? Which one?
- **DRM params** (queue, cores, memory, walltime)?
- Environment (variables e.g. `$PATH`, source an env file, run a command)?

---

# The default job configuration

.left[`config/job_conf.xml.sample_basic`:]
```xml
<?xml version="1.0"?>
<job_conf>
    <plugins>
        <plugin id="local" type="runner" load="galaxy.jobs.runners.local:LocalJobRunner" workers="4"/>
    </plugins>
    <handlers>
        <handler id="main"/>
    </handlers>
    <destinations>
        <destination id="local" runner="local"/>
    </destinations>
</job_conf>
```
---

# Job Config - Tags

Both destinations and handlers can be grouped by **tags**

- Allows random selection from multiple resources
- Allows concurrency limits at the destination group level
---

# Job Environment

`<env>` tag in destinations: configure the job exec environment

| tag syntax | function |
| ---- | ---- |
| `<env id="NAME">VALUE</env>` | Set `$NAME` to `VALUE` |
| `<env file="/path/to/file" />` | Source shell file at `/path/to/file` |
| `<env exec="CMD" />` | Execute `CMD` |

Source and command execution will be handled on the remote destination, don't need to work on the Galaxy server

---

# Limits

Available limits

- Walltime (if not available with your DRM)
- Output size (if *any* tool output grows larger than this limit)
- Concurrency: Number of "active" (queued or running) jobs

---

# Concurrency Limits

Available limits

- Number of active jobs per registered user
- Number of active jobs per unregistered user
- Number of active jobs per registered user in a specified destination or destination tag
- Number of total active jobs in a specified destination or destination tag

---

# Shared Filesystem

Most job plugins require a **shared filesystem** between the Galaxy server and compute.

The exception is **Pulsar**. More on this in *Using heterogeneous compute resources*

---

# Shared Filesystem

Our simple example works because of two important principles:

1. Some things are located *at the same path* on Galaxy server and node(s)
  - Galaxy application (`/srv/galaxy/server`)
  - Tool dependencies
2. Some things *are the same* on Galaxy server and node(s)
  - Job working directory
  - Input and output datasets

The first can be worked around with symlinks or Pulsar embedded (later)

The second can be worked around with Pulsar REST/MQ (with a performance/throughput penalty)

---

# Multiprocessing

Some tools can greatly improve performance by using multiple cores

Galaxy automatically sets `$GALAXY_SLOTS` to the CPU/core count you specify when submitting, for example, 4:
- Slurm: `sbatch --ntasks=4`
- SGE: `qsub -pe threads 4`
- Torque/PBS Pro: `qsub -l nodes=1:ppn=4`
- Condor: `request_cpus: 4`

Tool configs: Consume `\${GALAXY_SLOTS:-4}`

---

# Memory requirements

For **Slurm only**, Galaxy will set `$GALAXY_MEMORY_MB` and `$GALAXY_MEMORY_MB_PER_SLOT` as integers.

**Other DRMs:** Please PR the [appropriate code](https://github.com/galaxyproject/galaxy/blob/dev/lib/galaxy/jobs/runners/util/job_script/MEMORY_STATEMENT.sh).

For Java tools, be sure to set `-Xmx`, e.g.:

```xml
<destination id="foo" ...>
    <env id="_JAVA_OPTIONS">-Xmx4096m</env>
</destination>
```

---

# Run jobs as the "real" user

If your Galaxy users == System users:
- Submit jobs to cluster as the actual user
- Configurable callout scripts before/after job to change ownership
- Probably requires limited sudo for Galaxy user

See: [Cluster documentation](https://wiki.galaxyproject.org/Admin/Config/Performance/Cluster)


---

## Job Config - Mapping Tools to Destinations

Problem: Tool A uses single core, Tool B uses multiple
- Both submit to the same cluster
- Need different submit parameters (`--ntasks=1` vs. `--ntasks=4` in Slurm)

---

## Job Config - Mapping Tools to Destinations

Solution:
```xml
    <destinations default="single">
        <destination id="single" runner="slurm" />
        <destination id="multi" runner="slurm">
            <param id="nativeSpecification">--ntasks=4</param>
        </destination>
    <tools>
        <tool id="hisat2" destination="multi"/>
    </tools>
```

---

# The Dynamic Job Runner

For when basic tool-to-destination mapping isn't enough

---

## The Dynamic Job Runner

A special built-in job runner plugin

Map jobs to destinations on more than just tool IDs

.left[Two types:]
- Dynamic Tool Destinations
- Python function

See: [Dynamic Destination Mapping](https://docs.galaxyproject.org/en/master/admin/jobs.html#dynamic-destination-mapping)

---

## Dynamic Tool Destinations

.left[Configurable mappings without programming:]
- YAML format config file `tool_destinations.yml`
- Map based on tool ID plus:
  - Input dataset size(s)
  - Input dataset number of records
  - User
- Maps to static destinations defined in job config

---

## Arbitrary Python Functions

.left[Programmable mappings:]
- Written as Python function in `lib/galaxy/jobs/rules/`
- Map based on:
  - Tool ID
  - User email or username
  - Inputs
  - Tool Parameters
  - Defined "helper" functions based on DB contents
  - Anything else discoverable
    - Cluster queue depth?
    - ...?
- Can dynamically modify destinations in job config (i.e. `sbatch` params)

---

## Tool Dependency Resolution

- Galaxy can resolve dependencies using Conda, Singularity, Docker, and other systems like Modules
- You can read more about this in the [Galaxy Tool Management with Ephemeris slides](../tool-management/slides.html)
