<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <title>Deep Learning (Part 1) - Feedforward neural networks (FNN)</title>
        
            <script async defer data-domain="training.galaxyproject.org" src="https://plausible.galaxyproject.eu/js/plausible.js"></script>

        
        <link rel="stylesheet" href="/training-material/assets/css/bootstrap.min.css?v=3">
        <link rel="stylesheet" href="/training-material/assets/css/bootstrap-toc.min.css">
        <link rel="stylesheet" href="/training-material/assets/css/main.css?v=2">
        <script src="https://kit.fontawesome.com/67b3f98409.js" crossorigin="anonymous"></script>
        <link rel="stylesheet" href="/training-material/assets/css/academicons.css">
        <link rel="stylesheet" href="/training-material/assets/css/syntax_highlighting.css">
        <link rel="shortcut icon" href="/training-material/favicon.ico" type="image/x-icon" />
        <link rel="alternate" type="application/atom+xml" href="/training-material/feed.xml" />

        
        
        
        
        
        <meta name="description" content="Statistical Analyses for omics data and machine learning ..." />
        <meta property="og:title" content="Galaxy Training: Deep Learning (Part 1) - Feedforward neural networks (FNN)" />
        <meta property="og:description" content="Statistical Analyses for omics data and machine learning ..." />
        <meta property="og:image" content="/training-material/assets/images/GTNLogo1000.png" />
    </head>
    <body data-spy="scroll" data-target="#toc">
        <header>
    <nav class="navbar navbar-expand-md navbar-dark" aria-label="Site Navigation">
        <div class="container">
            <a class="navbar-brand" href="/training-material/">
                <img src="/training-material/assets/images/GTN-60px.png" height="30" alt="Galaxy Training Network logo">
                Galaxy Training!
            </a>

            <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#top-navbar" aria-controls="top-navbar" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="top-navbar">
                <ul class="navbar-nav">
                    <li class="nav-item">
                        
                        <a class="nav-link" href="/training-material/topics/statistics" title="Go back to list of tutorials">
                            <i class="far fa-folder" aria-hidden="true"></i> Statistics and machine learning
                        </a>
                        
                    </li>
                    <li class="nav-item dropdown">
    <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-expanded="false" title="Help">
        <i class="far fa-question-circle" aria-hidden="true"></i><span class="visually-hidden">help</span> Help
    </a>
    <div class="dropdown-menu dropdown-menu-right">
        <!-- disable Tess for now
        <form method="get" action="https://tess.elixir-europe.org/materials">
            <input type="text" id="search" name="q" value="" style="margin-left: 0.5em;/*! border-radius: 0px; */">
            <input type="hidden" value="Galaxy Training" name="content_provider">
            <input type="submit" value="Search on TeSS" style="width: 92%;border-radius: 0px;margin: 0.5em;background: #f47d20;border: 0px;padding: 0.25em;" class="">
        </form>
        -->

        <a class="dropdown-item" href="/training-material/faq" title="Check our FAQs">
           <i class="far fa-question-circle" aria-hidden="true"></i><span class="visually-hidden">question</span> FAQs
        </a>
        
        <a class="dropdown-item" href="/training-material/topics/statistics/faqs/" title="Check our FAQs for the Statistics and machine learning topic">
           <i class="far fa-question-circle" aria-hidden="true"></i><span class="visually-hidden">question</span> Topic FAQs
        </a>
        
        <a class="dropdown-item" href="https://help.galaxyproject.org/" title="Discuss on Galaxy Help">
            <i class="far fa-comments" aria-hidden="true"></i><span class="visually-hidden">feedback</span> Galaxy Help Forum
        </a>
        <a class="dropdown-item" href="https://gitter.im/Galaxy-Training-Network/Lobby" title="Discuss on gitter">
           <i class="fab fa-gitter" aria-hidden="true"></i><span class="visually-hidden">gitter</span> Discuss on Gitter
        </a>
    </div>
</li>


                    <li class="nav-item dropdown">
    <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-expanded="false" title="Extras">
        <i class="far fa-star" aria-hidden="true"></i><span class="visually-hidden">galaxy-star</span> Extras
    </a>
    <div class="dropdown-menu dropdown-menu-right">

        
        <a class="dropdown-item" href="https://github.com/galaxyproject/training-material/edit/main/topics/statistics/tutorials/FNN/tutorial.md" title="Edit on GitHub">
          <i class="fab fa-github" aria-hidden="true"></i><span class="visually-hidden">github</span> Edit on GitHub
        </a>

        <a class="dropdown-item" href="/training-material/stats.html" title="Show view statistics about this repository">
            <i class="fas fa-chart-bar" aria-hidden="true"></i><span class="visually-hidden">galaxy-barchart</span> GTN statistics
        </a>

        <a class="dropdown-item" href="https://plausible.galaxyproject.eu/training.galaxyproject.org?period=12mo&page=/training-material/topics/statistics/tutorials/FNN/tutorial.html" title="Show view statistics of this page">
            <i class="fas fa-chart-bar" aria-hidden="true"></i><span class="visually-hidden">galaxy-barchart</span> Page Metrics
        </a>

        
            <a class="dropdown-item" href="/training-material/feedback.html" title="Show feedback statistics about this repository">
                <i class="fas fa-chart-bar" aria-hidden="true"></i><span class="visually-hidden">galaxy-barchart</span> GTN feedback
            </a>
        

        <div class="dropdown-item">
            <div>
                <i class="fas fa-language" aria-hidden="true"></i><span class="visually-hidden">language</span> Translate this page
            </div>

            <div id="lang-selector">
                
                <strong>Automatic Translations</strong>

<a class="btn btn-info" href="https://translate.google.com/translate?hl=jp&sl=en&tl=fr&u=https%3A%2F%2Ftraining.galaxyproject.org/topics/statistics/tutorials/FNN/tutorial.html&edit-text=&act=url">
                Fran√ßais
                </a>
                
<a class="btn btn-info" href="https://translate.google.com/translate?hl=jp&sl=en&tl=ja&u=https%3A%2F%2Ftraining.galaxyproject.org/topics/statistics/tutorials/FNN/tutorial.html&edit-text=&act=url">
                Êó•Êú¨Ë™û
                </a>
                
<a class="btn btn-info" href="https://translate.google.com/translate?hl=jp&sl=en&tl=es&u=https%3A%2F%2Ftraining.galaxyproject.org/topics/statistics/tutorials/FNN/tutorial.html&edit-text=&act=url">
                Espa√±ol
                </a>
                
<a class="btn btn-info" href="https://translate.google.com/translate?hl=jp&sl=en&tl=pt&u=https%3A%2F%2Ftraining.galaxyproject.org/topics/statistics/tutorials/FNN/tutorial.html&edit-text=&act=url">
                Portugu√™s
                </a>
                
<a class="btn btn-info" href="https://translate.google.com/translate?hl=jp&sl=en&tl=ar&u=https%3A%2F%2Ftraining.galaxyproject.org/topics/statistics/tutorials/FNN/tutorial.html&edit-text=&act=url">
                ÿßŸÑÿπÿ±ÿ®Ÿäÿ©
                </a>
                
                <a class="btn btn-info" href="https://translate.google.com/translate?hl=jp&sl=en&tl=&u=https%3A%2F%2Ftraining.galaxyproject.org/topics/statistics/tutorials/FNN/tutorial.html&edit-text=&act=url" title="">
                And more!
                </a>
            </div>
        </div>

        <div class="dropdown-item">
            <div>
                <i class="fas fa-palette" aria-hidden="true"></i><span class="visually-hidden">gtn-theme</span> Theme
            </div>

            <div id="theme-selector" data-toggle="buttons">
                <label data-value="default" class="btn btn-secondary">
                    <input type="radio" name="options" id="default" autocomplete="off"> Default
                </label>
                <label data-value="night" class="btn btn-secondary">
                    <input type="radio" name="options" id="night" autocomplete="off"> Night
                </label>
                <label data-value="midnight" class="btn btn-secondary">
                    <input type="radio" name="options" id="midnight" autocomplete="off"> Midnight
                </label>
                <label data-value="rainbow" class="btn btn-secondary">
                    <input type="radio" name="options" id="rainbow" autocomplete="off"> Rainbow
                </label>
                <label data-value="progress" class="btn btn-secondary">
                    <input type="radio" name="options" id="progress" autocomplete="off">üè≥Ô∏è‚Äçüåà
                </label>
                <label data-value="halloween" class="btn btn-secondary">
                    <input type="radio" name="options" id="halloween" autocomplete="off"> üéÉ
                </label>
                <label data-value="straya" class="btn btn-secondary">
                    <input type="radio" name="options" id="downunder" autocomplete="off"> üá¶üá∫
                </label>
            </div>

        </div>

        <div class="dropdown-item">
            <div>
                <i class="fas fa-history" aria-hidden="true"></i><span class="visually-hidden">galaxy-rulebuilder-history</span> Previous Versions
            </div>

            
            <div id="archive-selector">
            
                <a class="btn btn-warning" href="https://training.galaxyproject.org/archive/2021-10-01/topics/statistics/tutorials/FNN/tutorial.html" title="Version 2021-10-01">2021-10-01</a>
            
                <a class="btn btn-warning" href="https://training.galaxyproject.org/archive/2021-09-01/topics/statistics/tutorials/FNN/tutorial.html" title="Version 2021-09-01">2021-09-01</a>
            
                <a class="btn btn-warning" href="https://training.galaxyproject.org/archive/2021-08-01/topics/statistics/tutorials/FNN/tutorial.html" title="Version 2021-08-01">2021-08-01</a>
            
                <a class="btn btn-warning" href="https://training.galaxyproject.org/archive/" title="Older Versions">Older Versions</a>
            </div>

        </div>

    </div>
</li>


                    <!-- Search bar-->
                    <li class="nav-item">
                      <div id="navbarSupportedContent" role="search">
                        <!-- Search form -->
                        <form class="form-inline mr-auto" method="GET" action="/training-material/search">
                          <i class="fas fa-search nav-link" aria-hidden="true"></i>
                          <div class="md-form mb-2">
                            <input name="query" class="form-control nicer" type="text" placeholder="Search Tutorials" aria-label="Search">
                          </div>
                        </form>
                      </div>
                    </li>
                </ul>
            </div>
        </div>
    </nav>
</header>

        <div class="container main-content" role="main">
        











<!-- Gitter -->




<script>
  ((window.gitter = {}).chat = {}).options = {
  room: 'Galaxy-Training-Network/Lobby'
  };
</script>
<script src="https://sidecar.gitter.im/dist/sidecar.v1.js" async defer></script>

<script type="application/ld+json">
    


{
  "@context": "http://schema.org",
  "@type": "Course",
  "accessMode": [
    "textual",
    "visual"
  ],
  "accessModeSufficient": [
    "textual",
    "visual"
  ],
  "accessibilityControl": [
    "fullKeyboardControl",
    "fullMouseControl"
  ],
  "accessibilityFeature": [
    "alternativeText",
    "tableOfContents"
  ],
  "accessibilitySummary": "Short descriptions are present but long descriptions will be needed for non-visual users",
  "audience": {
    "@type": "EducationalAudience",
    "educationalRole": "students"
  },
  "citation": {
    "@type": "CreativeWork",
    "name": "Community-Driven Data Analysis Training for Biology",
    "url": "https://doi.org/10.1016/j.cels.2018.05.012"
  },
  "copyrightHolder": {
    "@type": "Organization",
    "email": "galaxytrainingnetwork@gmail.com",
    "name": "Galaxy Training Network",
    "url": "https://galaxyproject.org/teach/gtn/"
  },
  "dateModified": "1970-01-01 00:33:41 +0000",
  "discussionUrl": "https://gitter.im/Galaxy-Training-Network/Lobby",
  "headline": "Deep Learning (Part 1) - Feedforward neural networks (FNN)",
  "interactivityType": "mixed",
  "isAccessibleForFree": true,
  "isFamilyFriendly": true,
  "license": "https://spdx.org/licenses/CC-BY-4.0.html",
  "producer": {
    "@type": "Organization",
    "email": "galaxytrainingnetwork@gmail.com",
    "name": "Galaxy Training Network",
    "url": "https://galaxyproject.org/teach/gtn/"
  },
  "provider": {
    "@type": "Organization",
    "email": "galaxytrainingnetwork@gmail.com",
    "name": "Galaxy Training Network",
    "url": "https://galaxyproject.org/teach/gtn/"
  },
  "sourceOrganization": {
    "@type": "Organization",
    "email": "galaxytrainingnetwork@gmail.com",
    "name": "Galaxy Training Network",
    "url": "https://galaxyproject.org/teach/gtn/"
  },
  "isPartOf": {
    "@type": "CreativeWork",
    "name": "Statistics and machine learning",
    "description": "Statistical Analyses for omics data and machine learning using Galaxy tools",
    "url": "https://training.galaxyproject.org//training-material/topics/statistics/"
  },
  "courseCode": "statistics / FNN / hands-on",
  "learningResourceType": "hands-on tutorial",
  "name": "Hands-on for 'Deep Learning (Part 1) - Feedforward neural networks (FNN)' tutorial",
  "url": "https://training.galaxyproject.org//training-material/topics/statistics/tutorials/FNN/tutorial.html",
  "timeRequired": "PT2H",
  "description": "The questions this  addresses are:\n - What is a feedforward neural network (FNN)?\n - What are some applications of FNN?\n\n\\nThe objectives are:\n - Understand the inspiration for neural networks\n - Learn various activation functions, and classification/regression problems solved by neural networks\n - Discuss various cost/loss functions and the backpropagation algorithm\n - Learn how to create a neural network using Galaxy's deep learning tools\n - Solve a simple regression problem, car purchase price prediction, via FNN in Galaxy\n\n",
  "inLanguage": {
    "@type": "Language",
    "name": "English",
    "alternateName": "en"
  },
  "coursePrerequisites": [
    {
      "@type": "CreativeWork",
      "url": "https://training.galaxyproject.org//training-material/topics/introduction/",
      "name": "Introduction to Galaxy Analyses",
      "description": "Introduction to Galaxy Analyses",
      "provider": {
        "@type": "Organization",
        "email": "galaxytrainingnetwork@gmail.com",
        "name": "Galaxy Training Network",
        "url": "https://galaxyproject.org/teach/gtn/"
      }
    },
    {
      "@type": "Course",
      "url": "https://training.galaxyproject.org//training-material/topics/statistics/tutorials/intro_deep_learning/tutorial.html",
      "name": "Introduction to deep learning",
      "description": "Hands-on for 'Introduction to deep learning' tutorial",
      "learningResourceType": "hands-on tutorial",
      "interactivityType": "expositive",
      "provider": {
        "@type": "Organization",
        "email": "galaxytrainingnetwork@gmail.com",
        "name": "Galaxy Training Network",
        "url": "https://galaxyproject.org/teach/gtn/"
      }
    }
  ],
  "hasPart": [

  ],
  "author": [
    {
      "@type": "Person",
      "name": "Kaivan Kamali"
    }
  ],
  "contributor": [
    {
      "@type": "Person",
      "name": "Kaivan Kamali"
    }
  ],
  "about": [
    {
      "@type": "CreativeWork",
      "name": "Statistics and machine learning",
      "description": "Statistical Analyses for omics data and machine learning using Galaxy tools",
      "url": "https://training.galaxyproject.org//training-material/topics/statistics/"
    },
    {
      "@type": "DefinedTerm",
      "@id": "http://edamontology.org/topic_2269",
      "inDefinedTermSet": "http://edamontology.org",
      "termCode": "topic_2269",
      "url": "https://bioportal.bioontology.org/ontologies/EDAM/?p=classes&conceptid=http%3A%2F%2Fedamontology.org%2Ftopic_2269"
    }
  ]
}
</script>

<section class="tutorial topic-statistics">
    <h1 data-toc-skip>Deep Learning (Part 1) - Feedforward neural networks (FNN)</h1>
    

    <div class="contributors-line">Authors: <a href="/training-material/hall-of-fame/kxk302/" class="contributor-badge contributor-kxk302"><img src="https://avatars.githubusercontent.com/kxk302?s=27" alt="Avatar">Kaivan Kamali</a></div>

    <blockquote class="overview">
        <h3>Overview</h3>
        
        <img alt="Creative Commons License" class="float-right" style="border-width:0; display: inline-block; margin:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" aria-hidden="true" />
        
        <strong><i class="far fa-question-circle" aria-hidden="true"></i> Questions:</strong>
        <ul>
        
        <li><p>What is a feedforward neural network (FNN)?</p>
</li>
        
        <li><p>What are some applications of FNN?</p>
</li>
        
        </ul>

        <strong><i class="fas fa-bullseye" aria-hidden="true"></i> Objectives: </strong>
        <ul>
        
        <li><p>Understand the inspiration for neural networks</p>
</li>
        
        <li><p>Learn various activation functions, and classification/regression problems solved by neural networks</p>
</li>
        
        <li><p>Discuss various cost/loss functions and the backpropagation algorithm</p>
</li>
        
        <li><p>Learn how to create a neural network using Galaxy‚Äôs deep learning tools</p>
</li>
        
        <li><p>Solve a simple regression problem, car purchase price prediction, via FNN in Galaxy</p>
</li>
        
        </ul>

        
        <strong><i class="fas fa-check-circle" aria-hidden="true"></i> Requirements:</strong>
        <ul>
        
    <li>
    
        
        
        <a href="/training-material/topics/introduction">Introduction to Galaxy Analyses</a>
        
    
    </li>

        
    <li>
    
        
        
        <a href="/training-material/topics/statistics">Statistics and machine learning</a>
        
            <ul>
                
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                            
                            <li> Introduction to deep learning:
                            
                            
                                
                                     <a href="/training-material/topics/statistics/tutorials/intro_deep_learning/tutorial.html"><i class="fas fa-laptop" aria-hidden="true"></i><span class="visually-hidden">tutorial</span> hands-on</a>
                                
                            
                            </li>
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                
            </ul>
        
    
    </li>

        </ul>
        

        
        <div><strong><i class="fas fa-hourglass-half" aria-hidden="true"></i> Time estimation:</strong> 2 hours</div>
        

        

        
        

        
        <div id="supporting-materials"><strong><i class="fa fa-external-link" aria-hidden="true"></i> Supporting Materials:</strong></div>
        <ul class="supporting_material">
            
                <li class="btn btn-default"><a href="/training-material/topics/statistics/tutorials/FNN/slides.html" title="Slides for this tutorial">
                    <i class="fab fa-slideshare" aria-hidden="true"></i> Slides
                </a></li>
            

            
                <li class="btn btn-default supporting_material">


<a class="topic-icon" href="https://zenodo.org/record/4660497">
    <i class="far fa-copy" aria-hidden="true"></i> Datasets
</a>

</li>
            

            
                <li class="btn btn-default supporting_material">


    <a class="topic-icon" href="/training-material/topics/statistics/tutorials/FNN/workflows/" title="Workflows" alt="Deep Learning (Part 1) - Feedforward neural networks (FNN) workflows">
        <i class="fas fa-share-alt" aria-hidden="true"></i> Workflows
    </a>

</li>
            

            

            

            
            
            

            
                <li class="btn btn-default supporting_material">


    <a href="#" class="btn btn-default dropdown-toggle topic-icon" data-toggle="dropdown" aria-expanded="false" title="Where to run the tutorial">
        <i class="fas fa-globe" aria-hidden="true"></i><span class="visually-hidden">instances</span> Available on these Galaxies 
    </a>
    <ul class="dropdown-menu">
    
        <li>
            <a class="dropdown-item" href="https://github.com/galaxyproject/training-material/tree/main/topics/statistics/docker" title="Docker image for this tutorial">
                <i class="fab fa-docker" aria-hidden="true"></i><span class="visually-hidden">docker_image</span> Docker image
            </a>
        </li>
    
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
        <a class="dropdown-item" href="https://africa.usegalaxy.eu/" title="Galaxy Africa">
            Galaxy Africa
        </a>
        
    
        
        <a class="dropdown-item" href="https://india.usegalaxy.eu/" title="Galaxy India">
            Galaxy India
        </a>
        
    
        
    
        
        <a class="dropdown-item" href="https://test.galaxyproject.org/" title="Galaxy Test">
            Galaxy Test
        </a>
        
    
        
        <a class="dropdown-item" href="https://ecology.usegalaxy.eu/" title="Galaxy for Ecology">
            Galaxy for Ecology
        </a>
        
    
        
        <a class="dropdown-item" href="https://galaxy.genouest.org" title="Galaxy@GenOuest">
            Galaxy@GenOuest
        </a>
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
        <a class="dropdown-item" href="https://streetscience.usegalaxy.eu/" title="Street Science">
            Street Science
        </a>
        
    
        
        <a class="dropdown-item" href="https://usegalaxy.be/" title="UseGalaxy.be">
            UseGalaxy.be
        </a>
        
    
        
        <a class="dropdown-item" href="https://usegalaxy.eu" title="UseGalaxy.eu">
            UseGalaxy.eu
        </a>
        
    
        
    
        
        <a class="dropdown-item" href="https://usegalaxy.no/" title="UseGalaxy.no">
            UseGalaxy.no
        </a>
        
    
        
        <a class="dropdown-item" href="https://usegalaxy.org" title="UseGalaxy.org (Main)">
            UseGalaxy.org (Main)
        </a>
        
    
        
        <a class="dropdown-item" href="https://usegalaxy.org.au" title="UseGalaxy.org.au">
            UseGalaxy.org.au
        </a>
        
    
        
    
        
    
        
    
        
    
    </ul>


</li>
            
        </ul>
        

        <div><strong><i class="far fa-calendar" aria-hidden="true"></i> Last modification:</strong> Jun 26, 2021 </div>
        <div><strong><i class="fas fa-balance-scale" aria-hidden="true"></i> License:</strong>
            
            <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Tutorial Content is licensed under Creative Commons Attribution 4.0 International License</a>
            
            <a rel="license" href="https://github.com/galaxyproject/training-material/blob/main/LICENSE.md">The GTN Framework is licensed under MIT</a>
        </div>
    </blockquote>

    <div class="container">
        <div class="row">
            <!-- sidebar, which will move to the top on a small screen -->
            <div class="col-sm-2">
                <nav id="toc" data-toggle="toc" class="sticky-top" aria-label="Table of Contents"></nav>
            </div>
            <div class="col-sm-10">
                

                <h1 class="no_toc" id="introduction">Introduction</h1>

<p>Artificial neural networks are a machine learning discipline roughly inspired by how neurons in a
human brain work. In the past decade, there has been a huge resurgence of neural networks thanks
to the vast availability of data and enormous increases in computing capacity (Successfully
training complex neural networks in some domains requires lots of data and compute capacity). There
are various types of neural networks (Feedforward, recurrent, etc.). In this tutorial, we discuss
feedforward neural networks (FNN), which have been successfully applied to pattern classification,
clustering, regression, association, optimization, control, and forecasting (<span class="citation"><a href="#JainEtAl">Jain <i>et al.</i> 1996</a></span>).
We will discuss biological neurons that inspired artificial neural networks, review activation
functions, classification/regression problems solved by neural networks, and the backpropagation
learning algorithm. Finally, we construct a FNN to solve a regression problem using car purchase
price prediction dataset.</p>

<blockquote class="agenda">
  <h3 id="agenda">Agenda</h3>

  <p>In this tutorial, we will cover:</p>

<ol id="markdown-toc">
  <li><a href="#inspiration-for-artificial-neural-networks" id="markdown-toc-inspiration-for-artificial-neural-networks">Inspiration for artificial neural networks</a></li>
  <li><a href="#perceptron" id="markdown-toc-perceptron">Perceptron</a></li>
  <li><a href="#activation-functions" id="markdown-toc-activation-functions">Activation functions</a></li>
  <li><a href="#supervised-learning" id="markdown-toc-supervised-learning">Supervised learning</a></li>
  <li><a href="#losscost-function" id="markdown-toc-losscost-function">Loss/Cost function</a></li>
  <li><a href="#backpropagation-learning-algorithm" id="markdown-toc-backpropagation-learning-algorithm">Backpropagation Learning algorithm</a></li>
  <li><a href="#get-data" id="markdown-toc-get-data">Get Data</a></li>
  <li><a href="#solve-a-simple-regression-problem-using-car-purchase-price-prediction-dataset-via-fnn-in-galaxy" id="markdown-toc-solve-a-simple-regression-problem-using-car-purchase-price-prediction-dataset-via-fnn-in-galaxy">Solve a simple regression problem using car purchase price prediction dataset via FNN in <span class="notranslate">Galaxy</span></a></li>
</ol>

</blockquote>

<h1 id="inspiration-for-artificial-neural-networks">Inspiration for artificial neural networks</h1>

<p>A neuron is a special biological cell with information processing ability (<span class="citation"><a href="#JainEtAl">Jain <i>et al.</i> 1996</a></span>).
Figure 1 shows a biological neuron. It has a cell body and two outreaching tree-like branches:
axon and dendrites. A neuron receives signals from other neurons through its dendrites, and
transmits signals generated by its cell body to other neurons via its axon. A synapse is a place
of contact between two neurons, an axon strand of one neuron and a dendrite strand of another
neuron. A synapse can either enhance or inhibit the signal that passes through it. Learning occurs
by changing the effectiveness of synapse. If the signals received exceeds a threshold, the neuron
<em>fires</em>, i.e., it transmits a signal to other neurons. If not, it will not fire.</p>

<figure id="figure-1"><img src="../../images/FNN_bio_neuron.png" alt="Sketch of a biological neuron and its components. " loading="lazy" /><figcaption><span class="figcaption-prefix">Figure 1:</span> A biological neuron (<span class="citation"><a href="#JainEtAl">Jain <i>et al.</i> 1996</a></span>)</figcaption></figure>

<p>Cerebral cortex, the outer most layer of the brain, is a sheet of neurons about 2 to 3 mm thick,
with a surface area of about 2,200 \(cm^{2}\). Cerebral cortex has about \(10^{11}\) neurons.
Each neuron is connected to \(10^{3} to 10^{4}\) neurons. Hence, a human brain has around
 \(10^{14} to 10^{15}\) connections. Neurons communicate by a short train of signals, usually
milliseconds in duration. The frequency in which the signals are transmitted can be up to several
hundred Hertz, which is millions of times slower than an electronic circuit. However, complex tasks,
such as face recognition are made within a few hundred milliseconds. This implies that computation
involved cannot take more than 100 serial steps, i.e., brain runs parallel programs that are about
100 steps long for such complex perceptual tasks. The amount of information sent from one neuron to
another is also very small. This implies that critical information is not transmitted directly, but
is captured by the interconnections. What enables slow computing elements in the brain to perform
complex tasks so quickly is the distributed computation and representation nature of the brain (<span class="citation"><a href="#JainEtAl">Jain <i>et al.</i> 1996</a></span>).</p>

<h1 id="perceptron">Perceptron</h1>

<p>Perceptron (<span class="citation"><a href="#Rosenblatt">Rosenblatt 1957</a></span>) is the oldest neural network still in use today. It‚Äôs a form of a
feedforward neural network, in which the connections between the nodes do not form a loop. It accepts
multiple inputs, each input is multiplied by a weight, and the products are added up. The weights
simulate the role of synapse in biological neurons (to enhance or inhibit a signal). A <em>bias</em> value
is then added to the result before it is passed to an <em>activation function</em>. An activation function
simulates the neuron firing or not. For example, in a <em>binary step</em> activation function, if the sum of
weighted inputs and bias is greater than zero, the neuron output is 1 (it fires). Else, the neuron
output is 0 (it does not fire). Bias allows us to shift the activation function.</p>

\[f(x) =
\left\{
	\begin{array}{ll}
		1  &amp; \mbox{if } \boldsymbol{x} \cdot \boldsymbol{w} + b \geq 0 \\
		0  &amp; \mbox{otherwise }
	\end{array}
\right.\]

<p>Figure 2 shows a Perceptron, a single layer FNN, where the input is 3 dimensional (input layer has
3 nodes), and output is 1 dimensional (output layer has 1 node).</p>

<figure id="figure-2"><img src="../../images/FFNN_no_hidden.png" alt="Neurons forming the input and output layers of a single layer feedforward neural network. " loading="lazy" /><figcaption><span class="figcaption-prefix">Figure 2:</span> A perceptron</figcaption></figure>

<p>In supervised learning, we are given a set of input-output pairs, called the <em>training
set</em>. Given the training set, the learning algorithm (iteratively) adjusts the model
parameters (weights and biases), so that the model can accurately map inputs to outputs.
The learning algorithm for Perceptron is very simple and reduces the weights (via a
small learning rate multiplier) if the predicted output is more than the expected output
and increases them otherwise (<span class="citation"><a href="#Rosenblatt">Rosenblatt 1957</a></span>).</p>

<p>Minsky and Papert showed that a single layer FNN cannot solve problems in which the data is not
linearly separable, such as the XOR problem (<span class="citation"><a href="#Newell780">Newell 1969</a></span>). Adding one (or more) hidden
layers to FNN enables it to solve problems in which data is non-linearly separable. Per Universal
Approximation Theorem, a FNN with one hidden layer can represent any function (<span class="citation"><a href="#Cybenko1989">Cybenko 1989</a></span>),
although in practice training such a model is very difficult (if not impossible), hence, we usually
add multiple hidden layers to solve complex problems.</p>

<figure id="figure-3"><img src="../../images/FFNN.png" alt="Neurons forming the input, output, and hidden layers of a multi-layer feedforward neural network. " loading="lazy" /><figcaption><span class="figcaption-prefix">Figure 3:</span> Feedforward neural network with a hidden layer. Biases to hidden/output layer neurons are omitted for clarity</figcaption></figure>

<p>The problem with multi-layer FNN was lack of a learning algorithm, as the Perceptron‚Äôs learning
algorithm could not be extended to multi-layer FNN. This along with Minsky and Papert highlighting
the limitations of Perceptron resulted in sudden drop in interest in neural networks (referred to
as <em>AI winter</em>). In the 80‚Äôs the backpropagation algorithm was proposed (<span class="citation"><a href="#Rumelhart1986">Rumelhart <i>et al.</i> 1986</a></span>),
which enabled learning in multi-layer FNN and resulted in a renewed interest in the field.</p>

<p>In a multi-layer neural network, we have an input layer, an output layer, and one or more hidden layers
(between input and output layers). The input layer has as many neurons as the dimension of the input
data. The number of neurons in the output layer depends on the type of the problem the neural network
is trying to solve (See Supervised learning section below). The more hidden layers that we have (and
the more neurons we have in each hidden layer), our neural network can estimate more complex functions.
However, this comes at the cost of increased training time (due to increased number of parameters) and
increased likelihood of <em>overfitting</em>. Overfitting happens when a model captures the details of the
training data, performs well on the training data, but is unable to perform well on data not used in
the training. The neural network, hence, cannot <em>generalize</em> to unseen data. There are regularization
techniques that can prevent that (<span class="citation"><a href="#KukackaEtAl">Kukacka <i>et al.</i> 2017</a></span>) but they are outside the scope of this tutorial.</p>

<h1 id="activation-functions">Activation functions</h1>

<p>There are many activation functions besides the step function used in Perceptron (<span class="citation"><a href="#nwankpaEtAl">Nwankpa <i>et al.</i> 2018</a></span>). Figure 4 shows
some of the more common activation functions.</p>

<figure id="figure-4"><img src="../../images/FNN_activation_functions.png" alt="Table showing the formula, graph, derivative, and range of common activation functions. " loading="lazy" /><figcaption><span class="figcaption-prefix">Figure 4:</span> Common activation functions (Source: https://en.wikipedia.org/wiki/Activation_function)</figcaption></figure>

<p>Linear activation function is used in the output layer of a network when we have a regression problem. It does
not make sense to use it in all layers, as such multi-layer network can be reduced to a single layer network.
Also, networks with linear activation functions cannot model non-linear relationships between input and output.</p>

<p>Binary step activation function is used in Perceptron. It cannot be used in multi-layers networks as they use
back propagation learning algorithm, which changes network weights/biases based on the derivative
of the activation function, which is zero. Hence, there would no weights/biases updates in back propagation.</p>

<p>Sigmoid activation function can be used both at the output layer and hidden layers of a multilayer network. They
allow the network to model non-linear relationships between input and output. The problem with Sigmoid activation
function is that the derivative values away from the origin are very small and quickly approach zero. In a multi
layer network, in order to calculate weight updates in layers closer to the input layer, we use the chain rule
which requires multiplying multiple Sigmoid derivative values (formula given in Backpropagation learning algorithm
section below). Multiplying multiple small numbers results in a <em>very</em> small number, meaning that the weight updates
will be minimal and the learning algorithm will be very slow. This is known as the <em>vanishing gradient</em> problem. In
networks with many hidden layers (so called <em>deep networks</em>), we generally avoid Sigmoid and use ReLU activation
function.</p>

<p>Hyperbolic tangent (or tanh), similar to Sigmoid function, is a soft step function. But its range is between -1
and 1 (instead of 0 and 1). One benefit of tanh over Sigmoid is that its derivative values are larger, so it
suffers less from the vanishing gradient problem.</p>

<p>Finally, ReLU (Rectified Linear Unit) is an activation function popular is deep neural networks. Since it
does not suffer from vanishing gradient problem, it is preferred to Sigmoid or tanh. Sigmoid or tanh can
still be used in the output layer of deep networks.</p>

<h1 id="supervised-learning">Supervised learning</h1>

<p>In supervised learning a <em>training set</em> is defined as
\({(\boldsymbol{x^{(1)}}, \boldsymbol{y^{(1)}}), ((\boldsymbol{x^{(2)}}, \boldsymbol{y^{(2)}}), ..., ((\boldsymbol{x^{(m)}}, \boldsymbol{y^{(m)}})}\)
and each pair \((\boldsymbol{x^{(i)}}, \boldsymbol{y^{(i)}})\) is called a <em>training example</em>.
<em>m</em> is the number of training examples and \(\boldsymbol{x^{(i)}}\) is called <em>feature vector</em>
or <em>input vector</em>.  Each element of the vector is called a <em>feature</em>. Each \(\boldsymbol{x^{(i)}}\)
corresponds to a label \(\boldsymbol{y^{(i)}}\). We assume there is an unknown function
\(\boldsymbol{y} = f(\boldsymbol{x})\) that maps the feature vectors to labels. The goal of
supervised learning is to use the training set to learn or estimate <em>f</em>. We call this estimated
function \(\hat{f}(\boldsymbol{x})\). We want \(\hat{f}(\boldsymbol{x})\) to be close to
\(f(\boldsymbol{x})\) not only for training set, but for training examples not in training
set (<span class="citation"><a href="#Bagheri">Bagheri 2020</a></span>).</p>

<p>When the label is a numerical variable, we call the problem a <em>regression</em> problem, and when it‚Äôs a categorical variable,
we call it a <em>classification</em> problem. In classification problems, the label can be represented by the set
\(\boldsymbol{y^{i}} \in {1,2,...,c}\), where each number is a class label and <em>c</em> is the number of classes.
If <em>c</em>=2, the class labels are mutually exclusive, we call it a <em>binary classification</em> problem. If <em>c</em> &gt; 2, and
the labels are mutually exclusive, we call <em>multiclass classification</em> problem. If labels are <em>not</em> mutually exclusive,
we call it a <em>multilabel classification</em> problem (<span class="citation"><a href="#Bagheri">Bagheri 2020</a></span>).</p>

<p>We use a method called <em>one-hot encoding</em> to convert binary and multiclass classification class label numbers into
binary values. We convert the scalar label <em>y</em> into a vector \(\boldsymbol{y}\) which has <em>c</em> elements. When y is
equal to k, the k-th element of \(\boldsymbol{y}\) is one and all other elements are zero. When labels are <em>not</em>
mutually exclusive, we use another method called <em>multi-hot encoding</em>. Suppose we are doing a multilabel image
classification, where an image can have a dog, panda, or cat in it. We represent the label by a vector of 3, and if
dog and cat are present in the image, first and third element of the vector are one and the second element is zero (<span class="citation"><a href="#Bagheri">Bagheri 2020</a></span>).</p>

<figure id="figure-5"><img src="../../images/FNN_output_encoding.png" alt="Three images illustrating binary, multiclass, and multilabel classifications and their label representation. " loading="lazy" /><figcaption><span class="figcaption-prefix">Figure 5:</span> Examples of binary, multiclass, and multilabel classifications and their label representation (<span class="citation"><a href="#Bagheri">Bagheri 2020</a></span>)</figcaption></figure>

<p>Figure 5 shows examples of binary, multiclass, and multilabel classification problems and their associated one-hot
encoded or multi-hot encoded labels. The output layer of a neural network for binary classification usually has a
single neuron with Sigmoid activation function. If the neuron‚Äôs output is greater than 0.5, we assume the output is 1,
and otherwise, we assume the output is 0. For multilabel classification problems, the output layer of the neural network
usually has as many neurons as the number of classes and the neurons use Sigmoid activation function. Again, we use a
threshold of 0.5 to determine whether the output of each neuron is 1 or 0. For multiclass classification problems, the
output layer usually has as many neurons as the number of classes. However, instead of Sigmoid,  we use a <em>Softmax</em>
activation function, which takes the input to all the neurons in the output layer, and creates a probability distribution,
so, the sum of outputs of all output layer neurons adds up to 1. The neuron with the highest probability denotes the predicted
label.</p>

<h1 id="losscost-function">Loss/Cost function</h1>

<p>During training, for each training example in the training set \((\boldsymbol{x^{(i)}}, \boldsymbol{y^{(i)}})\), we
present the feature vector \(\boldsymbol{x^{(i)}}\) to the neural network, and compare the network‚Äôs predicted output
\(\boldsymbol{\hat{y}}\) with the corresponding label \(\boldsymbol{y^{(1)}}\). We need to define a <strong>loss function</strong>
to objectively measure how much the network‚Äôs predicted output is different than the expected output (the corresponding
label). We use the <strong>cross entropy</strong> loss function for classification problems, and <em>quadratic</em> loss function for
regression problems.</p>

<p>For multiclass classification problems, the cross entropy is calculated as below</p>

\[\mathcal{L}(\boldsymbol{\hat{y}^{(j)}}, \boldsymbol{y^{(j)}}) = - \sum_{i=1}^{c} \boldsymbol{y_{i}^{(j)}}ln(\boldsymbol{\hat{y}_{i}^{(j)}})\]

<p>You can find the cross entropy formula for binary and multilabel classifications in <span class="citation"><a href="#Bagheri">Bagheri 2020</a></span>. They are just special
cases of multiclass cross entropy and are not give here for the sake of brevity.</p>

<p>The loss function is calculated for each training example in the training set. The average of the calculated loss functions
for all training examples in the training set is the <strong>cost function</strong>. For multiclass classification problems, the cost function
is calculated as below (again refer to <span class="citation"><a href="#Bagheri">Bagheri 2020</a></span> for binary classification and multilabel classification formulas).</p>

\[J(\boldsymbol{W}, \boldsymbol{b}) = - \frac{1}{m} \sum_{j=1}^{m} \sum_{i=1}^{c} \boldsymbol{y_{i}^{(j)}}ln(\boldsymbol{\hat{y}_{i}^{(j)}})\]

<p>For regression problems, the quadratic loss function is calculated as below:</p>

\[\mathcal{L}(\boldsymbol{\hat{y}^{(j)}}, \boldsymbol{y^{(j)}}) = \frac{1}{2} \| \boldsymbol{y^{(j)}} - \boldsymbol{\hat{y}^{(j)}} \| ^ 2\]

<p>Similarly, the <em>quadratic</em> cost function (or <em>Mean Squared Error (MSE)</em>) is the average of the calculated loss functions
for all training examples in the training set.</p>

\[J(\boldsymbol{W}, \boldsymbol{b}) = \frac{1}{2m} \sum_{j=1}^{m} \| \boldsymbol{y^{(j)}} - \boldsymbol{\hat{y}^{(j)}} \| ^ 2\]

<h1 id="backpropagation-learning-algorithm">Backpropagation Learning algorithm</h1>

<p>The <strong>backpropagation</strong> algorithm <span class="citation"><a href="#Rumelhart1986">Rumelhart <i>et al.</i> 1986</a></span> is a gradient descent technique. Gradient descent aims to find
a local minimum of a function by iteratively moving in the opposite direction of the gradient (i.e., the slope) of the
function at the current point. The goal of a learning in neural networks is to minimize the cost function given the
training set. The cost function is a function of network weights and biases of all the neurons in all the layers.
Backpropagation iteratively computes the gradient of cost function relative to each weight and bias, then updates
the weights and biases in the opposite direction of the gradient, to find a local minimum.</p>

<p>In order to specify the formula for backpropagation, we need to define the error of the \(i^{th}\) neuron in \(l^{th}\)
layer of a network for the \(j^{th}\) training example as follows (where \(z_{i}^{[l](j)}\) is the weighted some of
input to the neuron, and \(\mathcal{L}\) is the loss function):</p>

\[\delta_{i}^{[l](j)} = \frac{\partial \mathcal{L}(\boldsymbol{\hat{y}^{(j)}}, \boldsymbol{y^{(j)}})}{\partial z_{i}^{[l](j)}}\]

<p>Backpropagation formulas are expressed in terms of the error defined above. Full derivation of the formulas below is outside
the scope of this tutorial (Repeated use of chain rule is needed. Please refer to the excellent article by <span class="citation"><a href="#Bagheri">Bagheri 2020</a></span>
for details). Note that in formulas below L denotes the output layer, g the activation function, \(\nabla\) the gradient,
\(W^{[l]^{T}}\) layer l weights transposed, \(b_{i}^{l}\) bias of neuron i at layer l, \(w_{ik}^{l}\) weight to neuron
i at layer l from neuron k from layer l-1, and \(a_{k}^{[l-1](j)}\) activation of neuron k at layer l-1 for training example j.</p>

\[\boldsymbol{\delta}^{[L](j)} = \nabla_{\boldsymbol{\hat{y}^{(j)}}}\mathcal{L} \odot (g^{[L]})^{'} (\boldsymbol{z}^{[L](j)}) = \boldsymbol{\hat{y}^{(j)}} - \boldsymbol{y^{(j)}}\]

\[\boldsymbol{\delta}^{[l](j)} = W^{[l+1]^{T}} \boldsymbol{\delta}^{[l+1](j)}  \odot (g^{[l]})^{'} (\boldsymbol{z}^{[l](j)})\]

\[\frac{\partial L}{\partial b_{i}^{[l]}} = \boldsymbol{\delta}_{i}^{[l](j)}\]

\[\frac{\partial L}{\partial w_{ik}^{[l]}} = \boldsymbol{\delta}_{i}^{[l](j)} a_{k}^{[l-1](j)}\]

<p>As you can see, we can calculate the error at the output layer for sample <em>j</em> using the first equation. Afterwards, we can calculate
the error in the layer right before the output layer for sample <em>j</em> using the second equation. The second equation is recursive,
meaning that we can calculate the error in any layer, given the error values for the next layer. This backward calculation of the
errors is the reason this algorithm is called <em>backpropagation</em>.</p>

<p>After the error values for all the layers are calculated for sample <em>j</em>, we use the third and fourth equations to calculate the gradient
of loss function relative to biases and weights for sample <em>j</em>. We can repeat these steps for all the samples, average the gradients of
the loss function relative to biases and weights and use the average value to update the biases and weights. This is called <em>batch
gradient descent</em>. If we have too many samples, such calculations will take a long time. An alte<span class="notranslate">rna</span>tive is to update the biases/weights
based on the gradient of each sample. This is called <em>stochastic gradient descent</em>. While this is much faster than batch gradient descent,
the gradient calculated based on a single sample is not a good estimate of the gradient calculated in the batch version of the algorithm.
A middle ground solution is to calculate the gradient of a <em>batch</em>, and update the biases and weights based on the average of the gradients
in the batch. This is called <em>mini-batch gradient descent</em> and is preferred to the other two variations of the algorithm.</p>

<p>Also, note that in the second equation which is recursive, we have a term that is the derivative of the activation function for that
layer. The recursive nature of this equation means, calculating the error values in the layer prior to the output layer requires 1
multiplication by the derivative value; calculating the error values in two (or more) layers before the output layer requires 2 (or more)
multiplication by the derivative values. If these derivative values are small, as could be the case for the Sigmoid function, the product
of multiple small values will result in a <em>very</em> small value (e.g., 0.001). Since these error values decide the updates for biases and weights, this
means the update to biases and weights in layers closer to the input layer will be very small, slowing the learning algorithm to a halt.
This phenomenon is known as the <em>vanishing gradient</em> problem and is the reason Sigmoid function cannot be used in very deep networks (And
why ReLU is so popular in deep networks).</p>

<h1 id="get-data">Get Data</h1>

<blockquote class="notranslate hands_on">
  <h3 id="hands_on-hands-on-data-upload"><i class="fas fa-pencil-alt" aria-hidden="true"></i><span class="visually-hidden">hands_on</span> Hands-on: Data upload</h3>

  <ol>
    <li>
      <p>Create a new history for this tutorial</p>

      <!--SNIPPET-->
      <blockquote class="tip">  <h3 data-toc-skip="" id="-tip-creating-a-new-history"><i class="far fa-lightbulb" aria-hidden="true"></i><span class="visually-hidden"></span> Tip: Creating a new history</h3>  <p>Click the <i class="fas fa-plus" aria-hidden="true"></i><span class="visually-hidden">new-history</span> icon at the top of the history panel.</p>  <p>If the <i class="fas fa-plus" aria-hidden="true"></i><span class="visually-hidden">new-history</span> is missing:</p>  <ol>  <li>Click on the <i class="fas fa-cog" aria-hidden="true"></i><span class="visually-hidden"><span class="notranslate">galaxy</span>-gear</span> icon (<strong>History options</strong>) on the top of the history panel</li>  <li>Select the option <strong>Create New</strong> from the menu</li></ol></blockquote>
    </li>
    <li>
      <p>Import the files from <a href="https://zenodo.org/record/4660497">Zenodo</a> or from the shared data library</p>

      <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>https://zenodo.org/record/4660497/files/X_test.tsv
https://zenodo.org/record/4660497/files/X_train.tsv
https://zenodo.org/record/4660497/files/y_test.tsv
https://zenodo.org/record/4660497/files/y_train.tsv
</code></pre></div>      </div>

      <!--SNIPPET-->
      <blockquote class="tip">  <h3 data-toc-skip="" id="-tip-importing-via-links"><i class="far fa-lightbulb" aria-hidden="true"></i><span class="visually-hidden"></span> Tip: Importing via links</h3>  <ul>  <li>Copy the link location</li>  <li>    <p>Open the <span class="notranslate">Galaxy</span> Upload Manager (<i class="fas fa-upload" aria-hidden="true"></i><span class="visually-hidden"><span class="notranslate">galaxy</span>-upload</span> on the top-right of the tool panel)</p>  </li>  <li>Select <strong>Paste/Fetch Data</strong></li>  <li>    <p>Paste the link into the text field</p>  </li>  <li>    <p>Press <strong>Start</strong></p>  </li>  <li>    <p><strong>Close</strong> the window</p>  </li>  <li>By default, <span class="notranslate">Galaxy</span> uses the URL as the name, so rename the files with a more useful name.</li></ul></blockquote>

      <!--SNIPPET-->
      <blockquote class="tip">  <h3 data-toc-skip="" id="-tip-importing-data-from-a-data-library"><i class="far fa-lightbulb" aria-hidden="true"></i><span class="visually-hidden"></span> Tip: Importing data from a data library</h3>  <p>As an alte<span class="notranslate">rna</span>tive to uploading the data from a URL or your computer, the files may also have been made available from a <em>shared data library</em>:</p>  <ul>  <li>Go into <strong>Shared data</strong> (top panel) then <strong>Data libraries</strong></li>  <li>Navigate to the correct folder as indicated by your instructor</li>  <li>Select the desired files</li>  <li>Click on the <strong>To History</strong> button near the top and select <strong>as Datasets</strong> from the dropdown menu</li>  <li>In the pop-up window, select the history you want to import the files to (or create a new one)</li>  <li>Click on <strong>Import</strong></li></ul></blockquote>
    </li>
    <li>
      <p>Rename the datasets as <code class="language-plaintext highlighter-rouge">X_test</code>, <code class="language-plaintext highlighter-rouge">X_train</code>, <code class="language-plaintext highlighter-rouge">y_test</code>, and <code class="language-plaintext highlighter-rouge">y_train</code> respectively.</p>

      <!--SNIPPET-->
      <blockquote class="tip">  <h3 data-toc-skip="" id="-tip-renaming-a-dataset"><i class="far fa-lightbulb" aria-hidden="true"></i><span class="visually-hidden"></span> Tip: Renaming a dataset</h3>  <ul>  <li>Click on the <i class="fas fa-pencil-alt" aria-hidden="true"></i><span class="visually-hidden"><span class="notranslate">galaxy</span>-pencil</span> <strong>pencil icon</strong> for the dataset to edit its attributes</li>  <li>In the central panel, change the <strong>Name</strong> field</li>  <li>Click the <strong>Save</strong> button</li></ul></blockquote>
    </li>
    <li>
      <p>Check that the datatype of all the four datasets is <code class="language-plaintext highlighter-rouge">tabular</code>. If not, change the dataset‚Äôs datatype to tabular.</p>

      <!--SNIPPET-->
      <blockquote class="tip">  <h3 data-toc-skip="" id="-tip-changing-the-datatype"><i class="far fa-lightbulb" aria-hidden="true"></i><span class="visually-hidden"></span> Tip: Changing the datatype</h3>  <ul>  <li>Click on the <i class="fas fa-pencil-alt" aria-hidden="true"></i><span class="visually-hidden"><span class="notranslate">galaxy</span>-pencil</span> <strong>pencil icon</strong> for the dataset to edit its attributes</li>  <li>In the central panel, click on the <i class="fas fa-database" aria-hidden="true"></i><span class="visually-hidden"><span class="notranslate">galaxy</span>-chart-select-data</span> <strong>Datatypes</strong> tab on the top</li>  <li>Select <code class="language-plaintext highlighter-rouge">tabular</code></li>  <li>Click the <strong>Save</strong> button</li></ul></blockquote>
    </li>
  </ol>

</blockquote>

<h1 id="solve-a-simple-regression-problem-using-car-purchase-price-prediction-dataset-via-fnn-in-galaxy">Solve a simple regression problem using car purchase price prediction dataset via FNN in <span class="notranslate">Galaxy</span></h1>

<p>In this section, we define a FNN (<span class="citation"><a href="#Python">Python and tutorials 2018</a></span>) and train it using car purchase price prediction dataset (<span class="citation"><a href="#Grogan">Grogan 2020</a></span>). Given 5 attributes
about an individual (age, gender, average miles driven per day, personal debt, and monthly income), and the money they spent on purchasing
a car, the goal is to learn a model such that given an individual‚Äôs attributes, we can accurately predict how much money they are will spend
purchasing a car. We then evaluate the trained FNN on the test dataset and plot various graphs to assess the model‚Äôs performance. Our training
dataset has 723 training examples, and our test dataset has 242 test examples. Input features have been scaled to be in 0 to 1 range.</p>

<h3 id="create-a-deep-learning-model-architecture"><strong>Create a deep learning model architecture</strong></h3>

<blockquote class="notranslate hands_on">
  <h3 id="hands_on-hands-on-model-config"><i class="fas fa-pencil-alt" aria-hidden="true"></i><span class="visually-hidden">hands_on</span> Hands-on: Model config</h3>

  <ul>
    <li><span class="tool" data-tool="toolshed.g2.bx.psu.edu/repos/bgruening/keras_model_config/keras_model_config/0.4.2" title="Tested with toolshed.g2.bx.psu.edu/repos/bgruening/keras_model_config/keras_model_config/0.4.2"><strong>Create a deep learning model architecture</strong> <i class="fas fa-wrench" aria-hidden="true"></i><i aria-hidden="true" class="fas fa-cog"></i><span class="visually-hidden">Tool: <span class="notranslate">toolshed</span>.g2.bx.psu.edu/repos/bgruening/keras_model_config/keras_model_config/0.4.2</span></span>
      <ul>
        <li><em>‚ÄúSelect keras model type‚Äù</em>: <code class="language-plaintext highlighter-rouge">sequential</code></li>
        <li><em>‚Äúinput_shape‚Äù</em>: <code class="language-plaintext highlighter-rouge">(5,)</code></li>
        <li>In <em>‚ÄúLAYER‚Äù</em>:
          <ul>
            <li><i class="far fa-plus-square" aria-hidden="true"></i><span class="visually-hidden">param-repeat</span> <em>‚Äú1: LAYER‚Äù</em>:
              <ul>
                <li><em>‚ÄúChoose the type of layer‚Äù</em>: <code class="language-plaintext highlighter-rouge">Core -- Dense</code>
                  <ul>
                    <li><em>‚Äúunits‚Äù</em>‚Äù: <code class="language-plaintext highlighter-rouge">12</code></li>
                    <li><em>‚ÄúActivation function‚Äù</em>: <code class="language-plaintext highlighter-rouge">relu</code></li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><i class="far fa-plus-square" aria-hidden="true"></i><span class="visually-hidden">param-repeat</span> <em>‚Äú2: LAYER‚Äù</em>:
              <ul>
                <li><em>‚ÄúChoose the type of layer‚Äù</em>: <code class="language-plaintext highlighter-rouge">Core -- Dense</code>
                  <ul>
                    <li><em>‚Äúunits‚Äù</em>‚Äù: <code class="language-plaintext highlighter-rouge">8</code></li>
                    <li><em>‚ÄúActivation function‚Äù</em>: <code class="language-plaintext highlighter-rouge">relu</code></li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><i class="far fa-plus-square" aria-hidden="true"></i><span class="visually-hidden">param-repeat</span> <em>‚Äú3: LAYER‚Äù</em>:
              <ul>
                <li><em>‚ÄúChoose the type of layer‚Äù</em>: <code class="language-plaintext highlighter-rouge">Core -- Dense</code>
                  <ul>
                    <li><em>‚Äúunits‚Äù</em>‚Äù: <code class="language-plaintext highlighter-rouge">1</code></li>
                    <li><em>‚ÄúActivation function‚Äù</em>: <code class="language-plaintext highlighter-rouge">linear</code></li>
                  </ul>
                </li>
              </ul>
            </li>
          </ul>
        </li>
        <li>Click <em>‚ÄúExecute‚Äù</em></li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>Input has 5 attributes: age, gender, average miles driven per day, personal debt, and monthly income. Our neural network has 3 layers. All
three layers are fully connected. The last layer has a single neuron with a linear activation function, used in regression problems. Prior
layers use ReLU activation function. The model config can be downloaded as a JSON file.</p>

<h3 id="create-a-deep-learning-model"><strong>Create a deep learning model</strong></h3>

<blockquote class="notranslate hands_on">
  <h3 id="hands_on-hands-on-model-builder-optimizer-loss-function-and-fit-parameters"><i class="fas fa-pencil-alt" aria-hidden="true"></i><span class="visually-hidden">hands_on</span> Hands-on: Model builder (Optimizer, loss function, and fit parameters)</h3>

  <ul>
    <li><span class="tool" data-tool="toolshed.g2.bx.psu.edu/repos/bgruening/keras_model_builder/keras_model_builder/0.4.2" title="Tested with toolshed.g2.bx.psu.edu/repos/bgruening/keras_model_builder/keras_model_builder/0.4.2"><strong>Create deep learning model</strong> <i class="fas fa-wrench" aria-hidden="true"></i><i aria-hidden="true" class="fas fa-cog"></i><span class="visually-hidden">Tool: <span class="notranslate">toolshed</span>.g2.bx.psu.edu/repos/bgruening/keras_model_builder/keras_model_builder/0.4.2</span></span>
      <ul>
        <li><em>‚ÄúChoose a building mode‚Äù</em>: <code class="language-plaintext highlighter-rouge">Build a training model</code></li>
        <li><em>‚ÄúSelect the dataset containing model configuration‚Äù</em>: Select the <em>Keras Model Config</em> from the previous step.</li>
        <li><em>‚ÄúDo classification or regression?‚Äù</em>: <code class="language-plaintext highlighter-rouge">KerasGRegressor</code></li>
        <li>In <em>‚ÄúCompile Parameters‚Äù</em>:
          <ul>
            <li><em>‚ÄúSelect a loss function‚Äù</em>: <code class="language-plaintext highlighter-rouge">mse / MSE / mean_squared_error</code></li>
            <li><em>‚ÄúSelect an optimizer‚Äù</em>: <code class="language-plaintext highlighter-rouge">Adam - Adam optimizer </code></li>
            <li><em>‚ÄúSelect metrics‚Äù</em>: <code class="language-plaintext highlighter-rouge">mse / MSE / mean_squared_error</code></li>
          </ul>
        </li>
        <li>In <em>‚ÄúFit Parameters‚Äù</em>:
          <ul>
            <li><em>‚Äúepochs‚Äù</em>: <code class="language-plaintext highlighter-rouge">150</code></li>
            <li><em>‚Äúbatch_size‚Äù</em>: <code class="language-plaintext highlighter-rouge">50</code></li>
          </ul>
        </li>
        <li>Click <em>‚ÄúExecute‚Äù</em></li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>A loss function measures how different the predicted output is from the expected output. For regression problems, we use
<em>Mean Squared Error (MSE)</em> loss function, which averages the square of the difference between predicted and actual values for
the batch. Epochs is the number of times the whole training data is used to train the model. If we update network weights/biases
after all the training data is fed to the network, the training will be slow (as we have 723 training examples in our dataset).
To speed up the training, we present only a subset of the training examples to the network, after which we update the weights/biases.
batch_size decides the size of this subset (which we set to 50). The model builder can be downloaded as a zip file.</p>

<h3 id="deep-learning-training-and-evaluation"><strong>Deep learning training and evaluation</strong></h3>

<blockquote class="notranslate hands_on">
  <h3 id="hands_on-hands-on-training-the-model"><i class="fas fa-pencil-alt" aria-hidden="true"></i><span class="visually-hidden">hands_on</span> Hands-on: Training the model</h3>

  <ul>
    <li><span class="tool" data-tool="toolshed.g2.bx.psu.edu/repos/bgruening/keras_train_and_eval/keras_train_and_eval/1.0.8.2" title="Tested with toolshed.g2.bx.psu.edu/repos/bgruening/keras_train_and_eval/keras_train_and_eval/1.0.8.2"><strong>Deep learning training and evaluation</strong> <i class="fas fa-wrench" aria-hidden="true"></i><i aria-hidden="true" class="fas fa-cog"></i><span class="visually-hidden">Tool: <span class="notranslate">toolshed</span>.g2.bx.psu.edu/repos/bgruening/keras_train_and_eval/keras_train_and_eval/1.0.8.2</span></span>
      <ul>
        <li><em>‚ÄúSelect a scheme‚Äù</em>: <code class="language-plaintext highlighter-rouge">Train and Validate</code></li>
        <li><em>‚ÄúChoose the dataset containing pipeline/estimator object‚Äù</em>: Select the <em>Keras Model Builder</em> from the previous step.</li>
        <li><em>‚ÄúSelect input type:‚Äù</em>: <code class="language-plaintext highlighter-rouge">tabular data</code>
          <ul>
            <li><em>‚ÄúTraining samples dataset‚Äù</em>: Select <code class="language-plaintext highlighter-rouge">X_train</code> dataset</li>
            <li><em>‚ÄúChoose how to select data by column:‚Äù</em>: <code class="language-plaintext highlighter-rouge">All columns</code></li>
            <li><em>‚ÄúDoes the dataset contain header:‚Äù</em>: <code class="language-plaintext highlighter-rouge">Yes</code></li>
            <li><em>‚ÄúDataset containing class labels or target values‚Äù</em>: Select <code class="language-plaintext highlighter-rouge">y_train</code> dataset</li>
            <li><em>‚ÄúChoose how to select data by column:‚Äù</em>: <code class="language-plaintext highlighter-rouge">All columns</code></li>
            <li><em>‚ÄúDoes the dataset contain header:‚Äù</em>: <code class="language-plaintext highlighter-rouge">Yes</code></li>
          </ul>
        </li>
        <li>Click <em>‚ÄúExecute‚Äù</em></li>
      </ul>
    </li>
  </ul>

</blockquote>

<p>The training step generates 3 datasets. 1) accuracy of the trained model, 2) the trained model, downloadable as a zip file, and 3) the trained
model weights, downloadable as an hdf5 file. These files are needed for prediction in the next step.</p>

<h3 id="model-prediction"><strong>Model Prediction</strong></h3>

<blockquote class="notranslate hands_on">
  <h3 id="hands_on-hands-on-testing-the-model"><i class="fas fa-pencil-alt" aria-hidden="true"></i><span class="visually-hidden">hands_on</span> Hands-on: Testing the model</h3>

  <ul>
    <li><span class="tool" data-tool="toolshed.g2.bx.psu.edu/repos/bgruening/model_prediction/model_prediction/1.0.8.2" title="Tested with toolshed.g2.bx.psu.edu/repos/bgruening/model_prediction/model_prediction/1.0.8.2"><strong>Model Prediction</strong> <i class="fas fa-wrench" aria-hidden="true"></i><i aria-hidden="true" class="fas fa-cog"></i><span class="visually-hidden">Tool: <span class="notranslate">toolshed</span>.g2.bx.psu.edu/repos/bgruening/model_prediction/model_prediction/1.0.8.2</span></span>
      <ul>
        <li><em>‚ÄúChoose the dataset containing pipeline/estimator object‚Äù</em> : Select the trained model from the previous step.</li>
        <li><em>‚ÄúChoose the dataset containing weights for the estimator above‚Äù</em> : Select the trained model weights from the previous step.</li>
        <li><em>‚ÄúSelect invocation method‚Äù</em>: <code class="language-plaintext highlighter-rouge">predict</code></li>
        <li><em>‚ÄúSelect input data type for prediction‚Äù</em>: <code class="language-plaintext highlighter-rouge">tabular data</code></li>
        <li><em>‚ÄúTraining samples dataset‚Äù</em>: Select <code class="language-plaintext highlighter-rouge">X_test</code> dataset</li>
        <li><em>‚ÄúChoose how to select data by column:‚Äù</em>: <code class="language-plaintext highlighter-rouge">All columns</code></li>
        <li><em>‚ÄúDoes the dataset contain header:‚Äù</em>: <code class="language-plaintext highlighter-rouge">Yes</code></li>
        <li>Click <em>‚ÄúExecute‚Äù</em></li>
      </ul>
    </li>
  </ul>

</blockquote>

<p>The prediction step generates 1 dataset. It‚Äôs a file that has the predicted car purchase price for every row in the test dataset.</p>

<h3 id="plot-actual-vs-predicted-curves-and-residual-plots"><strong>Plot actual vs predicted curves and residual plots</strong></h3>

<blockquote class="notranslate hands_on">
  <h3 id="hands_on-hands-on-check-and-visualize-the-predictions"><i class="fas fa-pencil-alt" aria-hidden="true"></i><span class="visually-hidden">hands_on</span> Hands-on: Check and visualize the predictions</h3>

  <ul>
    <li><span class="tool" data-tool="toolshed.g2.bx.psu.edu/repos/bgruening/plotly_regression_performance_plots/plotly_regression_performance_plots/0.1" title="Tested with toolshed.g2.bx.psu.edu/repos/bgruening/plotly_regression_performance_plots/plotly_regression_performance_plots/0.1"><strong>Plot actual vs predicted curves and residual plots</strong> <i class="fas fa-wrench" aria-hidden="true"></i><i aria-hidden="true" class="fas fa-cog"></i><span class="visually-hidden">Tool: <span class="notranslate">toolshed</span>.g2.bx.psu.edu/repos/bgruening/plotly_regression_performance_plots/plotly_regression_performance_plots/0.1</span></span>
      <ul>
        <li><em>‚ÄúSelect input data file‚Äù</em>: <code class="language-plaintext highlighter-rouge">y_test</code></li>
        <li><em>‚ÄúSelect predicted data file‚Äù</em>‚Äù: Select <code class="language-plaintext highlighter-rouge">Model Prediction</code> from the previous step</li>
        <li>Click <em>‚ÄúExecute‚Äù</em></li>
      </ul>
    </li>
  </ul>

</blockquote>

<p>This step generates 3 graphs. The first graph (Figure 6) plots true vs predicted values. The closer the points are to each other,
the better our model‚Äôs performance at predicting. The second graph (Figure 7) is a scatter plot of true vs predicted values. If
our model predicts all values correctly, we would get a diagonal line (going from bottom left to upper right). The more the
predicted vs true points are off this diagonal line, the worse our model‚Äôs performance at predicting. The R2 (coefficient of
determination) score for our model is 0.87 (out of the best possible score of 1.0). The RMSE (root mean squared error) is 0.11.
The best value for RMSE is obviously 0 for perfect prediction. The third graph (Figure 8) plots residual (predicted - true) vs
predicted values. The better our model‚Äôs predictions, the closer the points to y=0 line.</p>

<figure id="figure-6"><img src="../../images/FNN_true_predicted_plot.png" alt="True vs predicted values plot. " loading="lazy" /><figcaption><span class="figcaption-prefix">Figure 6:</span> True vs predicted values plot</figcaption></figure>

<figure id="figure-7"><img src="../../images/FNN_scatter_plot.png" alt="Scatterplot of true vs predicted values plot. " loading="lazy" /><figcaption><span class="figcaption-prefix">Figure 7:</span> Scatterplot of true vs predicted values plot</figcaption></figure>

<figure id="figure-8"><img src="../../images/FNN_residual_plot.png" alt="Residual vs predicted values plot. " loading="lazy" /><figcaption><span class="figcaption-prefix">Figure 8:</span> Residual vs predicted values plot</figcaption></figure>

<h1 class="no_toc" id="conclusion">Conclusion</h1>

<p>In this tutorial, we discussed the inspiration behind the neural networks, and explained Perceptron, one of the earliest neural
networks still in use today. We then discussed different activation functions, what supervised learning is, what are loss/cost functions,
and how backpropagation minimizes the cost function. Finally, we implemented a FNN in <span class="notranslate">Galaxy</span> to solve a regression problem on car purchase price prediction data.</p>


                

                <h1>Frequently Asked Questions</h1>
                Have questions about this tutorial? Check out the  <a href="/training-material/topics/statistics/faqs/">FAQ page for the Statistics and machine learning topic</a> to see if your question is listed there.
                If not, please ask your question on the <a href="https://gitter.im/Galaxy-Training-Network/Lobby">GTN Gitter Channel</a> or the
                <a href="https://help.galaxyproject.org">Galaxy Help Forum</a>

                

                
                <h1 id="bibliography">References</h1>
                <ol class="bibliography"><li id="Rosenblatt">Rosenblatt, F., 1957 <b>The Perceptron, a Perceiving and Recognizing Automaton Project Para</b>. <i>Cornell Aeronautical Laboratory</i>. <a href="https://books.google.com/books?id=P_XGPgAACAAJ">https://books.google.com/books?id=P_XGPgAACAAJ</a></li>
<li id="Newell780">Newell, A., 1969 <b>Perceptrons. An Introduction to Computational Geometry. Marvin Minsky and Seymour Papert. M.I.T. Press, Cambridge, Mass., 1969. vi + 258 pp., illus. Cloth, 12; paper, 4.95</b>. Science 165: 780‚Äì782. <a href="https://doi.org/10.1126/science.165.3895.780">10.1126/science.165.3895.780</a> <a href="https://science.sciencemag.org/content/165/3895/780">https://science.sciencemag.org/content/165/3895/780</a></li>
<li id="Rumelhart1986">Rumelhart, D. E., G. E. Hinton, and R. J. Williams, 1986 <b>Learning representations by back-propagating errors</b>. Nature 323: 533‚Äì536. <a href="https://doi.org/10.1038/323533a0">10.1038/323533a0</a></li>
<li id="Cybenko1989">Cybenko, G., 1989 <b>Approximation by superpositions of a sigmoidal function</b>. Mathematics of Control, Signals and Systems 2: 303‚Äì314. <a href="https://doi.org/10.1007/BF02551274">10.1007/BF02551274</a></li>
<li id="JainEtAl">Jain, A. K., J. Mao, and K. M. Mohiuddin, 1996 <b>Artificial neural networks: a tutorial</b>. Computer 29: 31‚Äì44. <a href="https://doi.org/10.1109/2.485891">10.1109/2.485891</a></li>
<li id="KukackaEtAl">Kukacka, J., V. Golkov, and D. Cremers, 2017 <b>Regularization for Deep Learning: A Taxonomy</b>. CoRR abs/1710.10686: <a href="https://doi.org/1710.10686">1710.10686</a> <a href="http://arxiv.org/abs/1710.10686">http://arxiv.org/abs/1710.10686</a></li>
<li id="nwankpaEtAl">Nwankpa, C., W. Ijomah, A. Gachagan, and S. Marshall, 2018 <b>Activation Functions: Comparison of trends in Practice and Research for Deep Learning</b>. CoRR abs/1811.03378: <a href="http://arxiv.org/abs/1811.03378">http://arxiv.org/abs/1811.03378</a></li>
<li id="Python">Python, and R. tutorials, 2018 <b>Keras: Regression-based neural networks</b> (datascienceplus.com, Ed.). Online; posted 07-October-2018. <a href="https://datascienceplus.com/keras-regression-based-neural-networks/">https://datascienceplus.com/keras-regression-based-neural-networks/</a></li>
<li id="Bagheri">Bagheri, R., 2020 <b>An introduction to deep learning feedforward networks</b> (towardsdatascience.com, Ed.). Online; posted 20-July-2020. <a href="https://towardsdatascience.com/an-introduction-to-deep-feedforward-neural-networks-1af281e306cd">https://towardsdatascience.com/an-introduction-to-deep-feedforward-neural-networks-1af281e306cd</a></li>
<li id="Grogan">Grogan, M., 2020 <b>Keras: Regression-based neural networks</b> (github.com/MGCodesandStats, Ed.). Online; posted 25-March-2020. <a href="https://github.com/MGCodesandStats/datasets/blob/master/cars.csv">https://github.com/MGCodesandStats/datasets/blob/master/cars.csv</a></li></ol>
                

                

                <h1>Feedback</h1>
                <p class="text-muted">Did you use this material as an instructor? Feel free to give us feedback on <a href="https://github.com/galaxyproject/training-material/issues/1452" target="_blank">how it went</a>.</p>

                <div id="feedback-button">
                    <img src="/training-material/shared/images/feedback.png" title="Click to activate" alt="Click here to load Google feedback frame" />
                </div>
                <div id="feedback-form">
                </div>
                <script type="text/javascript">
                    (function (window, document) {
                        function onDocumentReady(fn) {
                            if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
                                fn();
                            } else {
                                document.addEventListener('DOMContentLoaded', fn);
                            }
                        }

                        onDocumentReady(function () {
                            $("#feedback-button").click(function(evt){
                                var e = $(evt.target)
                                e.hide();

                                $("#feedback-form").html(`
                                    <iframe id="feedback-google" class="google-form" src="https://docs.google.com/forms/d/e/1FAIpQLSd4VZptFTQ03kHkMz0JyW9b6_S8geU5KjNE_tLM0dixT3ZQmA/viewform?embedded=true&entry.1235803833=Deep Learning (Part 1) - Feedforward neural networks (FNN) (Statistics and machine learning)">Loading...</iframe>
                                `)
                            })
                        });
                    })(window, document);
                </script>



                <h1>Citing this Tutorial</h1>
                <p>
                    <ol>
                        <li id="citation-text">Kaivan Kamali, 2021 <b>Deep Learning (Part 1) - Feedforward neural networks (FNN) (Galaxy Training Materials)</b>. <a href="https://training.galaxyproject.org/training-material/topics/statistics/tutorials/FNN/tutorial.html">https://training.galaxyproject.org/training-material/topics/statistics/tutorials/FNN/tutorial.html</a> Online; accessed TODAY
                        </li>
                        <li>
                        Batut et al., 2018 <b>Community-Driven Data Analysis Training for Biology</b> Cell Systems <a href="https://doi.org/10.1016%2Fj.cels.2018.05.012">10.1016/j.cels.2018.05.012</a>
                        </li>
                    </ol>
                </p>


                <blockquote class="details">
                  <h3><i class="fa fa-info-circle" aria-hidden="true"></i><span class="visually-hidden">details</span> BibTeX</h3>
                  <p style="display: none;">

                <div class="highlighter-rouge"><div class="highlight"><pre class="highlight">
<code id="citation-code">@misc{statistics-FNN,
author = "Kaivan Kamali",
title = "Deep Learning (Part 1) - Feedforward neural networks (FNN) (Galaxy Training Materials)",
year = "2021",
month = "06",
day = "26"
url = "\url{https://training.galaxyproject.org/training-material/topics/statistics/tutorials/FNN/tutorial.html}",
note = "[Online; accessed TODAY]"
}
@article{Batut_2018,
    doi = {10.1016/j.cels.2018.05.012},
    url = {https://doi.org/10.1016%2Fj.cels.2018.05.012},
    year = 2018,
    month = {jun},
    publisher = {Elsevier {BV}},
    volume = {6},
    number = {6},
    pages = {752--758.e1},
    author = {B{\'{e}}r{\'{e}}nice Batut and Saskia Hiltemann and Andrea Bagnacani and Dannon Baker and Vivek Bhardwaj and Clemens Blank and Anthony Bretaudeau and Loraine Brillet-Gu{\'{e}}guen and Martin {\v{C}}ech and John Chilton and Dave Clements and Olivia Doppelt-Azeroual and Anika Erxleben and Mallory Ann Freeberg and Simon Gladman and Youri Hoogstrate and Hans-Rudolf Hotz and Torsten Houwaart and Pratik Jagtap and Delphine Larivi{\`{e}}re and Gildas Le Corguill{\'{e}} and Thomas Manke and Fabien Mareuil and Fidel Ram{\'{\i}}rez and Devon Ryan and Florian Christoph Sigloch and Nicola Soranzo and Joachim Wolff and Pavankumar Videm and Markus Wolfien and Aisanjiang Wubuli and Dilmurat Yusuf and James Taylor and Rolf Backofen and Anton Nekrutenko and Bj√∂rn Gr√ºning},
    title = {Community-Driven Data Analysis Training for Biology},
    journal = {Cell Systems}
}</code>
                </pre></div></div>
                </p>
                </blockquote>


<script type="text/javascript">
// update the date on load, or leave fallback of 'today'
d = new Date();
document.getElementById("citation-code").innerHTML = document.getElementById("citation-code").innerHTML.replace("TODAY", d.toDateString());
document.getElementById("citation-text").innerHTML = document.getElementById("citation-text").innerHTML.replace("TODAY", d.toDateString());
</script>

                <h3><i class="far fa-thumbs-up" aria-hidden="true"></i> Congratulations on successfully completing this tutorial!</h3>

                

                
                <blockquote class="agenda follow-up">
                    <strong class="follow-up"><i class="fas fa-graduation-cap" aria-hidden="true"></i> Do you want to extend your knowledge? Follow one of our recommended follow-up trainings:</strong>
                    <ul>
                        
    <li>
    
        
        
        <a href="/training-material/topics/statistics">Statistics and machine learning</a>
        
            <ul>
                
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                            
                            <li> Deep Learning (Part 2) - Recurrent neural networks (RNN):
                            
                                <a href="/training-material/topics/statistics/tutorials/RNN/slides.html"><i class="fab fa-slideshare" aria-hidden="true"></i><span class="visually-hidden">slides</span> slides</a>
                                
                            
                            
                                
                                    - <a href="/training-material/topics/statistics/tutorials/RNN/tutorial.html"><i class="fas fa-laptop" aria-hidden="true"></i><span class="visually-hidden">tutorial</span> hands-on</a>
                                
                            
                            </li>
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                
            </ul>
        
    
    </li>

                    </ul>
                </blockquote>
                

            </div>
        </div>
    </div>
</section>
<br/>
<br/>
<br/>

        </div>
        
    </body>
    <script type="text/javascript" src="/training-material/assets/js/jquery.slim.min.js"></script>
    <script type="text/javascript" src="/training-material/assets/js/popper.min.js"></script>
    <script type="text/javascript" src="/training-material/assets/js/bootstrap.min.js?v=3"></script>
    <script type="text/javascript" src="/training-material/assets/js/details-element-polyfill.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="/training-material/assets/js/bootstrap-toc.min.js"></script>
    <script type="text/javascript" src="/training-material/assets/js/main.js"></script>
    <script type="text/javascript" src="/training-material/assets/js/theme.js"></script>

    <script type="text/javascript" src="/training-material/assets/js/clipboard.min.js"></script>
    <script type="text/javascript">
    var snippets=document.querySelectorAll('div.highlight');
    [].forEach.call(snippets,function(snippet){
        snippet.firstChild.insertAdjacentHTML('beforebegin','<button class="btn btn-light" data-clipboard-snippet><i class="fa fa-copy"></i>&nbsp;Copy</button>');
    });

    var clipboardSnippets=new ClipboardJS('[data-clipboard-snippet]',{
        target:function(trigger){return trigger.nextElementSibling;
    }});
    </script>
    

    <script type="text/javascript">
        if(window.location.hostname === "galaxyproject.github.io") {
            // Redirect
            var redirect = "https://training.galaxyproject.org" + window.location.pathname + window.location.search;
            $('div.container.main-content').prepend("<div class='alert alert-warning'><strong>Note: </strong>This content has a new home at <a href=\"" + redirect + "\">" + redirect + "</a>, which you will be redirected to in 5 seconds.</div>");

            window.setTimeout(function(){
                window.location.href = redirect;
            }, 5000)

        }
    </script>
</html>
