<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <title>Deep Learning (Part 3) - Convolutional neural networks (CNN)</title>
        
            <script async defer data-domain="training.galaxyproject.org" src="https://plausible.galaxyproject.eu/js/plausible.js"></script>

        
        <link rel="stylesheet" href="/training-material/assets/css/bootstrap.min.css?v=3">
        <link rel="stylesheet" href="/training-material/assets/css/bootstrap-toc.min.css">
        <link rel="stylesheet" href="/training-material/assets/css/main.css?v=2">
        <script src="https://kit.fontawesome.com/67b3f98409.js" crossorigin="anonymous"></script>
        <link rel="stylesheet" href="/training-material/assets/css/academicons.css">
        <link rel="stylesheet" href="/training-material/assets/css/syntax_highlighting.css">
        <link rel="shortcut icon" href="/training-material/favicon.ico" type="image/x-icon" />
        <link rel="alternate" type="application/atom+xml" href="/training-material/feed.xml" />

        
        
        
        
        
        <meta name="description" content="Statistical Analyses for omics data and machine learning ..." />
        <meta property="og:title" content="Galaxy Training: Deep Learning (Part 3) - Convolutional neural networks (CNN)" />
        <meta property="og:description" content="Statistical Analyses for omics data and machine learning ..." />
        <meta property="og:image" content="/training-material/assets/images/GTNLogo1000.png" />
    </head>
    <body data-spy="scroll" data-target="#toc">
        <header>
    <nav class="navbar navbar-expand-md navbar-dark" aria-label="Site Navigation">
        <div class="container">
            <a class="navbar-brand" href="/training-material/">
                <img src="/training-material/assets/images/GTN-60px.png" height="30" alt="Galaxy Training Network logo">
                Galaxy Training!
            </a>

            <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#top-navbar" aria-controls="top-navbar" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="top-navbar">
                <ul class="navbar-nav">
                    <li class="nav-item">
                        
                        <a class="nav-link" href="/training-material/topics/statistics" title="Go back to list of tutorials">
                            <i class="far fa-folder" aria-hidden="true"></i> Statistics and machine learning
                        </a>
                        
                    </li>
                    <li class="nav-item dropdown">
    <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-expanded="false" title="Help">
        <i class="far fa-question-circle" aria-hidden="true"></i><span class="visually-hidden">help</span> Help
    </a>
    <div class="dropdown-menu dropdown-menu-right">
        <!-- disable Tess for now
        <form method="get" action="https://tess.elixir-europe.org/materials">
            <input type="text" id="search" name="q" value="" style="margin-left: 0.5em;/*! border-radius: 0px; */">
            <input type="hidden" value="Galaxy Training" name="content_provider">
            <input type="submit" value="Search on TeSS" style="width: 92%;border-radius: 0px;margin: 0.5em;background: #f47d20;border: 0px;padding: 0.25em;" class="">
        </form>
        -->

        <a class="dropdown-item" href="/training-material/faq" title="Check our FAQs">
           <i class="far fa-question-circle" aria-hidden="true"></i><span class="visually-hidden">question</span> FAQs
        </a>
        
        <a class="dropdown-item" href="/training-material/topics/statistics/faqs/" title="Check our FAQs for the Statistics and machine learning topic">
           <i class="far fa-question-circle" aria-hidden="true"></i><span class="visually-hidden">question</span> Topic FAQs
        </a>
        
        <a class="dropdown-item" href="https://help.galaxyproject.org/" title="Discuss on Galaxy Help">
            <i class="far fa-comments" aria-hidden="true"></i><span class="visually-hidden">feedback</span> Galaxy Help Forum
        </a>
        <a class="dropdown-item" href="https://gitter.im/Galaxy-Training-Network/Lobby" title="Discuss on gitter">
           <i class="fab fa-gitter" aria-hidden="true"></i><span class="visually-hidden">gitter</span> Discuss on Gitter
        </a>
    </div>
</li>


                    <li class="nav-item dropdown">
    <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-expanded="false" title="Extras">
        <i class="far fa-star" aria-hidden="true"></i><span class="visually-hidden">galaxy-star</span> Extras
    </a>
    <div class="dropdown-menu dropdown-menu-right">

        
        <a class="dropdown-item" href="https://github.com/galaxyproject/training-material/edit/main/topics/statistics/tutorials/CNN/tutorial.md" title="Edit on GitHub">
          <i class="fab fa-github" aria-hidden="true"></i><span class="visually-hidden">github</span> Edit on GitHub
        </a>

        <a class="dropdown-item" href="/training-material/stats.html" title="Show view statistics about this repository">
            <i class="fas fa-chart-bar" aria-hidden="true"></i><span class="visually-hidden">galaxy-barchart</span> GTN statistics
        </a>

        <a class="dropdown-item" href="https://plausible.galaxyproject.eu/training.galaxyproject.org?period=12mo&page=/training-material/topics/statistics/tutorials/CNN/tutorial.html" title="Show view statistics of this page">
            <i class="fas fa-chart-bar" aria-hidden="true"></i><span class="visually-hidden">galaxy-barchart</span> Page Metrics
        </a>

        
            <a class="dropdown-item" href="/training-material/feedback.html" title="Show feedback statistics about this repository">
                <i class="fas fa-chart-bar" aria-hidden="true"></i><span class="visually-hidden">galaxy-barchart</span> GTN feedback
            </a>
        

        <div class="dropdown-item">
            <div>
                <i class="fas fa-language" aria-hidden="true"></i><span class="visually-hidden">language</span> Translate this page
            </div>

            <div id="lang-selector">
                
                <strong>Automatic Translations</strong>

<a class="btn btn-info" href="https://translate.google.com/translate?hl=jp&sl=en&tl=fr&u=https%3A%2F%2Ftraining.galaxyproject.org/topics/statistics/tutorials/CNN/tutorial.html&edit-text=&act=url">
                Fran√ßais
                </a>
                
<a class="btn btn-info" href="https://translate.google.com/translate?hl=jp&sl=en&tl=ja&u=https%3A%2F%2Ftraining.galaxyproject.org/topics/statistics/tutorials/CNN/tutorial.html&edit-text=&act=url">
                Êó•Êú¨Ë™û
                </a>
                
<a class="btn btn-info" href="https://translate.google.com/translate?hl=jp&sl=en&tl=es&u=https%3A%2F%2Ftraining.galaxyproject.org/topics/statistics/tutorials/CNN/tutorial.html&edit-text=&act=url">
                Espa√±ol
                </a>
                
<a class="btn btn-info" href="https://translate.google.com/translate?hl=jp&sl=en&tl=pt&u=https%3A%2F%2Ftraining.galaxyproject.org/topics/statistics/tutorials/CNN/tutorial.html&edit-text=&act=url">
                Portugu√™s
                </a>
                
<a class="btn btn-info" href="https://translate.google.com/translate?hl=jp&sl=en&tl=ar&u=https%3A%2F%2Ftraining.galaxyproject.org/topics/statistics/tutorials/CNN/tutorial.html&edit-text=&act=url">
                ÿßŸÑÿπÿ±ÿ®Ÿäÿ©
                </a>
                
                <a class="btn btn-info" href="https://translate.google.com/translate?hl=jp&sl=en&tl=&u=https%3A%2F%2Ftraining.galaxyproject.org/topics/statistics/tutorials/CNN/tutorial.html&edit-text=&act=url" title="">
                And more!
                </a>
            </div>
        </div>

        <div class="dropdown-item">
            <div>
                <i class="fas fa-palette" aria-hidden="true"></i><span class="visually-hidden">gtn-theme</span> Theme
            </div>

            <div id="theme-selector" data-toggle="buttons">
                <label data-value="default" class="btn btn-secondary">
                    <input type="radio" name="options" id="default" autocomplete="off"> Default
                </label>
                <label data-value="night" class="btn btn-secondary">
                    <input type="radio" name="options" id="night" autocomplete="off"> Night
                </label>
                <label data-value="midnight" class="btn btn-secondary">
                    <input type="radio" name="options" id="midnight" autocomplete="off"> Midnight
                </label>
                <label data-value="rainbow" class="btn btn-secondary">
                    <input type="radio" name="options" id="rainbow" autocomplete="off"> Rainbow
                </label>
                <label data-value="progress" class="btn btn-secondary">
                    <input type="radio" name="options" id="progress" autocomplete="off">üè≥Ô∏è‚Äçüåà
                </label>
                <label data-value="halloween" class="btn btn-secondary">
                    <input type="radio" name="options" id="halloween" autocomplete="off"> üéÉ
                </label>
                <label data-value="straya" class="btn btn-secondary">
                    <input type="radio" name="options" id="downunder" autocomplete="off"> üá¶üá∫
                </label>
            </div>

        </div>

        <div class="dropdown-item">
            <div>
                <i class="fas fa-history" aria-hidden="true"></i><span class="visually-hidden">galaxy-rulebuilder-history</span> Previous Versions
            </div>

            
            <div id="archive-selector">
            
                <a class="btn btn-warning" href="https://training.galaxyproject.org/archive/2021-10-01/topics/statistics/tutorials/CNN/tutorial.html" title="Version 2021-10-01">2021-10-01</a>
            
                <a class="btn btn-warning" href="https://training.galaxyproject.org/archive/2021-09-01/topics/statistics/tutorials/CNN/tutorial.html" title="Version 2021-09-01">2021-09-01</a>
            
                <a class="btn btn-warning" href="https://training.galaxyproject.org/archive/2021-08-01/topics/statistics/tutorials/CNN/tutorial.html" title="Version 2021-08-01">2021-08-01</a>
            
                <a class="btn btn-warning" href="https://training.galaxyproject.org/archive/" title="Older Versions">Older Versions</a>
            </div>

        </div>

    </div>
</li>


                    <!-- Search bar-->
                    <li class="nav-item">
                      <div id="navbarSupportedContent" role="search">
                        <!-- Search form -->
                        <form class="form-inline mr-auto" method="GET" action="/training-material/search">
                          <i class="fas fa-search nav-link" aria-hidden="true"></i>
                          <div class="md-form mb-2">
                            <input name="query" class="form-control nicer" type="text" placeholder="Search Tutorials" aria-label="Search">
                          </div>
                        </form>
                      </div>
                    </li>
                </ul>
            </div>
        </div>
    </nav>
</header>

        <div class="container main-content" role="main">
        











<!-- Gitter -->




<script>
  ((window.gitter = {}).chat = {}).options = {
  room: 'Galaxy-Training-Network/Lobby'
  };
</script>
<script src="https://sidecar.gitter.im/dist/sidecar.v1.js" async defer></script>

<script type="application/ld+json">
    


{
  "@context": "http://schema.org",
  "@type": "Course",
  "accessMode": [
    "textual",
    "visual"
  ],
  "accessModeSufficient": [
    "textual",
    "visual"
  ],
  "accessibilityControl": [
    "fullKeyboardControl",
    "fullMouseControl"
  ],
  "accessibilityFeature": [
    "alternativeText",
    "tableOfContents"
  ],
  "accessibilitySummary": "Short descriptions are present but long descriptions will be needed for non-visual users",
  "audience": {
    "@type": "EducationalAudience",
    "educationalRole": "students"
  },
  "citation": {
    "@type": "CreativeWork",
    "name": "Community-Driven Data Analysis Training for Biology",
    "url": "https://doi.org/10.1016/j.cels.2018.05.012"
  },
  "copyrightHolder": {
    "@type": "Organization",
    "email": "galaxytrainingnetwork@gmail.com",
    "name": "Galaxy Training Network",
    "url": "https://galaxyproject.org/teach/gtn/"
  },
  "dateModified": "1970-01-01 00:33:41 +0000",
  "discussionUrl": "https://gitter.im/Galaxy-Training-Network/Lobby",
  "headline": "Deep Learning (Part 3) - Convolutional neural networks (CNN)",
  "interactivityType": "mixed",
  "isAccessibleForFree": true,
  "isFamilyFriendly": true,
  "license": "https://spdx.org/licenses/CC-BY-4.0.html",
  "producer": {
    "@type": "Organization",
    "email": "galaxytrainingnetwork@gmail.com",
    "name": "Galaxy Training Network",
    "url": "https://galaxyproject.org/teach/gtn/"
  },
  "provider": {
    "@type": "Organization",
    "email": "galaxytrainingnetwork@gmail.com",
    "name": "Galaxy Training Network",
    "url": "https://galaxyproject.org/teach/gtn/"
  },
  "sourceOrganization": {
    "@type": "Organization",
    "email": "galaxytrainingnetwork@gmail.com",
    "name": "Galaxy Training Network",
    "url": "https://galaxyproject.org/teach/gtn/"
  },
  "isPartOf": {
    "@type": "CreativeWork",
    "name": "Statistics and machine learning",
    "description": "Statistical Analyses for omics data and machine learning using Galaxy tools",
    "url": "https://training.galaxyproject.org//training-material/topics/statistics/"
  },
  "courseCode": "statistics / CNN / hands-on",
  "learningResourceType": "hands-on tutorial",
  "name": "Hands-on for 'Deep Learning (Part 3) - Convolutional neural networks (CNN)' tutorial",
  "url": "https://training.galaxyproject.org//training-material/topics/statistics/tutorials/CNN/tutorial.html",
  "timeRequired": "PT2H",
  "description": "The questions this  addresses are:\n - What is a convolutional neural network (CNN)?\n - What are some applications of CNN?\n\n\\nThe objectives are:\n - Understand the inspiration behind CNN and learn the CNN architecture\n - Learn the convolution operation and its parameters\n - Learn how to create a CNN using Galaxy's deep learning tools\n - Solve an image classification problem on MNIST digit classification dataset using CNN in Galaxy\n\n",
  "inLanguage": {
    "@type": "Language",
    "name": "English",
    "alternateName": "en"
  },
  "coursePrerequisites": [
    {
      "@type": "CreativeWork",
      "url": "https://training.galaxyproject.org//training-material/topics/introduction/",
      "name": "Introduction to Galaxy Analyses",
      "description": "Introduction to Galaxy Analyses",
      "provider": {
        "@type": "Organization",
        "email": "galaxytrainingnetwork@gmail.com",
        "name": "Galaxy Training Network",
        "url": "https://galaxyproject.org/teach/gtn/"
      }
    },
    {
      "@type": "Course",
      "url": "https://training.galaxyproject.org//training-material/topics/statistics/tutorials/intro_deep_learning/tutorial.html",
      "name": "Introduction to deep learning",
      "description": "Hands-on for 'Introduction to deep learning' tutorial",
      "learningResourceType": "hands-on tutorial",
      "interactivityType": "expositive",
      "provider": {
        "@type": "Organization",
        "email": "galaxytrainingnetwork@gmail.com",
        "name": "Galaxy Training Network",
        "url": "https://galaxyproject.org/teach/gtn/"
      }
    },
    {
      "@type": "Course",
      "url": "https://training.galaxyproject.org//training-material/topics/statistics/tutorials/FNN/slides.html",
      "name": "Feedforward neural networks (FNN) \n Deep Learning - Part 1",
      "description": "Slides for 'Feedforward neural networks (FNN) \n Deep Learning - Part 1' tutorial",
      "learningResourceType": "slides",
      "interactivityType": "expositive",
      "provider": {
        "@type": "Organization",
        "email": "galaxytrainingnetwork@gmail.com",
        "name": "Galaxy Training Network",
        "url": "https://galaxyproject.org/teach/gtn/"
      }
    },
    {
      "@type": "Course",
      "url": "https://training.galaxyproject.org//training-material/topics/statistics/tutorials/FNN/tutorial.html",
      "name": "Deep Learning (Part 1) - Feedforward neural networks (FNN)",
      "description": "Hands-on for 'Deep Learning (Part 1) - Feedforward neural networks (FNN)' tutorial",
      "learningResourceType": "hands-on tutorial",
      "interactivityType": "expositive",
      "provider": {
        "@type": "Organization",
        "email": "galaxytrainingnetwork@gmail.com",
        "name": "Galaxy Training Network",
        "url": "https://galaxyproject.org/teach/gtn/"
      }
    },
    {
      "@type": "Course",
      "url": "https://training.galaxyproject.org//training-material/topics/statistics/tutorials/FNN/slides.html",
      "name": "Feedforward neural networks (FNN) \n Deep Learning - Part 1",
      "description": "Slides for 'Feedforward neural networks (FNN) \n Deep Learning - Part 1' tutorial",
      "learningResourceType": "slides",
      "interactivityType": "expositive",
      "provider": {
        "@type": "Organization",
        "email": "galaxytrainingnetwork@gmail.com",
        "name": "Galaxy Training Network",
        "url": "https://galaxyproject.org/teach/gtn/"
      }
    },
    {
      "@type": "Course",
      "url": "https://training.galaxyproject.org//training-material/topics/statistics/tutorials/RNN/slides.html",
      "name": "Recurrent neural networks (RNN) \n Deep Learning - Part 2",
      "description": "Slides for 'Recurrent neural networks (RNN) \n Deep Learning - Part 2' tutorial",
      "learningResourceType": "slides",
      "interactivityType": "expositive",
      "provider": {
        "@type": "Organization",
        "email": "galaxytrainingnetwork@gmail.com",
        "name": "Galaxy Training Network",
        "url": "https://galaxyproject.org/teach/gtn/"
      }
    },
    {
      "@type": "Course",
      "url": "https://training.galaxyproject.org//training-material/topics/statistics/tutorials/RNN/tutorial.html",
      "name": "Deep Learning (Part 2) - Recurrent neural networks (RNN)",
      "description": "Hands-on for 'Deep Learning (Part 2) - Recurrent neural networks (RNN)' tutorial",
      "learningResourceType": "hands-on tutorial",
      "interactivityType": "expositive",
      "provider": {
        "@type": "Organization",
        "email": "galaxytrainingnetwork@gmail.com",
        "name": "Galaxy Training Network",
        "url": "https://galaxyproject.org/teach/gtn/"
      }
    },
    {
      "@type": "Course",
      "url": "https://training.galaxyproject.org//training-material/topics/statistics/tutorials/RNN/slides.html",
      "name": "Recurrent neural networks (RNN) \n Deep Learning - Part 2",
      "description": "Slides for 'Recurrent neural networks (RNN) \n Deep Learning - Part 2' tutorial",
      "learningResourceType": "slides",
      "interactivityType": "expositive",
      "provider": {
        "@type": "Organization",
        "email": "galaxytrainingnetwork@gmail.com",
        "name": "Galaxy Training Network",
        "url": "https://galaxyproject.org/teach/gtn/"
      }
    }
  ],
  "hasPart": [

  ],
  "author": [
    {
      "@type": "Person",
      "name": "Kaivan Kamali"
    }
  ],
  "contributor": [
    {
      "@type": "Person",
      "name": "Kaivan Kamali"
    }
  ],
  "about": [
    {
      "@type": "CreativeWork",
      "name": "Statistics and machine learning",
      "description": "Statistical Analyses for omics data and machine learning using Galaxy tools",
      "url": "https://training.galaxyproject.org//training-material/topics/statistics/"
    },
    {
      "@type": "DefinedTerm",
      "@id": "http://edamontology.org/topic_2269",
      "inDefinedTermSet": "http://edamontology.org",
      "termCode": "topic_2269",
      "url": "https://bioportal.bioontology.org/ontologies/EDAM/?p=classes&conceptid=http%3A%2F%2Fedamontology.org%2Ftopic_2269"
    }
  ]
}
</script>

<section class="tutorial topic-statistics">
    <h1 data-toc-skip>Deep Learning (Part 3) - Convolutional neural networks (CNN)</h1>
    

    <div class="contributors-line">Authors: <a href="/training-material/hall-of-fame/kxk302/" class="contributor-badge contributor-kxk302"><img src="https://avatars.githubusercontent.com/kxk302?s=27" alt="Avatar">Kaivan Kamali</a></div>

    <blockquote class="overview">
        <h3>Overview</h3>
        
        <img alt="Creative Commons License" class="float-right" style="border-width:0; display: inline-block; margin:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" aria-hidden="true" />
        
        <strong><i class="far fa-question-circle" aria-hidden="true"></i> Questions:</strong>
        <ul>
        
        <li><p>What is a convolutional neural network (CNN)?</p>
</li>
        
        <li><p>What are some applications of CNN?</p>
</li>
        
        </ul>

        <strong><i class="fas fa-bullseye" aria-hidden="true"></i> Objectives: </strong>
        <ul>
        
        <li><p>Understand the inspiration behind CNN and learn the CNN architecture</p>
</li>
        
        <li><p>Learn the convolution operation and its parameters</p>
</li>
        
        <li><p>Learn how to create a CNN using Galaxy‚Äôs deep learning tools</p>
</li>
        
        <li><p>Solve an image classification problem on MNIST digit classification dataset using CNN in Galaxy</p>
</li>
        
        </ul>

        
        <strong><i class="fas fa-check-circle" aria-hidden="true"></i> Requirements:</strong>
        <ul>
        
    <li>
    
        
        
        <a href="/training-material/topics/introduction">Introduction to Galaxy Analyses</a>
        
    
    </li>

        
    <li>
    
        
        
        <a href="/training-material/topics/statistics">Statistics and machine learning</a>
        
            <ul>
                
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                            
                            <li> Introduction to deep learning:
                            
                            
                                
                                     <a href="/training-material/topics/statistics/tutorials/intro_deep_learning/tutorial.html"><i class="fas fa-laptop" aria-hidden="true"></i><span class="visually-hidden">tutorial</span> hands-on</a>
                                
                            
                            </li>
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                            
                            <li> Deep Learning (Part 1) - Feedforward neural networks (FNN):
                            
                                <a href="/training-material/topics/statistics/tutorials/FNN/slides.html"><i class="fab fa-slideshare" aria-hidden="true"></i><span class="visually-hidden">slides</span> slides</a>
                                
                            
                            
                                
                                    - <a href="/training-material/topics/statistics/tutorials/FNN/tutorial.html"><i class="fas fa-laptop" aria-hidden="true"></i><span class="visually-hidden">tutorial</span> hands-on</a>
                                
                            
                            </li>
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                            
                            <li> Deep Learning (Part 2) - Recurrent neural networks (RNN):
                            
                                <a href="/training-material/topics/statistics/tutorials/RNN/slides.html"><i class="fab fa-slideshare" aria-hidden="true"></i><span class="visually-hidden">slides</span> slides</a>
                                
                            
                            
                                
                                    - <a href="/training-material/topics/statistics/tutorials/RNN/tutorial.html"><i class="fas fa-laptop" aria-hidden="true"></i><span class="visually-hidden">tutorial</span> hands-on</a>
                                
                            
                            </li>
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                        
                    
                
            </ul>
        
    
    </li>

        </ul>
        

        
        <div><strong><i class="fas fa-hourglass-half" aria-hidden="true"></i> Time estimation:</strong> 2 hours</div>
        

        

        
        

        
        <div id="supporting-materials"><strong><i class="fa fa-external-link" aria-hidden="true"></i> Supporting Materials:</strong></div>
        <ul class="supporting_material">
            
                <li class="btn btn-default"><a href="/training-material/topics/statistics/tutorials/CNN/slides.html" title="Slides for this tutorial">
                    <i class="fab fa-slideshare" aria-hidden="true"></i> Slides
                </a></li>
            

            
                <li class="btn btn-default supporting_material">


<a class="topic-icon" href="https://zenodo.org/record/4697906">
    <i class="far fa-copy" aria-hidden="true"></i> Datasets
</a>

</li>
            

            
                <li class="btn btn-default supporting_material">


    <a class="topic-icon" href="/training-material/topics/statistics/tutorials/CNN/workflows/" title="Workflows" alt="Deep Learning (Part 3) - Convolutional neural networks (CNN) workflows">
        <i class="fas fa-share-alt" aria-hidden="true"></i> Workflows
    </a>

</li>
            

            

            

            
            
            

            
                <li class="btn btn-default supporting_material">


    <a href="#" class="btn btn-default dropdown-toggle topic-icon" data-toggle="dropdown" aria-expanded="false" title="Where to run the tutorial">
        <i class="fas fa-globe" aria-hidden="true"></i><span class="visually-hidden">instances</span> Available on these Galaxies 
    </a>
    <ul class="dropdown-menu">
    
        <li>
            <a class="dropdown-item" href="https://github.com/galaxyproject/training-material/tree/main/topics/statistics/docker" title="Docker image for this tutorial">
                <i class="fab fa-docker" aria-hidden="true"></i><span class="visually-hidden">docker_image</span> Docker image
            </a>
        </li>
    
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
        <a class="dropdown-item" href="https://africa.usegalaxy.eu/" title="Galaxy Africa">
            Galaxy Africa
        </a>
        
    
        
        <a class="dropdown-item" href="https://india.usegalaxy.eu/" title="Galaxy India">
            Galaxy India
        </a>
        
    
        
    
        
        <a class="dropdown-item" href="https://test.galaxyproject.org/" title="Galaxy Test">
            Galaxy Test
        </a>
        
    
        
        <a class="dropdown-item" href="https://ecology.usegalaxy.eu/" title="Galaxy for Ecology">
            Galaxy for Ecology
        </a>
        
    
        
        <a class="dropdown-item" href="https://galaxy.genouest.org" title="Galaxy@GenOuest">
            Galaxy@GenOuest
        </a>
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
        <a class="dropdown-item" href="https://ml.usegalaxy.eu/" title="Machine Learning Workbench">
            Machine Learning Workbench
        </a>
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
        <a class="dropdown-item" href="https://streetscience.usegalaxy.eu/" title="Street Science">
            Street Science
        </a>
        
    
        
    
        
        <a class="dropdown-item" href="https://usegalaxy.eu" title="UseGalaxy.eu">
            UseGalaxy.eu
        </a>
        
    
        
    
        
    
        
        <a class="dropdown-item" href="https://usegalaxy.org" title="UseGalaxy.org (Main)">
            UseGalaxy.org (Main)
        </a>
        
    
        
        <a class="dropdown-item" href="https://usegalaxy.org.au" title="UseGalaxy.org.au">
            UseGalaxy.org.au
        </a>
        
    
        
    
        
    
        
    
        
    
    </ul>


</li>
            
        </ul>
        

        <div><strong><i class="far fa-calendar" aria-hidden="true"></i> Last modification:</strong> May 24, 2021 </div>
        <div><strong><i class="fas fa-balance-scale" aria-hidden="true"></i> License:</strong>
            
            <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Tutorial Content is licensed under Creative Commons Attribution 4.0 International License</a>
            
            <a rel="license" href="https://github.com/galaxyproject/training-material/blob/main/LICENSE.md">The GTN Framework is licensed under MIT</a>
        </div>
    </blockquote>

    <div class="container">
        <div class="row">
            <!-- sidebar, which will move to the top on a small screen -->
            <div class="col-sm-2">
                <nav id="toc" data-toggle="toc" class="sticky-top" aria-label="Table of Contents"></nav>
            </div>
            <div class="col-sm-10">
                

                <h1 class="no_toc" id="introduction">Introduction</h1>

<p>Artificial neural networks are a machine learning discipline that have been successfully applied to problems
in pattern classification, clustering, regression, association, time series prediction, optimiztion, and control <span class="citation"><a href="#JainEtAl">Jain <i>et al.</i> 1996</a></span>.
With the increasing popularity of social media in the past decade, image and video processing tasks have become very
important. The previous neural network architectures (e.g. feedforward neural networks) could not scale up to handle
image and video processing tasks. This gave way to the development of convolutional neural networks that are specifically
tailored to image and video processing tasks. In this tutorial, we explain what convolutional neural networks are, discuss
their architecture, and solve an image classification problem using MNIST digit classification dataset using a CNN in <span class="notranslate">Galaxy</span>.</p>

<blockquote class="agenda">
  <h3 id="agenda">Agenda</h3>

  <p>In this tutorial, we will cover:</p>

<ol id="markdown-toc">
  <li><a href="#limitations-of-feedforward-neural-networks-fnn-for-image-processing" id="markdown-toc-limitations-of-feedforward-neural-networks-fnn-for-image-processing">Limitations of feedforward neural networks (FNN) for image processing</a></li>
  <li><a href="#inspiration-for-convolutional-neural-networks" id="markdown-toc-inspiration-for-convolutional-neural-networks">Inspiration for convolutional neural networks</a></li>
  <li><a href="#architecture-of-cnn" id="markdown-toc-architecture-of-cnn">Architecture of CNN</a></li>
  <li><a href="#mnist-dataset" id="markdown-toc-mnist-dataset">MNIST dataset</a></li>
  <li><a href="#get-data" id="markdown-toc-get-data">Get data</a></li>
  <li><a href="#classification-of-mnist-dataset-images-with-cnn" id="markdown-toc-classification-of-mnist-dataset-images-with-cnn">Classification of MNIST dataset images with CNN</a></li>
</ol>

</blockquote>

<h2 id="limitations-of-feedforward-neural-networks-fnn-for-image-processing">Limitations of feedforward neural networks (FNN) for image processing</h2>

<p>In a fully connected FNN (Figure 1), all the nodes in a layer are connected to all the nodes in the next layer. Each connection has a weight
\(w_{i,j}\) that needs to be learned by the learning algorithm. Lets say our input is a 64 pixel by 64 pixel grayscale image. Each grayscale
pixel is represented by 1 value, usually between 0 to 255, where 0 represents black, 255 represents white, and the values in between represent
various shades of gray. Since each grayscale pixel can be represented by 1 value, we say the <em>channel</em> size is 1. Such an image can be represented
by 64 X 64 X 1 = 4,096 values (rows X columns X channels). Hence, the input layer of a FNN processing such an image has 4096 nodes.</p>

<p>Lets assume the next layer has 500 nodes. Since all the nodes in subsequent layers are fully connected, we will have 4,096 X 500 = 2,048,000 weights
between the input and the first hidden layer. For complex problems, we usually need multiple hidden layers in our FNN, as a simpler FNN may not be
able to learn the model <span class="notranslate">mapping</span> the inputs to outputs in the training data. Having multiple hidden layers compounds the problem of having many weights
in our FNN. Having many weights makes the learning process more difficult as the dimension of the search space is increased. It also makes the training
more time and resource consuming and increases the likelihood of overfitting. This problem is further compunded for color images. Unlike grayscale
images, each pixel in a color image is represented by 3 values, representing red, green, and blue colors (Called RGB color mode), where every color
can be represented by various combination of these primary colors. Since each color pixel can be represented by 3 values, we say the <strong>channel</strong> size
is 3. Such an image can be represented by 64 X 64 X 3 = 12,288 values (rows X columns X channels). The number of weights between the input layer and
the first hidden layer with 500 nodes is now 12,288 X 500 = 6,144,000. It is clear that a FNN cannot scale to handle larger images
(<span class="citation"><a href="#OSheaEtAl">O‚ÄôShea and Nash 2015</a></span>) and that we need a more scalable architecture.</p>

<figure id="figure-1"><img src="../../images/FFNN.png" alt="Neurons forming the input, output, and hidden layers of a multi-layer feedforward neural network. " loading="lazy" /><figcaption><span class="figcaption-prefix">Figure 1:</span> Feedforward neural network with a hidden layer. Biases to hidden/output layer neurons are omitted for clarity</figcaption></figure>

<p>Another problem with using FNN for image processing is that a 2 dimensional image is represented as a 1 dimensional vector in the input layer,
hence, any spatial relationship in the data is ignored. CNN, on the other hand, maintains the spatial structure of the data, and is better suited
for finding spatial relationships in the image data.</p>

<h2 id="inspiration-for-convolutional-neural-networks">Inspiration for convolutional neural networks</h2>

<p>In 1959 Hubel and Wiesel conducted an experiment to understand how the visual cortex of the brain processes visual information (<span class="citation"><a href="#HubelWiesel">Hubel and Wiesel 1959</a></span>).
They recorded the activity of the neurons in the visual cortex of a cat while moving a bright line in front of the cat. They noticed that some cells fire
when the bright line is shown at a particular angle and a particular location (They called these <strong>simple</strong> cells). Other neurons fired when the bright
line was shown regardless of the angle/location and seemed to detect movement (They called these <strong>complex</strong> cells). It seemed complex cells receive
inputs from multiple simple cells and have an hierarchical structure. Hubel and Wiesel won the Noble prize for their findings in 1981.</p>

<p>In 1980, inspired by hierarchical structure of complex and simple cells, Fukushima proposed <em>Neocognitron</em> (<span class="citation"><a href="#Fukishima">Fukushima 1988</a></span>), a hierarchical neural
network used for handwritten Japanese character recognition. Neocognitron was the first CNN, and had its own training algorithm. In 1989, LeCun et. al.
(<span class="citation"><a href="#LeCunEtAl">LeCun <i>et al.</i> 1989</a></span>) proposed a CNN that could be trained by backpropagation algorithm. CNN gained immense popularity when they outperformed other
models at ILSVRC (ImageNet Large Scale Visual Recognition Challenge). ILSVRC is a competition in object classification and detection on hundreds of
object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty
institutions (<span class="citation"><a href="#RussakovskyEtAl">Russakovsky <i>et al.</i> 2015</a></span>). Notable CNN architectures that won ILSVRC are AlexNet in 2012 (<span class="citation"><a href="#KrizhevskyEtAl">Krizhevsky <i>et al.</i> 2012</a></span>), ZFNet in 2013 (
<span class="citation"><a href="#ZeilerEtAl">Zeiler and Fergus 2013</a></span>), GoogLeNet and VGG in 2014 (<span class="citation"><a href="#SzegedyEtAl">Szegedy <i>et al.</i> 2014</a></span>, <span class="citation"><a href="#SimonyanEtAl">Simonyan and Zisserman 2015</a></span>), and ResNet in 2015 (<span class="citation"><a href="#HeEtAl">He <i>et al.</i> 2015</a></span>).</p>

<h2 id="architecture-of-cnn">Architecture of CNN</h2>

<p>A typical CNN has the following 4 layers (<span class="citation"><a href="#OSheaEtAl">O‚ÄôShea and Nash 2015</a></span>)</p>

<ol>
  <li>Input layer</li>
  <li>Convolution layer</li>
  <li>Pooling layer</li>
  <li>Fully connected layer</li>
</ol>

<p>Please note that we will explain a 2 dimensional (2D) CNN here. But the same concepts apply to a 1 (or 3) dimensional CNN as well.</p>

<h3 id="input-layer">Input layer</h3>

<p>The input layer represents the input to the CNN. An example input, could be a 28 pixel by 28 pixel grayscale image. Unlike FNN, we do not
‚Äúflatten‚Äù the input to a 1D vector, and the input is presented to the network in 2D as a 28 x 28 matrix. This makes capturing
spatial relationships easier.</p>

<h3 id="convolution-layer">Convolution layer</h3>

<p>The convolution layer is composed of multiple filters (also called kernels). Filters for a 2D image are also 2D. Suppose we have
a 28 pixel by 28 pixel grayscale image. Each pixel is represented by a number between 0 and 255, where 0 represents the color black,
255 represents the color white, and the values in between represent different shades of gray. Suppose we have a 3 by 3 filter (9
values in total), and the values are randomly set to 0 or 1. Convolution is the process of placing the 3 by 3 filter on the top left
corner of the image, multiplying filter values by the pixel values and adding the results, moving the filter to the right one pixel at
a time and repeating this process. When we get to the top right corner of the image, we simply move the filter down one pixel and
restart from the right. This process ends when we get to the bottom right corner of the image.</p>

<figure id="figure-2"><img src="../../images/Conv_no_padding_no_strides.gif" alt="A 3 by 3 filter applied to a 4 by 4 image, resulting in a 2 by 2 image. " loading="lazy" /><figcaption><span class="figcaption-prefix">Figure 2:</span> A 3 by 3 filter applied to a 4 by 4 image, resulting in a 2 by 2 image (<span class="citation"><a href="#DumoulinVisin">Dumoulin and Visin 2016</a></span>)</figcaption></figure>

<p>Covolution operator has the following parameters:</p>

<ol>
  <li>Filter size</li>
  <li>Padding</li>
  <li>Stride</li>
  <li>Dilation</li>
  <li>Activation function</li>
</ol>

<p>Filter size can be 5 by 5, 3 by 3, and so on. Larger filter sizes should be avoided as the learning algorithm needs to learn filter values (weights),
and larger filters increase the number of weights to be learned (more compute capacity, more training time, more chance of overfitting). Also, odd
sized filters are preferred to even sized filters, due to the nice geometric property of all the input pixels being around the output pixel.</p>

<p>If you look at Figure 2 you see that after applying a 3 by 3 filter to a 4 by 4 image, we end up with a 2 by 2 image ‚Äì the size of the image has gone
down. If we want to keep the resultant image size the same, we can use <em>padding</em>. We pad the input in every direction with 0‚Äôs before applying the
filter. If the padding is 1 by 1, then we add 1 zero in evey direction. If its 2 by 2, then we add 2 zeros in every direction, and so on.</p>

<figure id="figure-3"><img src="../../images/Conv_same_padding_no_strides.gif" alt="A 3 by 3 filter applied to a 5 by 5 image, with padding of 1, resulting in a 5 by 5 image. " loading="lazy" /><figcaption><span class="figcaption-prefix">Figure 3:</span> A 3 by 3 filter applied to a 5 by 5 image, with padding of 1, resulting in a 5 by 5 image (<span class="citation"><a href="#DumoulinVisin">Dumoulin and Visin 2016</a></span>)</figcaption></figure>

<p>As mentioned before, we start the convolution by placing the filter on the top left corner of the image, and after multiplying filter and image
values (and adding them), we move the filter to the right and repeat the process. How many pixels we move to the right (or down) is the <em>stride</em>.
In figure 2 and 3, the stride of the filter is 1. We move the filter one pixel to the right (or down). But we could use a different stride. Figure 4
shows an example of using stride of 2.</p>

<figure id="figure-4"><img src="../../images/Conv_no_padding_strides.gif" alt="A 3 by 3 filter applied to a 5 by 5 image, with stride of 2, resulting in a 2 by 2 image. " loading="lazy" /><figcaption><span class="figcaption-prefix">Figure 4:</span> A 3 by 3 filter applied to a 5 by 5 image, with stride of 2, resulting in a 2 by 2 image (<span class="citation"><a href="#DumoulinVisin">Dumoulin and Visin 2016</a></span>)</figcaption></figure>

<p>When we apply a, say 3 by 3, filter to an image, our filter‚Äôs output is affected by pixels in a 3 by 3 subset of the image. If we like to have a
larger <em>receptive field</em> (portion of the image that affect our filter‚Äôs output), we could use <em>dilation</em>. If we set the dilation to 2 (Figure 5),
instead of a contiguous 3 by 3 subset of the image, every other pixel of a 5 by 5 subset of the image affects the filter‚Äôs output.</p>

<figure id="figure-5"><img src="../../images/Conv_dilation.gif" alt="A 3 by 3 filter applied to a 7 by 7 image, with dilation of 2, resulting in a 3 by 3 image. " loading="lazy" /><figcaption><span class="figcaption-prefix">Figure 5:</span> A 3 by 3 filter applied to a 7 by 7 image, with dilation of 2, resulting in a 3 by 3 image (<span class="citation"><a href="#DumoulinVisin">Dumoulin and Visin 2016</a></span>)</figcaption></figure>

<p>After the filter scans the whole image, we apply an activation function to filter output to introduce non-linearlity. The preferred activation function
used in CNN is ReLU or one its variants like Leaky ReLU (<span class="citation"><a href="#NwankpaEtAl">Nwankpa <i>et al.</i> 2018</a></span>). ReLU leaves pixels with positive values in filter output as is, and
replacing negative values with 0 (or a small number in case of Leaky ReLU). Figure 6 shows the results of applying ReLU activation function to a filter
output.</p>

<figure id="figure-6"><img src="../../images/Conv_ReLU.png" alt="Two matrices representing filter output before and after ReLU activation function is applied. " loading="lazy" /><figcaption><span class="figcaption-prefix">Figure 6:</span> Applying ReLU activation function to filter output</figcaption></figure>

<p>Given the input size, filter size, padding, stride and dilation you can calculate the output size of the convolution operation as below.</p>

\[\frac{(\text{input size} - \text{(filter size + (filter size -1)*(dilation - 1)})) + (2*padding)}{stride} + 1\]

<figure id="figure-7"><img src="../../images/Conv_single_input_channel.png" alt="One matrix representing an input vector and another matrix representing a filter, along with calculation for single input channel two dimensional convolution operation. " loading="lazy" /><figcaption><span class="figcaption-prefix">Figure 7:</span> Illustration of single input channel two dimensional convolution</figcaption></figure>

<p>Figure 7 illustrates the calculations for a convolution operation, via a 3 by 3 filter on a single channel 5 by 5 input vector (5 x 5 x 1). Figure 8
illustrates the calculations when the input vector has 3 channels (5 x 5 x 3). To show this in 2 dimensions, we are displaying each channel in input
vector and filter separately. Figure 9 shows a sample multi-channel 2D convolution in 3 dimensions.</p>

<figure id="figure-8"><img src="../../images/Conv_multiple_input_channel.png" alt="Three matrices representing an input vector and another three matrices representing a filter, along with calculation for multiple input channel two dimensional convolution operation . " loading="lazy" /><figcaption><span class="figcaption-prefix">Figure 8:</span> Illustration of multiple input channel two dimensional convolution</figcaption></figure>

<p>As Figures 8 and 9 show the output of a multi-channel 2 dimensional filter is a single channel 2 dimensional image. Applying <em>multiple</em> filters to the
input image results in a multi-channel 2 dimensional image for the output. For example, if the input image is 28 by 28 by 3 (rows x columns x channels),
and we apply a 3 by 3 filter with 1 by 1 padding, we would get a 28 by 28 by 1 image. If we apply 15 filters to the input image, our output would be 28
by 28 by 15. Hence, the number of filters in a convolution layer allows us to increase or decrease the channel size.</p>

<figure id="figure-9"><img src="../../images/Conv_multiple_channel_3d.gif" alt="Multiple cubes representing input vector, filter, and output in a 3 channel 2 dimensional convolution operation. " loading="lazy" /><figcaption><span class="figcaption-prefix">Figure 9:</span> Three dimensional illustration of multiple input channel two dimensional convolution (Source: https://thomelane.github.io/convolutions/2DConvRGB.html)</figcaption></figure>

<h3 id="pooling-layer">Pooling layer</h3>

<p>The pooling layer performs down sampling to reduce the spatial dimensionality of the input. This decreases the number of parameters, which in turn
reduces the learning time and computation, and the likelihood of overfitting. The most popular type of pooling is <em>max pooling</em>. Its usually a 2 by 2
filter with a stride of 2 that returns the maximum value as it slides over the input data (similar to convolution filters).</p>

<h3 id="fully-connected-layer">Fully connected layer</h3>

<p>The last layer in a CNN is a fully connected layer. We connect all the nodes from the previous layer to this fully connected layer, which is responsible
for classification of the image.</p>

<figure id="figure-10"><img src="../../images/Conv_CNN.png" alt="A convolutional neural network with 3 convolution layers followed by 3 pooling layers. " loading="lazy" /><figcaption><span class="figcaption-prefix">Figure 10:</span> A convolutional neural network with 3 convolution layers followed by 3 pooling layers (<span class="citation"><a href="#OSheaEtAl">O‚ÄôShea and Nash 2015</a></span>)</figcaption></figure>

<p>As shown in Figure 10, a typical CNN usually has more than one convolution layer plus pooling layer. Each convolution plus pooling layer is responsible
for feature extraction at a different level of abstraction. For example, the filters in the first layer could detect horizental, vertical, and diagonal
edges. The filters in the next layer could detect shapes, and the filters in the last layer could detect collection of shapes. Filter values are randomly
initialized and are learned by the learning algorithm. This makes CNN very powerful as they not only do classification, but can also automatically do
feature extraction. This distinguishes CNN from other classification techniques (like Support Vector Machines), which cannot do feature extraction.</p>

<h2 id="mnist-dataset">MNIST dataset</h2>

<p>The MNIST database of handwritten digits (<span class="citation"><a href="#LeCunMnist">LeCun <i>et al.</i> 2010</a></span>) is composed of a training set of 60,000 images and a test set of 10,000 images. The digits
have been size-normalized and centered in a fixed-size image (28 by 28 pixels). Images are grayscale, where each pixel is represented by a number between
0 and 255 (0 for black, 255 for white, and other values for different shades of gray). MNIST database is a standard image classification dataset and is used
to compare various Machine Learning techniques.</p>

<h2 id="get-data">Get data</h2>

<blockquote class="notranslate hands_on">
  <h3 id="hands_on-hands-on-data-upload"><i class="fas fa-pencil-alt" aria-hidden="true"></i><span class="visually-hidden">hands_on</span> Hands-on: Data upload</h3>

  <ol>
    <li>
      <p>Make sure you have an empty analysis history.</p>

      <!--SNIPPET-->
      <blockquote class="tip">  <h3 data-toc-skip="" id="-tip-creating-a-new-history"><i class="far fa-lightbulb" aria-hidden="true"></i><span class="visually-hidden"></span> Tip: Creating a new history</h3>  <p>Click the <i class="fas fa-plus" aria-hidden="true"></i><span class="visually-hidden">new-history</span> icon at the top of the history panel.</p>  <p>If the <i class="fas fa-plus" aria-hidden="true"></i><span class="visually-hidden">new-history</span> is missing:</p>  <ol>  <li>Click on the <i class="fas fa-cog" aria-hidden="true"></i><span class="visually-hidden"><span class="notranslate">galaxy</span>-gear</span> icon (<strong>History options</strong>) on the top of the history panel</li>  <li>Select the option <strong>Create New</strong> from the menu</li></ol></blockquote>
    </li>
    <li>
      <p><strong>Rename your history</strong> to make it easy to recognize</p>

      <blockquote class="tip">
        <h3 id="tip-rename-a-history"><i class="far fa-lightbulb" aria-hidden="true"></i><span class="visually-hidden">tip</span> Rename a history</h3>

        <ul>
          <li>
            <p>Click on the title of the history (by default the title is <code class="language-plaintext highlighter-rouge">Unnamed history</code>)</p>

            <p><img src="../../../../shared/images/rename_history.png" alt="Renaming history. " loading="lazy" /></p>
          </li>
          <li>Type <code class="language-plaintext highlighter-rouge"><span class="notranslate">Galaxy</span> Introduction</code> as the name</li>
          <li>Press <kbd>Enter</kbd></li>
        </ul>

      </blockquote>
    </li>
    <li>
      <p>Import the files from <a href="https://zenodo.org/record/4697906">Zenodo</a></p>

      <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>https://zenodo.org/record/4697906/files/X_train.tsv
https://zenodo.org/record/4697906/files/y_train.tsv
https://zenodo.org/record/4697906/files/X_test.tsv
https://zenodo.org/record/4697906/files/y_test.tsv
</code></pre></div>      </div>

      <!--SNIPPET-->
      <blockquote class="tip">  <h3 data-toc-skip="" id="-tip-importing-via-links"><i class="far fa-lightbulb" aria-hidden="true"></i><span class="visually-hidden"></span> Tip: Importing via links</h3>  <ul>  <li>Copy the link location</li>  <li>    <p>Open the <span class="notranslate">Galaxy</span> Upload Manager (<i class="fas fa-upload" aria-hidden="true"></i><span class="visually-hidden"><span class="notranslate">galaxy</span>-upload</span> on the top-right of the tool panel)</p>  </li>  <li>Select <strong>Paste/Fetch Data</strong></li>  <li>    <p>Paste the link into the text field</p>  </li>  <li>    <p>Press <strong>Start</strong></p>  </li>  <li>    <p><strong>Close</strong> the window</p>  </li>  <li>By default, <span class="notranslate">Galaxy</span> uses the URL as the name, so rename the files with a more useful name.</li></ul></blockquote>
    </li>
    <li>
      <p>Rename the datasets as <code class="language-plaintext highlighter-rouge">X_train</code>, <code class="language-plaintext highlighter-rouge">y_train</code>, <code class="language-plaintext highlighter-rouge">X_test</code>, and <code class="language-plaintext highlighter-rouge">y_test</code> respectively.</p>

      <!--SNIPPET-->
      <blockquote class="tip">  <h3 data-toc-skip="" id="-tip-renaming-a-dataset"><i class="far fa-lightbulb" aria-hidden="true"></i><span class="visually-hidden"></span> Tip: Renaming a dataset</h3>  <ul>  <li>Click on the <i class="fas fa-pencil-alt" aria-hidden="true"></i><span class="visually-hidden"><span class="notranslate">galaxy</span>-pencil</span> <strong>pencil icon</strong> for the dataset to edit its attributes</li>  <li>In the central panel, change the <strong>Name</strong> field</li>  <li>Click the <strong>Save</strong> button</li></ul></blockquote>
    </li>
    <li>
      <p>Check that the datatype of all the three datasets is <code class="language-plaintext highlighter-rouge">tabular</code>.</p>

      <!--SNIPPET-->
      <blockquote class="tip">  <h3 data-toc-skip="" id="-tip-changing-the-datatype"><i class="far fa-lightbulb" aria-hidden="true"></i><span class="visually-hidden"></span> Tip: Changing the datatype</h3>  <ul>  <li>Click on the <i class="fas fa-pencil-alt" aria-hidden="true"></i><span class="visually-hidden"><span class="notranslate">galaxy</span>-pencil</span> <strong>pencil icon</strong> for the dataset to edit its attributes</li>  <li>In the central panel, click on the <i class="fas fa-database" aria-hidden="true"></i><span class="visually-hidden"><span class="notranslate">galaxy</span>-chart-select-data</span> <strong>Datatypes</strong> tab on the top</li>  <li>Select <code class="language-plaintext highlighter-rouge">datatypes</code></li>  <li>Click the <strong>Save</strong> button</li></ul></blockquote>
    </li>
  </ol>

</blockquote>

<h2 id="classification-of-mnist-dataset-images-with-cnn">Classification of MNIST dataset images with CNN</h2>

<p>In this section, we define a CNN and train it using MNIST dataset training data. The goal is to learn a model such that given an image
of a digit we can predict whether the digit (0 to 9). We then evaluate the trained CNN on the test dataset and plot the confusion matrix.</p>

<h3 id="create-a-deep-learning-model-architecture"><strong>Create a deep learning model architecture</strong></h3>

<blockquote class="notranslate hands_on">
  <h3 id="hands_on-hands-on-model-config"><i class="fas fa-pencil-alt" aria-hidden="true"></i><span class="visually-hidden">hands_on</span> Hands-on: Model config</h3>

  <ul>
    <li><span class="tool" data-tool="toolshed.g2.bx.psu.edu/repos/bgruening/keras_model_config/keras_model_config/0.5.0" title="Tested with toolshed.g2.bx.psu.edu/repos/bgruening/keras_model_config/keras_model_config/0.5.0"><strong>Create a deep learning model architecture</strong> <i class="fas fa-wrench" aria-hidden="true"></i><i aria-hidden="true" class="fas fa-cog"></i><span class="visually-hidden">Tool: <span class="notranslate">toolshed</span>.g2.bx.psu.edu/repos/bgruening/keras_model_config/keras_model_config/0.5.0</span></span>
      <ul>
        <li><em>‚ÄúSelect keras model type‚Äù</em>: <code class="language-plaintext highlighter-rouge">sequential</code></li>
        <li><em>‚Äúinput_shape‚Äù</em>: <code class="language-plaintext highlighter-rouge">(784,)</code></li>
        <li>In <em>‚ÄúLAYER‚Äù</em>:
          <ul>
            <li><i class="far fa-plus-square" aria-hidden="true"></i><span class="visually-hidden">param-repeat</span> <em>‚Äú1: LAYER‚Äù</em>:
              <ul>
                <li><em>‚ÄúChoose the type of layer‚Äù</em>: <code class="language-plaintext highlighter-rouge">Core -- Reshape</code>
                  <ul>
                    <li><em>‚Äútarget_shape‚Äù</em>: <code class="language-plaintext highlighter-rouge">(28,28,1)</code></li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><i class="far fa-plus-square" aria-hidden="true"></i><span class="visually-hidden">param-repeat</span> <em>‚Äú2: LAYER‚Äù</em>:
              <ul>
                <li><em>‚ÄúChoose the type of layer‚Äù</em>: <code class="language-plaintext highlighter-rouge">Convolutional -- Conv2D</code>
                  <ul>
                    <li><em>‚Äúfilters‚Äù</em>: <code class="language-plaintext highlighter-rouge">64</code></li>
                    <li><em>‚Äúkernel_size‚Äù</em>: <code class="language-plaintext highlighter-rouge">3</code></li>
                    <li><em>‚ÄúActivation function‚Äù</em>: <code class="language-plaintext highlighter-rouge">relu</code></li>
                    <li><em>‚ÄúType in key words arguments if different from the default‚Äù</em>: <code class="language-plaintext highlighter-rouge">padding='same'</code></li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><i class="far fa-plus-square" aria-hidden="true"></i><span class="visually-hidden">param-repeat</span> <em>‚Äú3: LAYER‚Äù</em>:
              <ul>
                <li><em>‚ÄúChoose the type of layer‚Äù</em>: <code class="language-plaintext highlighter-rouge">Pooling -- MaxPooling2D</code>
                  <ul>
                    <li><em>‚Äúpool_size‚Äù</em>: <code class="language-plaintext highlighter-rouge">(2,2)</code></li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><i class="far fa-plus-square" aria-hidden="true"></i><span class="visually-hidden">param-repeat</span> <em>‚Äú4: LAYER‚Äù</em>:
              <ul>
                <li><em>‚ÄúChoose the type of layer‚Äù</em>: <code class="language-plaintext highlighter-rouge">Convolutional -- Conv2D</code>
                  <ul>
                    <li><em>‚Äúfilters‚Äù</em>: <code class="language-plaintext highlighter-rouge">32</code></li>
                    <li><em>‚Äúkernel_size‚Äù</em>: <code class="language-plaintext highlighter-rouge">3</code></li>
                    <li><em>‚ÄúActivation function‚Äù</em>: <code class="language-plaintext highlighter-rouge">relu</code></li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><i class="far fa-plus-square" aria-hidden="true"></i><span class="visually-hidden">param-repeat</span> <em>‚Äú5: LAYER‚Äù</em>:
              <ul>
                <li><em>‚ÄúChoose the type of layer‚Äù</em>: <code class="language-plaintext highlighter-rouge">Pooling -- MaxPooling2D</code>
                  <ul>
                    <li><em>‚Äúpool_size‚Äù</em>: <code class="language-plaintext highlighter-rouge">(2,2)</code></li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><i class="far fa-plus-square" aria-hidden="true"></i><span class="visually-hidden">param-repeat</span> <em>‚Äú6: LAYER‚Äù</em>:
              <ul>
                <li><em>‚ÄúChoose the type of layer‚Äù</em>: <code class="language-plaintext highlighter-rouge">Core -- Flatten</code></li>
              </ul>
            </li>
            <li><i class="far fa-plus-square" aria-hidden="true"></i><span class="visually-hidden">param-repeat</span> <em>‚Äú7: LAYER‚Äù</em>:
              <ul>
                <li><em>‚ÄúChoose the type of layer‚Äù</em>: <code class="language-plaintext highlighter-rouge">Core -- Dense</code>
                  <ul>
                    <li><em>‚Äúunits‚Äù</em>‚Äù: <code class="language-plaintext highlighter-rouge">10</code></li>
                    <li><em>‚ÄúActivation function‚Äù</em>: <code class="language-plaintext highlighter-rouge">softmax</code></li>
                  </ul>
                </li>
              </ul>
            </li>
          </ul>
        </li>
        <li>Click <em>‚ÄúExecute‚Äù</em></li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>Each image is passed in as a 784 dimensional vector (28 x 28 = 784). The reshape layer reshapes it into (28, 28, 1) dimensions ‚Äì 28 rows (image height), 28 columns (image width), and
1 channel. Channel is 1 since the image is grayscale and each pixel can be represented by one integer. Color images are represented by 3 integers (RGB
values) and have channel size 3. Our CNN then has 2 convolution + pooling layers. First convolution layer has 64 filters (output would be 64 dimensional),
and filter size is 3 x 3. Second convolutional layer has 32 filters (output would be 32 dimensional), and filter size is 3 x 3. Both pooling layers are
MaxPool layers with pool size of 2 by 2. Afterwards, we flatten the previous layers output (every row/colum/channel would be an individual node). Finally,
we add a fully connected layer with 10 nodes and use a softmax activation function to get the probability of each digit. Digit with the highest
probability is predicted by CNN. The model config can be downloaded as a JSON file.</p>

<h3 id="create-a-deep-learning-model"><strong>Create a deep learning model</strong></h3>

<blockquote class="notranslate hands_on">
  <h3 id="hands_on-hands-on-model-builder-optimizer-loss-function-and-fit-parameters"><i class="fas fa-pencil-alt" aria-hidden="true"></i><span class="visually-hidden">hands_on</span> Hands-on: Model builder (Optimizer, loss function, and fit parameters)</h3>

  <ul>
    <li><span class="tool" data-tool="toolshed.g2.bx.psu.edu/repos/bgruening/keras_model_builder/keras_model_builder/0.5.0" title="Tested with toolshed.g2.bx.psu.edu/repos/bgruening/keras_model_builder/keras_model_builder/0.5.0"><strong>Create deep learning model</strong> <i class="fas fa-wrench" aria-hidden="true"></i><i aria-hidden="true" class="fas fa-cog"></i><span class="visually-hidden">Tool: <span class="notranslate">toolshed</span>.g2.bx.psu.edu/repos/bgruening/keras_model_builder/keras_model_builder/0.5.0</span></span>
      <ul>
        <li><em>‚ÄúChoose a building mode‚Äù</em>: <code class="language-plaintext highlighter-rouge">Build a training model</code></li>
        <li><em>‚ÄúSelect the dataset containing model configuration‚Äù</em>: Select the <em>Keras Model Config</em> from the previous step.</li>
        <li><em>‚ÄúDo classification or regression?‚Äù</em>: <code class="language-plaintext highlighter-rouge">KerasGClassifier</code></li>
        <li>In <em>‚ÄúCompile Parameters‚Äù</em>:
          <ul>
            <li><em>‚ÄúSelect a loss function‚Äù</em>: <code class="language-plaintext highlighter-rouge">categorical_crossentropy</code></li>
            <li><em>‚ÄúSelect an optimizer‚Äù</em>: <code class="language-plaintext highlighter-rouge">Adam - Adam optimizer </code></li>
            <li><em>‚ÄúSelect metrics‚Äù</em>: <code class="language-plaintext highlighter-rouge">acc/accuracy</code></li>
          </ul>
        </li>
        <li>In <em>‚ÄúFit Parameters‚Äù</em>:
          <ul>
            <li><em>‚Äúepochs‚Äù</em>: <code class="language-plaintext highlighter-rouge">2</code></li>
            <li><em>‚Äúbatch_size‚Äù</em>: <code class="language-plaintext highlighter-rouge">500</code></li>
          </ul>
        </li>
        <li>Click <em>‚ÄúExecute‚Äù</em></li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>A loss function measures how different the predicted output is versus the expected output. For multi-class classification problems, we use
<em>categorical cross entropy</em> as loss function. Epochs is the number of times the whole training data is used to train the model. Setting <em>epochs</em> to 2
means each training example in our dataset is used twice to train our model. If we update network weights/biases after all the training data is
feed to the network, the training will be very slow (as we have 60000 training examples in our dataset). To speed up the training, we present
only a subset of the training examples to the network, after which we update the weights/biases. <em>batch_size</em> decides the size of this subset.
The model builder can be downloaded as a zip file.</p>

<h3 id="deep-learning-training-and-evaluation"><strong>Deep learning training and evaluation</strong></h3>

<blockquote class="notranslate hands_on">
  <h3 id="hands_on-hands-on-training-the-model"><i class="fas fa-pencil-alt" aria-hidden="true"></i><span class="visually-hidden">hands_on</span> Hands-on: Training the model</h3>

  <ul>
    <li><span class="tool" data-tool="toolshed.g2.bx.psu.edu/repos/bgruening/keras_train_and_eval/keras_train_and_eval/1.0.8.2" title="Tested with toolshed.g2.bx.psu.edu/repos/bgruening/keras_train_and_eval/keras_train_and_eval/1.0.8.2"><strong>Deep learning training and evaluation</strong> <i class="fas fa-wrench" aria-hidden="true"></i><i aria-hidden="true" class="fas fa-cog"></i><span class="visually-hidden">Tool: <span class="notranslate">toolshed</span>.g2.bx.psu.edu/repos/bgruening/keras_train_and_eval/keras_train_and_eval/1.0.8.2</span></span>
      <ul>
        <li><em>‚ÄúSelect a scheme‚Äù</em>: <code class="language-plaintext highlighter-rouge">Train and Validate</code></li>
        <li><em>‚ÄúChoose the dataset containing pipeline/estimator object‚Äù</em>: Select the <em>Keras Model Builder</em> from the previous step.</li>
        <li><em>‚ÄúSelect input type:‚Äù</em>: <code class="language-plaintext highlighter-rouge">tabular data</code>
          <ul>
            <li><em>‚ÄúTraining samples dataset‚Äù</em>: Select <code class="language-plaintext highlighter-rouge">X_train</code> dataset</li>
            <li><em>‚ÄúChoose how to select data by column:‚Äù</em>: <code class="language-plaintext highlighter-rouge">All columns</code></li>
            <li><em>‚ÄúDataset containing class labels or target values‚Äù</em>: Select <code class="language-plaintext highlighter-rouge">y_train</code> dataset</li>
            <li><em>‚ÄúChoose how to select data by column:‚Äù</em>: <code class="language-plaintext highlighter-rouge">All columns</code></li>
          </ul>
        </li>
        <li>Click <em>‚ÄúExecute‚Äù</em></li>
      </ul>
    </li>
  </ul>

</blockquote>

<p>The training step generates 3 datasets. 1) accuracy of the trained model, 2) the trained model, downloadable as a zip file, and 3) the trained
model weights, downloadable as an hdf5 file. These files are needed for prediction in the next step.</p>

<h3 id="model-prediction"><strong>Model Prediction</strong></h3>

<blockquote class="notranslate hands_on">
  <h3 id="hands_on-hands-on-testing-the-model"><i class="fas fa-pencil-alt" aria-hidden="true"></i><span class="visually-hidden">hands_on</span> Hands-on: Testing the model</h3>

  <ul>
    <li><span class="tool" data-tool="toolshed.g2.bx.psu.edu/repos/bgruening/model_prediction/model_prediction/1.0.8.2" title="Tested with toolshed.g2.bx.psu.edu/repos/bgruening/model_prediction/model_prediction/1.0.8.2"><strong>Model Prediction</strong> <i class="fas fa-wrench" aria-hidden="true"></i><i aria-hidden="true" class="fas fa-cog"></i><span class="visually-hidden">Tool: <span class="notranslate">toolshed</span>.g2.bx.psu.edu/repos/bgruening/model_prediction/model_prediction/1.0.8.2</span></span>
      <ul>
        <li><em>‚ÄúChoose the dataset containing pipeline/estimator object‚Äù</em> : Select the trained model from the previous step.</li>
        <li><em>‚ÄúChoose the dataset containing weights for the estimator above‚Äù</em> : Select the trained model weights from the previous step.</li>
        <li><em>‚ÄúSelect invocation method‚Äù</em>: <code class="language-plaintext highlighter-rouge">predict</code></li>
        <li><em>‚ÄúSelect input data type for prediction‚Äù</em>: <code class="language-plaintext highlighter-rouge">tabular data</code></li>
        <li><em>‚ÄúTraining samples dataset‚Äù</em>: Select <code class="language-plaintext highlighter-rouge">X_test</code> dataset</li>
        <li><em>‚ÄúChoose how to select data by column:‚Äù</em>: <code class="language-plaintext highlighter-rouge">All columns</code></li>
        <li>Click <em>‚ÄúExecute‚Äù</em></li>
      </ul>
    </li>
  </ul>

</blockquote>

<p>The prediction step generates 1 dataset. It‚Äôs a file that has predictions (0 to 9 for the predicted digits) for every image in the test dataset.</p>

<h3 id="machine-learning-visualization-extension"><strong>Machine Learning Visualization Extension</strong></h3>

<blockquote class="notranslate hands_on">
  <h3 id="hands_on-hands-on-creating-the-confusion-matrix"><i class="fas fa-pencil-alt" aria-hidden="true"></i><span class="visually-hidden">hands_on</span> Hands-on: Creating the confusion matrix</h3>

  <ul>
    <li><span class="tool" data-tool="toolshed.g2.bx.psu.edu/repos/bgruening/ml_visualization_ex/ml_visualization_ex/1.0.8.2" title="Tested with toolshed.g2.bx.psu.edu/repos/bgruening/ml_visualization_ex/ml_visualization_ex/1.0.8.2"><strong>Machine Learning Visualization Extension</strong> <i class="fas fa-wrench" aria-hidden="true"></i><i aria-hidden="true" class="fas fa-cog"></i><span class="visually-hidden">Tool: <span class="notranslate">toolshed</span>.g2.bx.psu.edu/repos/bgruening/ml_visualization_ex/ml_visualization_ex/1.0.8.2</span></span>
      <ul>
        <li><em>‚ÄúSelect a plotting type‚Äù</em>: <code class="language-plaintext highlighter-rouge">Confusion matrix for classes</code></li>
        <li><em>‚ÄúSelect dataset containing the true labels‚Äù</em>‚Äù: <code class="language-plaintext highlighter-rouge">y_test</code></li>
        <li><em>‚ÄúChoose how to select data by column:‚Äù</em>: <code class="language-plaintext highlighter-rouge">All columns</code></li>
        <li><em>‚ÄúSelect dataset containing the predicted labels‚Äù</em>‚Äù: Select <code class="language-plaintext highlighter-rouge">Model Prediction</code> from the previous step</li>
        <li><em>‚ÄúDoes the dataset contain header:‚Äù</em>: <code class="language-plaintext highlighter-rouge">Yes</code></li>
        <li>Click <em>‚ÄúExecute‚Äù</em></li>
      </ul>
    </li>
  </ul>

</blockquote>

<p><strong>Confusion Matrix</strong> is a table that describes the performance of a classification model. It lists the number of examples that were correctly
classified by the model, True positives (TP) and true negatives (TN). It also lists the number of examples that were classified as positive that
were actually negative (False positive, FP, or Type I error), and the number of examples that were classified as negative that were actually
positive (False negative, FN, or Type 2 error). Given the confusion matrix, we can calculate <strong>precision</strong> and
<strong>recall</strong> <span class="citation"><a href="#TatbulEtAl">Tatbul <i>et al.</i> 2018</a></span>. Precision is the fraction of predicted positives that are true positives (Precision = TP / (TP + FP)). Recall
is the fraction of true positives that are predicted (Recall = TP / (TP + FN)). One way to describe the confusion matrix with just one value is
to use the <strong>F score</strong>, which is the harmonic mean of precision and recall</p>

\[Precision = \frac{\text{True positives}}{\text{True positives + False positives}}\]

\[Recall = \frac{\text{True positives}}{\text{True positives + False negatives}}\]

\[F score = \frac{2 * \text{Precision * Recall}}{\text{Precision + Recall}}\]

<figure id="figure-11"><img src="../../images/Conv_confusion_matrix.png" alt="Confusion matrix for MNIST image classification problem. " loading="lazy" /><figcaption><span class="figcaption-prefix">Figure 11:</span> Image classification confusion matrix</figcaption></figure>

<p>Figure 11 is the resultant confusion matrix for our image problem. The first row in the table represents the <em>true</em> digit 0 class labels (we have
967 + 2 + 1 + 8 + 2 = 980 digit 0 images). The second row represents the <em>true</em> digit 1 class labels (Again, we have 1,132 + 1 + 2 = 1,135 digit 1
images). Similarly, you can count the true class labels for digits 2 to 9 by adding up the numbers in the coresponding row. The first column from the
left represents the <em>predicted</em> digit 0 class labels (Our CNN predicted 967 + 4 + 2 + 8 +7 + 2 = 990 images as being digit 0). The second column from
the left represents the <em>predicted</em> digit 1 class labels (Our CNN predicted 1,132 + 7 + 3 + 12 + 3 + 5 = 1,162 images as being digit 1). Similarly,
you can count the predicted class labels for digits 2 to 9 by adding up the numbers in the corresponding column.</p>

<p>For digit 0, looking at the top-left green cell, we see that our CNN has correctly predicted 967 images as digit 0 (True positives). Adding the numbers
in the left most column besides the True positives (4 + 2 + 8 + 7 + 2 = 23), we see that our CNN has incorrectly predicted 23 images as being digit 0
(False positives). Adding the numbers on the top row besides the True positives (2 + 1 + 8 + 2 = 13), we see that our CNN has incorrectly predicted
13 digits 0 images as not being digit 0 (False negatives). Given these numbers we can calculate Precision, Recall, and the F score for digit 0 as
follows:</p>

\[Precision = \frac{\text{True positives}}{\text{True positives + False positives}} = \frac{967}{967 + 23} = 0.97\]

\[Recall = \frac{\text{True positives}}{\text{True positives + False negatives}} = \frac{967}{967 + 13} = 0.98\]

\[F score = \frac{2 * \text{Precision * Recall}}{\text{Precision + Recall}} = \frac{2 * 0.97 * 0.98}{0.97 + 0.98} = 0.97\]

<p>You can calculate the Precision, Recall, and F score for other digits in a similar manner.</p>

<h1 class="no_toc" id="conclusion">Conclusion</h1>

<p>In this tutorial, we explained the motivation for convolutional neural networks, explained their architecture, and discussed convolution
operator and its parameters. We then used <span class="notranslate">Galaxy</span> to solve an image classification problem using CNN on MNIST dataset.</p>


                

                <h1>Frequently Asked Questions</h1>
                Have questions about this tutorial? Check out the  <a href="/training-material/topics/statistics/faqs/">FAQ page for the Statistics and machine learning topic</a> to see if your question is listed there.
                If not, please ask your question on the <a href="https://gitter.im/Galaxy-Training-Network/Lobby">GTN Gitter Channel</a> or the
                <a href="https://help.galaxyproject.org">Galaxy Help Forum</a>

                

                
                <h1 id="bibliography">References</h1>
                <ol class="bibliography"><li id="HubelWiesel">Hubel, D. H., and T. N. Wiesel, 1959 <b>Receptive fields of single neurones in the cat‚Äôs striate cortex</b>. The Journal of physiology 148: 574‚Äì591. <a href="https://doi.org/10.1113/jphysiol.1959.sp006308">10.1113/jphysiol.1959.sp006308</a></li>
<li id="Fukishima">Fukushima, K., 1988 <b>Neocognitron: A hierarchical neural network capable of visual pattern recognition</b>. Neural Networks 1: 119‚Äì130. <a href="https://doi.org/https://doi.org/10.1016/0893-6080(88)90014-7">https://doi.org/10.1016/0893-6080(88)90014-7</a> <a href="https://www.sciencedirect.com/science/article/pii/0893608088900147">https://www.sciencedirect.com/science/article/pii/0893608088900147</a></li>
<li id="LeCunEtAl">LeCun, Y., B. Boser, J. S. Denker, D. Henderson, R. E. Howard <i>et al.</i>, 1989 <b>Backpropagation Applied to Handwritten Zip Code Recognition</b>. Neural Computation 1: 541‚Äì551. <a href="https://doi.org/10.1162/neco.1989.1.4.541">10.1162/neco.1989.1.4.541</a></li>
<li id="JainEtAl">Jain, A. K., J. Mao, and K. M. Mohiuddin, 1996 <b>Artificial neural networks: a tutorial</b>. Computer 29: 31‚Äì44. <a href="https://doi.org/10.1109/2.485891">10.1109/2.485891</a></li>
<li id="LeCunMnist">LeCun, Y., C. Cortes, and C. J. Burges, 2010 <b>MNIST handwritten digit database</b>. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist 2:</li>
<li id="KrizhevskyEtAl">Krizhevsky, A., I. Sutskever, and G. Hinton, 2012 <b>ImageNet Classification with Deep Convolutional Neural Networks</b>. Neural Information Processing Systems 25: <a href="https://doi.org/10.1145/3065386">10.1145/3065386</a></li>
<li id="ZeilerEtAl">Zeiler, M. D., and R. Fergus, 2013 <b>Visualizing and Understanding Convolutional Networks</b>. ECCV. <a href="https://doi.org/1311.2901">1311.2901</a></li>
<li id="SzegedyEtAl">Szegedy, C., W. Liu, Y. Jia, P. Sermanet, S. Reed <i>et al.</i>, 2014 <b>Going Deeper with Convolutions</b>. CVPR. <a href="https://doi.org/1409.4842">1409.4842</a></li>
<li id="HeEtAl">He, K., X. Zhang, S. Ren, and J. Sun, 2015 <b>Deep Residual Learning for Image Recognition</b>. CVPR. <a href="https://doi.org/1512.03385">1512.03385</a></li>
<li id="OSheaEtAl">O‚ÄôShea, K., and R. Nash, 2015 <b>An Introduction to Convolutional Neural Networks</b>. CoRR abs/1511.08458: <a href="http://arxiv.org/abs/1511.08458">http://arxiv.org/abs/1511.08458</a></li>
<li id="RussakovskyEtAl">Russakovsky, O., J. Deng, H. Su, J. Krause, S. Satheesh <i>et al.</i>, 2015 <b>ImageNet Large Scale Visual Recognition Challenge</b>. International Journal of Computer Vision 115: 2211‚Äì252. <a href="https://doi.org/10.1007/s11263-015-0816-y">10.1007/s11263-015-0816-y</a></li>
<li id="SimonyanEtAl">Simonyan, K., and A. Zisserman, 2015 <b>Very Deep Convolutional Networks for Large-Scale Image Recognition</b>. CVPR. <a href="https://doi.org/1409.1556">1409.1556</a></li>
<li id="DumoulinVisin">Dumoulin, V., and F. Visin, 2016 <b>A guide to convolution arithmetic for deep learning</b>. <a href="https://doi.org/1603.07285">1603.07285</a></li>
<li id="NwankpaEtAl">Nwankpa, C., W. Ijomah, A. Gachagan, and S. Marshall, 2018 <b>Activation Functions: Comparison of trends in Practice and Research for Deep Learning</b>. CoRR abs/1811.03378: <a href="http://arxiv.org/abs/1811.03378">http://arxiv.org/abs/1811.03378</a></li>
<li id="TatbulEtAl">Tatbul, N., T. J. Lee, S. Zdonik, M. Alam, and J. Gottschlich, 2018 <b>Precision and Recall for Time Series</b>. Advances in Neural Information Processing Systems 31: 1920‚Äì1930. <a href="https://proceedings.neurips.cc/paper/2018/file/8f468c873a32bb0619eaeb2050ba45d1-Paper.pdf">https://proceedings.neurips.cc/paper/2018/file/8f468c873a32bb0619eaeb2050ba45d1-Paper.pdf</a></li></ol>
                

                

                <h1>Feedback</h1>
                <p class="text-muted">Did you use this material as an instructor? Feel free to give us feedback on <a href="https://github.com/galaxyproject/training-material/issues/1452" target="_blank">how it went</a>.</p>

                <div id="feedback-button">
                    <img src="/training-material/shared/images/feedback.png" title="Click to activate" alt="Click here to load Google feedback frame" />
                </div>
                <div id="feedback-form">
                </div>
                <script type="text/javascript">
                    (function (window, document) {
                        function onDocumentReady(fn) {
                            if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
                                fn();
                            } else {
                                document.addEventListener('DOMContentLoaded', fn);
                            }
                        }

                        onDocumentReady(function () {
                            $("#feedback-button").click(function(evt){
                                var e = $(evt.target)
                                e.hide();

                                $("#feedback-form").html(`
                                    <iframe id="feedback-google" class="google-form" src="https://docs.google.com/forms/d/e/1FAIpQLSd4VZptFTQ03kHkMz0JyW9b6_S8geU5KjNE_tLM0dixT3ZQmA/viewform?embedded=true&entry.1235803833=Deep Learning (Part 3) - Convolutional neural networks (CNN) (Statistics and machine learning)">Loading...</iframe>
                                `)
                            })
                        });
                    })(window, document);
                </script>



                <h1>Citing this Tutorial</h1>
                <p>
                    <ol>
                        <li id="citation-text">Kaivan Kamali, 2021 <b>Deep Learning (Part 3) - Convolutional neural networks (CNN) (Galaxy Training Materials)</b>. <a href="https://training.galaxyproject.org/training-material/topics/statistics/tutorials/CNN/tutorial.html">https://training.galaxyproject.org/training-material/topics/statistics/tutorials/CNN/tutorial.html</a> Online; accessed TODAY
                        </li>
                        <li>
                        Batut et al., 2018 <b>Community-Driven Data Analysis Training for Biology</b> Cell Systems <a href="https://doi.org/10.1016%2Fj.cels.2018.05.012">10.1016/j.cels.2018.05.012</a>
                        </li>
                    </ol>
                </p>


                <blockquote class="details">
                  <h3><i class="fa fa-info-circle" aria-hidden="true"></i><span class="visually-hidden">details</span> BibTeX</h3>
                  <p style="display: none;">

                <div class="highlighter-rouge"><div class="highlight"><pre class="highlight">
<code id="citation-code">@misc{statistics-CNN,
author = "Kaivan Kamali",
title = "Deep Learning (Part 3) - Convolutional neural networks (CNN) (Galaxy Training Materials)",
year = "2021",
month = "05",
day = "24"
url = "\url{https://training.galaxyproject.org/training-material/topics/statistics/tutorials/CNN/tutorial.html}",
note = "[Online; accessed TODAY]"
}
@article{Batut_2018,
    doi = {10.1016/j.cels.2018.05.012},
    url = {https://doi.org/10.1016%2Fj.cels.2018.05.012},
    year = 2018,
    month = {jun},
    publisher = {Elsevier {BV}},
    volume = {6},
    number = {6},
    pages = {752--758.e1},
    author = {B{\'{e}}r{\'{e}}nice Batut and Saskia Hiltemann and Andrea Bagnacani and Dannon Baker and Vivek Bhardwaj and Clemens Blank and Anthony Bretaudeau and Loraine Brillet-Gu{\'{e}}guen and Martin {\v{C}}ech and John Chilton and Dave Clements and Olivia Doppelt-Azeroual and Anika Erxleben and Mallory Ann Freeberg and Simon Gladman and Youri Hoogstrate and Hans-Rudolf Hotz and Torsten Houwaart and Pratik Jagtap and Delphine Larivi{\`{e}}re and Gildas Le Corguill{\'{e}} and Thomas Manke and Fabien Mareuil and Fidel Ram{\'{\i}}rez and Devon Ryan and Florian Christoph Sigloch and Nicola Soranzo and Joachim Wolff and Pavankumar Videm and Markus Wolfien and Aisanjiang Wubuli and Dilmurat Yusuf and James Taylor and Rolf Backofen and Anton Nekrutenko and Bj√∂rn Gr√ºning},
    title = {Community-Driven Data Analysis Training for Biology},
    journal = {Cell Systems}
}</code>
                </pre></div></div>
                </p>
                </blockquote>


<script type="text/javascript">
// update the date on load, or leave fallback of 'today'
d = new Date();
document.getElementById("citation-code").innerHTML = document.getElementById("citation-code").innerHTML.replace("TODAY", d.toDateString());
document.getElementById("citation-text").innerHTML = document.getElementById("citation-text").innerHTML.replace("TODAY", d.toDateString());
</script>

                <h3><i class="far fa-thumbs-up" aria-hidden="true"></i> Congratulations on successfully completing this tutorial!</h3>

                

                

            </div>
        </div>
    </div>
</section>
<br/>
<br/>
<br/>

        </div>
        
    </body>
    <script type="text/javascript" src="/training-material/assets/js/jquery.slim.min.js"></script>
    <script type="text/javascript" src="/training-material/assets/js/popper.min.js"></script>
    <script type="text/javascript" src="/training-material/assets/js/bootstrap.min.js?v=3"></script>
    <script type="text/javascript" src="/training-material/assets/js/details-element-polyfill.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="/training-material/assets/js/bootstrap-toc.min.js"></script>
    <script type="text/javascript" src="/training-material/assets/js/main.js"></script>
    <script type="text/javascript" src="/training-material/assets/js/theme.js"></script>

    <script type="text/javascript" src="/training-material/assets/js/clipboard.min.js"></script>
    <script type="text/javascript">
    var snippets=document.querySelectorAll('div.highlight');
    [].forEach.call(snippets,function(snippet){
        snippet.firstChild.insertAdjacentHTML('beforebegin','<button class="btn btn-light" data-clipboard-snippet><i class="fa fa-copy"></i>&nbsp;Copy</button>');
    });

    var clipboardSnippets=new ClipboardJS('[data-clipboard-snippet]',{
        target:function(trigger){return trigger.nextElementSibling;
    }});
    </script>
    

    <script type="text/javascript">
        if(window.location.hostname === "galaxyproject.github.io") {
            // Redirect
            var redirect = "https://training.galaxyproject.org" + window.location.pathname + window.location.search;
            $('div.container.main-content').prepend("<div class='alert alert-warning'><strong>Note: </strong>This content has a new home at <a href=\"" + redirect + "\">" + redirect + "</a>, which you will be redirected to in 5 seconds.</div>");

            window.setTimeout(function(){
                window.location.href = redirect;
            }, 5000)

        }
    </script>
</html>
