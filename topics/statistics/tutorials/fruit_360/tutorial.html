<!DOCTYPE html>
<html lang="en" dir="auto">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Hands-on: Image classification in Galaxy with fruit 360 dataset / Image classification in Galaxy with fruit 360 dataset / Statistics and machine learning</title>
        
            <meta name="google-site-verification" content="9mOXn2JL833-i7-aioCCEuIdG4_tb6qjwUozB5GJnPQ" />

<!-- JavaScript Error Monitoring, and performance tracking. -->
<script
  src="https://browser.sentry-cdn.com/7.52.1/bundle.tracing.min.js"
  integrity="sha384-muuFXKS3752PNA4rPm9Uq6BLvOfV4CXyr9MHDBPvozOJJUWLKkogEFWOIRoVps43"
  crossorigin="anonymous"
></script>
<script type="text/javascript">
if(localStorage.getItem('sentry-opt-out') !== 'opt-out' && navigator.doNotTrack !== "1") {
	console.log("Sentry: opt-in");
	Sentry.init({
		dsn: "https://45e0ec6e4373462b92969505df37cf40@sentry.galaxyproject.org/10",
		release: "galaxy-training-network@066679f25030e7855a8739c65b9c688a61009b66",
		integrations: [new Sentry.BrowserTracing(), new Sentry.Replay()],
		sampleRate: 0.1,
		tracesSampleRate: 0.1,
		// Capture Replay for no sessions by default
		replaysSessionSampleRate: 0.01,
		// plus for 1% of sessions with an error
		replaysOnErrorSampleRate: 0.01,
		// PII OFF
		sendDefaultPii: false, // Off by default but just in case.
		environment: "production",
	});
}
</script>

<!-- Page view tracking -->
<script defer data-domain="training.galaxyproject.org" src="https://plausible.galaxyproject.eu/js/plausible.js"></script>
<script>
if(localStorage.getItem('plausible-opt-out') !== 'opt-out' && navigator.doNotTrack !== "1") {
	localStorage.removeItem("plausible_ignore")
	console.log("Plausible: opt-in");
	window.plausible = window.plausible || function() { (window.plausible.q = window.plausible.q || []).push(arguments) }
} else {
	// if they're opting-out, or DNT
	// we might get one page by accident but we won't get future ones.
	localStorage.setItem("plausible_ignore", "true")
}
</script>

        
        <link rel="shortcut icon" href="/training-material/favicon.ico" type="image/x-icon">
        <link rel="alternate" type="application/atom+xml" href="/training-material/feed.xml">
        <link rel="canonical" href="https://training.galaxyproject.org/training-material/topics/statistics/tutorials/fruit_360/tutorial.html">
        <link rel="license" href="https://spdx.org/licenses/CC-BY-4.0">
        <link rel="preload" href="/training-material/assets/fonts/AtkinsonHyperlegible/Atkinson-Hyperlegible-Regular-102a.woff2" as="font" type="font/woff2" crossorigin>
        <link rel="preload" href="/training-material/assets/fonts/AtkinsonHyperlegible/Atkinson-Hyperlegible-Bold-102a.woff2" as="font" type="font/woff2" crossorigin>
        <link rel="preload" href="/training-material/assets/fonts/AtkinsonHyperlegible/Atkinson-Hyperlegible-Italic-102a.woff2" as="font" type="font/woff2" crossorigin>
        
        <link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" as="script" crossorigin>
        
        <link rel="preload" href="/training-material/assets/css/main.css?v=3" as="style">
        <link rel='preload' href='/training-material/assets/js/bundle.theme.f1f2de89.js' as='script'>
<link rel='preload' href='/training-material/assets/js/bundle.main.40d4e218.js' as='script'>
        <link rel="stylesheet" href="/training-material/assets/css/main.css?v=3">
        <link rel="manifest" href="/training-material/manifest.json">
        <meta name="theme-color" content="#2c3143"/>
	

        <meta name="DC.identifier" content="https://github.com/galaxyproject/training-material">
<meta name="DC.type" content="text">
<meta name="DC.title" content="Image classification in Galaxy with fruit 360 dataset">
<meta name="DC.publisher" content="Galaxy Training Network">
<meta name="DC.date" content="2024-07-09 14:29:46 +0000">
<meta name="DC.creator" content="Kaivan Kamali"><meta name="description" content="Statistical Analyses for omics data and machine learning using Galaxy tools">
        <meta property="og:site_name" content="Galaxy Training Network">
	<meta property="og:title" content="Statistics and machine learning / Image classification in Galaxy with fruit 360 dataset / Hands-on: Image classification in Galaxy with fruit 360 dataset">
        <meta property="og:description" content="Statistical Analyses for omics data and machine learning using Galaxy tools">
        <meta property="og:image" content="https://galaxy-training.s3.amazonaws.com/social/topics/statistics/tutorials/fruit_360/tutorial.png">
	<script type="application/ld+json">


{
  "@context": "http://schema.org",
  "@type": "LearningResource",
  "http://purl.org/dc/terms/conformsTo": {
    "@id": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
    "@type": "CreativeWork"
  },
  "audience": {
    "@type": "EducationalAudience",
    "educationalRole": "Students"
  },
  "citation": [
    {
      "@type": "CreativeWork",
      "name": "Galaxy Training: A Powerful Framework for Teaching!",
      "url": "https://doi.org/10.1371/journal.pcbi.1010752"
    },
    {
      "@type": "CreativeWork",
      "name": "Community-Driven Data Analysis Training for Biology",
      "url": "https://doi.org/10.1016/j.cels.2018.05.012"
    }
  ],
  "discussionUrl": "https://gitter.im/Galaxy-Training-Network/Lobby",
  "headline": "Image classification in Galaxy with fruit 360 dataset",
  "interactivityType": "mixed",
  "isAccessibleForFree": true,
  "isFamilyFriendly": true,
  "license": "https://spdx.org/licenses/CC-BY-4.0.html",
  "producer": {
    "@type": "Organization",
    "http://purl.org/dc/terms/conformsTo": {
      "@id": "https://bioschemas.org/profiles/Organization/0.2-DRAFT-2019_07_19",
      "@type": "Organization"
    },
    "id": "https://training.galaxyproject.org",
    "email": "galaxytrainingnetwork@gmail.com",
    "name": "Galaxy Training Network",
    "legalName": "Galaxy Training Network",
    "alternateName": "GTN",
    "url": "https://training.galaxyproject.org",
    "logo": "https://training.galaxyproject.org/training-material/assets/images/GTNLogo1000.png",
    "fundingModel": "The GTN's infrastructure relies on GitHub and the Galaxy Project for hosting costs. There are no full time paid staff members of the GTN. Individuals are occasionally funded on GTN-adjacent projects.",
    "keywords": [
      "galaxy",
      "bioinformatics",
      "training",
      "fair",
      "accessible"
    ],
    "status": "active",
    "foundingDate": "2015-06-29",
    "socialMedia": "https://mstdn.science/@gtn",
    "type": "project"
  },
  "provider": {
    "@type": "Organization",
    "http://purl.org/dc/terms/conformsTo": {
      "@id": "https://bioschemas.org/profiles/Organization/0.2-DRAFT-2019_07_19",
      "@type": "Organization"
    },
    "id": "https://training.galaxyproject.org",
    "email": "galaxytrainingnetwork@gmail.com",
    "name": "Galaxy Training Network",
    "legalName": "Galaxy Training Network",
    "alternateName": "GTN",
    "url": "https://training.galaxyproject.org",
    "logo": "https://training.galaxyproject.org/training-material/assets/images/GTNLogo1000.png",
    "fundingModel": "The GTN's infrastructure relies on GitHub and the Galaxy Project for hosting costs. There are no full time paid staff members of the GTN. Individuals are occasionally funded on GTN-adjacent projects.",
    "keywords": [
      "galaxy",
      "bioinformatics",
      "training",
      "fair",
      "accessible"
    ],
    "status": "active",
    "foundingDate": "2015-06-29",
    "socialMedia": "https://mstdn.science/@gtn",
    "type": "project"
  },
  "sourceOrganization": {
    "@type": "Organization",
    "http://purl.org/dc/terms/conformsTo": {
      "@id": "https://bioschemas.org/profiles/Organization/0.2-DRAFT-2019_07_19",
      "@type": "Organization"
    },
    "id": "https://training.galaxyproject.org",
    "email": "galaxytrainingnetwork@gmail.com",
    "name": "Galaxy Training Network",
    "legalName": "Galaxy Training Network",
    "alternateName": "GTN",
    "url": "https://training.galaxyproject.org",
    "logo": "https://training.galaxyproject.org/training-material/assets/images/GTNLogo1000.png",
    "fundingModel": "The GTN's infrastructure relies on GitHub and the Galaxy Project for hosting costs. There are no full time paid staff members of the GTN. Individuals are occasionally funded on GTN-adjacent projects.",
    "keywords": [
      "galaxy",
      "bioinformatics",
      "training",
      "fair",
      "accessible"
    ],
    "status": "active",
    "foundingDate": "2015-06-29",
    "socialMedia": "https://mstdn.science/@gtn",
    "type": "project"
  },
  "workTranslation": [

  ],
  "creativeWorkStatus": "Active",
  "dateModified": "2024-07-09 14:29:46 +0000",
  "datePublished": "2021-12-01 15:54:59 +0000",
  "copyrightHolder": {
    "@type": "Organization",
    "http://purl.org/dc/terms/conformsTo": {
      "@id": "https://bioschemas.org/profiles/Organization/0.2-DRAFT-2019_07_19",
      "@type": "Organization"
    },
    "id": "https://training.galaxyproject.org",
    "email": "galaxytrainingnetwork@gmail.com",
    "name": "Galaxy Training Network",
    "legalName": "Galaxy Training Network",
    "alternateName": "GTN",
    "url": "https://training.galaxyproject.org",
    "logo": "https://training.galaxyproject.org/training-material/assets/images/GTNLogo1000.png",
    "fundingModel": "The GTN's infrastructure relies on GitHub and the Galaxy Project for hosting costs. There are no full time paid staff members of the GTN. Individuals are occasionally funded on GTN-adjacent projects.",
    "keywords": [
      "galaxy",
      "bioinformatics",
      "training",
      "fair",
      "accessible"
    ],
    "status": "active",
    "foundingDate": "2015-06-29",
    "socialMedia": "https://mstdn.science/@gtn",
    "type": "project"
  },
  "funder": [

  ],
  "funding": [

  ],
  "identifier": "https://gxy.io/GTN:T00265",
  "accessMode": [
    "textual",
    "visual"
  ],
  "accessModeSufficient": [
    "textual",
    "visual"
  ],
  "accessibilityControl": [
    "fullKeyboardControl",
    "fullMouseControl"
  ],
  "accessibilityFeature": [
    "alternativeText",
    "tableOfContents"
  ],
  "accessibilitySummary": "The text aims to be as accessible as possible. Image descriptions will vary per tutorial, from images being completely inaccessible, to images with good descriptions for non-visual users.",
  "isPartOf": {
    "@type": "CreativeWork",
    "name": "Statistics and machine learning",
    "description": "Statistical Analyses for omics data and machine learning using Galaxy tools",
    "url": "https://training.galaxyproject.org/training-material/topics/statistics/"
  },
  "abstract": "The classification of fruits and vegetables offers many useful applications such as",
  "learningResourceType": "e-learning",
  "name": "Image classification in Galaxy with fruit 360 dataset",
  "url": "https://training.galaxyproject.org/training-material/topics/statistics/tutorials/fruit_360/tutorial.html",
  "version": 8,
  "timeRequired": "PT2H",
  "teaches": "- Learn how to create a CNN using Galaxy's deep learning tools\n- Solve an image classification problem on fruit 360 dataset using CNN in Galaxy",
  "keywords": [
    "Statistics and machine learning"
  ],
  "description": "## Abstract\n\nThe classification of fruits and vegetables offers many useful applications such as\n\n\n## About This Material\n\nThis is a Hands-on Tutorial from the GTN which is usable either for individual self-study, or as a teaching material in a classroom.\n\n\n## Questions this  will address\n\n - How to solve an image classification problem using convolutional neural network (CNN)?\n\n\n## Learning Objectives\n\n- Learn how to create a CNN using Galaxy's deep learning tools\n- Solve an image classification problem on fruit 360 dataset using CNN in Galaxy\n\n",
  "inLanguage": {
    "@type": "Language",
    "name": "English",
    "alternateName": "en"
  },
  "competencyRequired": [
    {
      "@context": "http://schema.org",
      "@type": "LearningResource",
      "url": "https://training.galaxyproject.org/training-material/topics/introduction/",
      "name": "Introduction to Galaxy Analyses",
      "description": "Introduction to Galaxy Analyses",
      "provider": {
        "@type": "Organization",
        "http://purl.org/dc/terms/conformsTo": {
          "@id": "https://bioschemas.org/profiles/Organization/0.2-DRAFT-2019_07_19",
          "@type": "Organization"
        },
        "id": "https://training.galaxyproject.org",
        "email": "galaxytrainingnetwork@gmail.com",
        "name": "Galaxy Training Network",
        "legalName": "Galaxy Training Network",
        "alternateName": "GTN",
        "url": "https://training.galaxyproject.org",
        "logo": "https://training.galaxyproject.org/training-material/assets/images/GTNLogo1000.png",
        "fundingModel": "The GTN's infrastructure relies on GitHub and the Galaxy Project for hosting costs. There are no full time paid staff members of the GTN. Individuals are occasionally funded on GTN-adjacent projects.",
        "keywords": [
          "galaxy",
          "bioinformatics",
          "training",
          "fair",
          "accessible"
        ],
        "status": "active",
        "foundingDate": "2015-06-29",
        "socialMedia": "https://mstdn.science/@gtn",
        "type": "project"
      }
    },
    {
      "@context": "http://schema.org",
      "@type": "LearningResource",
      "url": "https://training.galaxyproject.org/training-material/topics/statistics/tutorials/CNN/slides.html",
      "name": "Convolutional neural networks (CNN) \n Deep Learning - Part 3",
      "description": "Slides for 'Convolutional neural networks (CNN) \n Deep Learning - Part 3' tutorial",
      "learningResourceType": "slides",
      "interactivityType": "expositive",
      "provider": {
        "@type": "Organization",
        "http://purl.org/dc/terms/conformsTo": {
          "@id": "https://bioschemas.org/profiles/Organization/0.2-DRAFT-2019_07_19",
          "@type": "Organization"
        },
        "id": "https://training.galaxyproject.org",
        "email": "galaxytrainingnetwork@gmail.com",
        "name": "Galaxy Training Network",
        "legalName": "Galaxy Training Network",
        "alternateName": "GTN",
        "url": "https://training.galaxyproject.org",
        "logo": "https://training.galaxyproject.org/training-material/assets/images/GTNLogo1000.png",
        "fundingModel": "The GTN's infrastructure relies on GitHub and the Galaxy Project for hosting costs. There are no full time paid staff members of the GTN. Individuals are occasionally funded on GTN-adjacent projects.",
        "keywords": [
          "galaxy",
          "bioinformatics",
          "training",
          "fair",
          "accessible"
        ],
        "status": "active",
        "foundingDate": "2015-06-29",
        "socialMedia": "https://mstdn.science/@gtn",
        "type": "project"
      }
    },
    {
      "@context": "http://schema.org",
      "@type": "LearningResource",
      "url": "https://training.galaxyproject.org/training-material/topics/statistics/tutorials/CNN/tutorial.html",
      "name": "Deep Learning (Part 3) - Convolutional neural networks (CNN)",
      "description": "Hands-on for 'Deep Learning (Part 3) - Convolutional neural networks (CNN)' tutorial",
      "learningResourceType": "e-learning",
      "interactivityType": "expositive",
      "provider": {
        "@type": "Organization",
        "http://purl.org/dc/terms/conformsTo": {
          "@id": "https://bioschemas.org/profiles/Organization/0.2-DRAFT-2019_07_19",
          "@type": "Organization"
        },
        "id": "https://training.galaxyproject.org",
        "email": "galaxytrainingnetwork@gmail.com",
        "name": "Galaxy Training Network",
        "legalName": "Galaxy Training Network",
        "alternateName": "GTN",
        "url": "https://training.galaxyproject.org",
        "logo": "https://training.galaxyproject.org/training-material/assets/images/GTNLogo1000.png",
        "fundingModel": "The GTN's infrastructure relies on GitHub and the Galaxy Project for hosting costs. There are no full time paid staff members of the GTN. Individuals are occasionally funded on GTN-adjacent projects.",
        "keywords": [
          "galaxy",
          "bioinformatics",
          "training",
          "fair",
          "accessible"
        ],
        "status": "active",
        "foundingDate": "2015-06-29",
        "socialMedia": "https://mstdn.science/@gtn",
        "type": "project"
      }
    }
  ],
  "author": [
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "http://purl.org/dc/terms/conformsTo": {
        "@id": "https://bioschemas.org/profiles/Person/0.3-DRAFT",
        "@type": "CreativeWork"
      },
      "url": "https://training.galaxyproject.org/training-material/hall-of-fame/kxk302/",
      "mainEntityOfPage": "https://training.galaxyproject.org/training-material/hall-of-fame/kxk302/",
      "name": "Kaivan Kamali",
      "image": "https://avatars.githubusercontent.com/kxk302",
      "description": "A contributor to the GTN project.",
      "memberOf": [
        {
          "@type": "Organization",
          "http://purl.org/dc/terms/conformsTo": {
            "@id": "https://bioschemas.org/profiles/Organization/0.2-DRAFT-2019_07_19",
            "@type": "Organization"
          },
          "id": "https://training.galaxyproject.org",
          "email": "galaxytrainingnetwork@gmail.com",
          "name": "Galaxy Training Network",
          "legalName": "Galaxy Training Network",
          "alternateName": "GTN",
          "url": "https://training.galaxyproject.org",
          "logo": "https://training.galaxyproject.org/training-material/assets/images/GTNLogo1000.png",
          "fundingModel": "The GTN's infrastructure relies on GitHub and the Galaxy Project for hosting costs. There are no full time paid staff members of the GTN. Individuals are occasionally funded on GTN-adjacent projects.",
          "keywords": [
            "galaxy",
            "bioinformatics",
            "training",
            "fair",
            "accessible"
          ],
          "status": "active",
          "foundingDate": "2015-06-29",
          "socialMedia": "https://mstdn.science/@gtn",
          "type": "project"
        }
      ]
    }
  ],
  "contributor": [
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "http://purl.org/dc/terms/conformsTo": {
        "@id": "https://bioschemas.org/profiles/Person/0.3-DRAFT",
        "@type": "CreativeWork"
      },
      "url": "https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/",
      "mainEntityOfPage": "https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/",
      "name": "Saskia Hiltemann",
      "image": "https://avatars.githubusercontent.com/shiltemann",
      "description": "Researcher at Erasmus Medical Center",
      "memberOf": [
        {
          "@type": "Organization",
          "http://purl.org/dc/terms/conformsTo": {
            "@id": "https://bioschemas.org/profiles/Organization/0.2-DRAFT-2019_07_19",
            "@type": "Organization"
          },
          "id": "https://training.galaxyproject.org",
          "email": "galaxytrainingnetwork@gmail.com",
          "name": "Galaxy Training Network",
          "legalName": "Galaxy Training Network",
          "alternateName": "GTN",
          "url": "https://training.galaxyproject.org",
          "logo": "https://training.galaxyproject.org/training-material/assets/images/GTNLogo1000.png",
          "fundingModel": "The GTN's infrastructure relies on GitHub and the Galaxy Project for hosting costs. There are no full time paid staff members of the GTN. Individuals are occasionally funded on GTN-adjacent projects.",
          "keywords": [
            "galaxy",
            "bioinformatics",
            "training",
            "fair",
            "accessible"
          ],
          "status": "active",
          "foundingDate": "2015-06-29",
          "socialMedia": "https://mstdn.science/@gtn",
          "type": "project"
        },
        {
          "@context": "https://schema.org",
          "@type": "Organization",
          "http://purl.org/dc/terms/conformsTo": {
            "@id": "https://bioschemas.org/profiles/Organization/0.3-DRAFT",
            "@type": "CreativeWork"
          },
          "id": "https://training.galaxyproject.org/training-material/hall-of-fame/uni-freiburg/",
          "name": "University of Freiburg",
          "description": "An organization supporting the Galaxy Training Network",
          "url": "https://www.uni-freiburg.de/"
        },
        {
          "@context": "https://schema.org",
          "@type": "Organization",
          "http://purl.org/dc/terms/conformsTo": {
            "@id": "https://bioschemas.org/profiles/Organization/0.3-DRAFT",
            "@type": "CreativeWork"
          },
          "id": "https://training.galaxyproject.org/training-material/hall-of-fame/nfdi4plants/",
          "name": "DataPLANT (NFDI4Plants)",
          "description": "An organization supporting the Galaxy Training Network",
          "url": "https://nfdi4plants.org"
        },
        {
          "@context": "https://schema.org",
          "@type": "Organization",
          "http://purl.org/dc/terms/conformsTo": {
            "@id": "https://bioschemas.org/profiles/Organization/0.3-DRAFT",
            "@type": "CreativeWork"
          },
          "id": "https://training.galaxyproject.org/training-material/hall-of-fame/elixir-europe/",
          "name": "ELIXIR Europe",
          "description": "An organization supporting the Galaxy Training Network",
          "url": "https://elixir-europe.org"
        }
      ],
      "identifier": "https://orcid.org/0000-0003-3803-468X",
      "orcid": "https://orcid.org/0000-0003-3803-468X"
    },
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "http://purl.org/dc/terms/conformsTo": {
        "@id": "https://bioschemas.org/profiles/Person/0.3-DRAFT",
        "@type": "CreativeWork"
      },
      "url": "https://training.galaxyproject.org/training-material/hall-of-fame/anuprulez/",
      "mainEntityOfPage": "https://training.galaxyproject.org/training-material/hall-of-fame/anuprulez/",
      "name": "Anup Kumar",
      "image": "https://avatars.githubusercontent.com/anuprulez",
      "description": "A contributor to the GTN project.",
      "memberOf": [
        {
          "@type": "Organization",
          "http://purl.org/dc/terms/conformsTo": {
            "@id": "https://bioschemas.org/profiles/Organization/0.2-DRAFT-2019_07_19",
            "@type": "Organization"
          },
          "id": "https://training.galaxyproject.org",
          "email": "galaxytrainingnetwork@gmail.com",
          "name": "Galaxy Training Network",
          "legalName": "Galaxy Training Network",
          "alternateName": "GTN",
          "url": "https://training.galaxyproject.org",
          "logo": "https://training.galaxyproject.org/training-material/assets/images/GTNLogo1000.png",
          "fundingModel": "The GTN's infrastructure relies on GitHub and the Galaxy Project for hosting costs. There are no full time paid staff members of the GTN. Individuals are occasionally funded on GTN-adjacent projects.",
          "keywords": [
            "galaxy",
            "bioinformatics",
            "training",
            "fair",
            "accessible"
          ],
          "status": "active",
          "foundingDate": "2015-06-29",
          "socialMedia": "https://mstdn.science/@gtn",
          "type": "project"
        },
        {
          "@context": "https://schema.org",
          "@type": "Organization",
          "http://purl.org/dc/terms/conformsTo": {
            "@id": "https://bioschemas.org/profiles/Organization/0.3-DRAFT",
            "@type": "CreativeWork"
          },
          "id": "https://training.galaxyproject.org/training-material/hall-of-fame/uni-freiburg/",
          "name": "University of Freiburg",
          "description": "An organization supporting the Galaxy Training Network",
          "url": "https://www.uni-freiburg.de/"
        },
        {
          "@context": "https://schema.org",
          "@type": "Organization",
          "http://purl.org/dc/terms/conformsTo": {
            "@id": "https://bioschemas.org/profiles/Organization/0.3-DRAFT",
            "@type": "CreativeWork"
          },
          "id": "https://training.galaxyproject.org/training-material/hall-of-fame/eurosciencegateway/",
          "name": "EuroScienceGateway",
          "description": "An organization supporting the Galaxy Training Network",
          "url": "https://galaxyproject.org/projects/esg/"
        },
        {
          "@context": "https://schema.org",
          "@type": "Organization",
          "http://purl.org/dc/terms/conformsTo": {
            "@id": "https://bioschemas.org/profiles/Organization/0.3-DRAFT",
            "@type": "CreativeWork"
          },
          "id": "https://training.galaxyproject.org/training-material/hall-of-fame/elixir-europe/",
          "name": "ELIXIR Europe",
          "description": "An organization supporting the Galaxy Training Network",
          "url": "https://elixir-europe.org"
        }
      ]
    },
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "http://purl.org/dc/terms/conformsTo": {
        "@id": "https://bioschemas.org/profiles/Person/0.3-DRAFT",
        "@type": "CreativeWork"
      },
      "url": "https://training.galaxyproject.org/training-material/hall-of-fame/kxk302/",
      "mainEntityOfPage": "https://training.galaxyproject.org/training-material/hall-of-fame/kxk302/",
      "name": "Kaivan Kamali",
      "image": "https://avatars.githubusercontent.com/kxk302",
      "description": "A contributor to the GTN project.",
      "memberOf": [
        {
          "@type": "Organization",
          "http://purl.org/dc/terms/conformsTo": {
            "@id": "https://bioschemas.org/profiles/Organization/0.2-DRAFT-2019_07_19",
            "@type": "Organization"
          },
          "id": "https://training.galaxyproject.org",
          "email": "galaxytrainingnetwork@gmail.com",
          "name": "Galaxy Training Network",
          "legalName": "Galaxy Training Network",
          "alternateName": "GTN",
          "url": "https://training.galaxyproject.org",
          "logo": "https://training.galaxyproject.org/training-material/assets/images/GTNLogo1000.png",
          "fundingModel": "The GTN's infrastructure relies on GitHub and the Galaxy Project for hosting costs. There are no full time paid staff members of the GTN. Individuals are occasionally funded on GTN-adjacent projects.",
          "keywords": [
            "galaxy",
            "bioinformatics",
            "training",
            "fair",
            "accessible"
          ],
          "status": "active",
          "foundingDate": "2015-06-29",
          "socialMedia": "https://mstdn.science/@gtn",
          "type": "project"
        }
      ]
    },
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "http://purl.org/dc/terms/conformsTo": {
        "@id": "https://bioschemas.org/profiles/Person/0.3-DRAFT",
        "@type": "CreativeWork"
      },
      "url": "https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/",
      "mainEntityOfPage": "https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/",
      "name": "Helena Rasche",
      "image": "https://avatars.githubusercontent.com/hexylena",
      "description": "A contributor to the GTN project.",
      "memberOf": [
        {
          "@type": "Organization",
          "http://purl.org/dc/terms/conformsTo": {
            "@id": "https://bioschemas.org/profiles/Organization/0.2-DRAFT-2019_07_19",
            "@type": "Organization"
          },
          "id": "https://training.galaxyproject.org",
          "email": "galaxytrainingnetwork@gmail.com",
          "name": "Galaxy Training Network",
          "legalName": "Galaxy Training Network",
          "alternateName": "GTN",
          "url": "https://training.galaxyproject.org",
          "logo": "https://training.galaxyproject.org/training-material/assets/images/GTNLogo1000.png",
          "fundingModel": "The GTN's infrastructure relies on GitHub and the Galaxy Project for hosting costs. There are no full time paid staff members of the GTN. Individuals are occasionally funded on GTN-adjacent projects.",
          "keywords": [
            "galaxy",
            "bioinformatics",
            "training",
            "fair",
            "accessible"
          ],
          "status": "active",
          "foundingDate": "2015-06-29",
          "socialMedia": "https://mstdn.science/@gtn",
          "type": "project"
        },
        {
          "@context": "https://schema.org",
          "@type": "Organization",
          "http://purl.org/dc/terms/conformsTo": {
            "@id": "https://bioschemas.org/profiles/Organization/0.3-DRAFT",
            "@type": "CreativeWork"
          },
          "id": "https://training.galaxyproject.org/training-material/hall-of-fame/erasmusmc/",
          "name": "Erasmus Medical Center",
          "description": "An organization supporting the Galaxy Training Network",
          "url": "https://www.erasmusmc.nl"
        },
        {
          "@context": "https://schema.org",
          "@type": "Organization",
          "http://purl.org/dc/terms/conformsTo": {
            "@id": "https://bioschemas.org/profiles/Organization/0.3-DRAFT",
            "@type": "CreativeWork"
          },
          "id": "https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/",
          "name": "Gallantries: Bridging Training Communities in Life Science, Environment and Health",
          "description": "An organization supporting the Galaxy Training Network",
          "url": "https://www.erasmusplus.nl"
        },
        {
          "@context": "https://schema.org",
          "@type": "Organization",
          "http://purl.org/dc/terms/conformsTo": {
            "@id": "https://bioschemas.org/profiles/Organization/0.3-DRAFT",
            "@type": "CreativeWork"
          },
          "id": "https://training.galaxyproject.org/training-material/hall-of-fame/by-covid/",
          "name": "BeYond-COVID",
          "description": "An organization supporting the Galaxy Training Network",
          "url": "https://by-covid.org/"
        },
        {
          "@context": "https://schema.org",
          "@type": "Organization",
          "http://purl.org/dc/terms/conformsTo": {
            "@id": "https://bioschemas.org/profiles/Organization/0.3-DRAFT",
            "@type": "CreativeWork"
          },
          "id": "https://training.galaxyproject.org/training-material/hall-of-fame/elixir-europe/",
          "name": "ELIXIR Europe",
          "description": "An organization supporting the Galaxy Training Network",
          "url": "https://elixir-europe.org"
        },
        {
          "@context": "https://schema.org",
          "@type": "Organization",
          "http://purl.org/dc/terms/conformsTo": {
            "@id": "https://bioschemas.org/profiles/Organization/0.3-DRAFT",
            "@type": "CreativeWork"
          },
          "id": "https://training.galaxyproject.org/training-material/hall-of-fame/elixir-converge/",
          "name": "ELIXIR-CONVERGE",
          "description": "An organization supporting the Galaxy Training Network",
          "url": "https://elixir-europe.org/about-us/how-funded/eu-projects/converge"
        }
      ],
      "identifier": "https://orcid.org/0000-0001-9760-8992",
      "orcid": "https://orcid.org/0000-0001-9760-8992"
    }
  ],
  "about": [
    {
      "@type": "CreativeWork",
      "name": "Statistics and machine learning",
      "description": "Statistical Analyses for omics data and machine learning using Galaxy tools",
      "url": "https://training.galaxyproject.org/training-material/topics/statistics/"
    },
    {
      "@type": "DefinedTerm",
      "@id": "http://edamontology.org/topic_2269",
      "inDefinedTermSet": "http://edamontology.org",
      "termCode": "topic_2269",
      "url": "https://bioportal.bioontology.org/ontologies/EDAM/?p=classes&conceptid=http%3A%2F%2Fedamontology.org%2Ftopic_2269"
    }
  ],
  "educationalLevel": "Beginner",
  "mentions": [
    {
      "@type": "Thing",
      "url": "https://training.galaxyproject.org/training-material/topics/statistics/tutorials/fruit_360/workflows/",
      "name": "Associated Workflows"
    },
    {
      "@type": "Thing",
      "url": "https://zenodo.org/record/5702887",
      "name": "Associated Training Datasets"
    }
  ]
}</script></head>
    <body data-spy="scroll" data-target="#toc" data-brightness="auto" data-contrast="auto">
        <script  src='/training-material/assets/js/bundle.theme.f1f2de89.js'></script>
        <header>
    <nav class="navbar navbar-expand-md navbar-dark" aria-label="Site Navigation">
        <div class="container">
            <a class="navbar-brand" href="/training-material/">
                <img src="/training-material/assets/images/GTN-60px.png" height="30" alt="Galaxy Training Network logo">
                
                    Galaxy Training!
                
            </a>

            <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#top-navbar" aria-controls="top-navbar" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="top-navbar">
                <ul class="navbar-nav">
                    <li class="nav-item">
                        
                        <a class="nav-link" href="/training-material/topics/statistics" title="Go back to list of tutorials">
                            <i class="far fa-folder" aria-hidden="true"></i> Statistics and machine learning
                        </a>
                        
                    </li>

                    <li class="nav-item">
                        
                        <a class="nav-link" href="/training-material/learning-pathways" title="Learning Pathways">
                           <i class="fas fa-graduation-cap" aria-hidden="true"></i><span class="visually-hidden">curriculum</span> Learning Pathways
                        </a>
                        
                    </li>

                    <li class="nav-item dropdown">
    <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-expanded="false" title="Help">
        <i class="far fa-question-circle" aria-hidden="true"></i><span class="visually-hidden">help</span> Help
    </a>
    <div class="dropdown-menu dropdown-menu-right">
	<a class="dropdown-item" href="/training-material/faqs/index.html" title="Check our FAQs">
           <i class="far fa-question-circle" aria-hidden="true"></i><span class="visually-hidden">question</span> FAQs
        </a>
        
        
        
        <a class="dropdown-item" href="/training-material/topics/statistics/faqs/" title="Check our FAQs for the Statistics and machine learning topic">
           <i class="far fa-question-circle" aria-hidden="true"></i><span class="visually-hidden">question</span> Topic FAQs
        </a>
        
        
        
        <a class="dropdown-item" href="https://help.galaxyproject.org/" title="Discuss on Galaxy Help">
            <i class="far fa-comments" aria-hidden="true"></i><span class="visually-hidden">feedback</span> Galaxy Help Forum
        </a>
        <a class="dropdown-item" href="https://gitter.im/Galaxy-Training-Network/Lobby" title="Discuss on gitter">
           <i class="fab fa-gitter" aria-hidden="true"></i><span class="visually-hidden">gitter</span> Discuss on Matrix
        </a>
    </div>
</li>


                    <li class="nav-item dropdown">
    <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-expanded="false" title="Settings">
	<i class="fas fa-cog" aria-hidden="true"></i><span class="visually-hidden">galaxy-gear</span> Settings
    </a>
    <div class="dropdown-menu dropdown-menu-right">

	<h6 class="dropdown-header">Preferences</h6>

	<a href="/training-material/user/theme.html" class="dropdown-item">
		<i class="fas fa-palette" aria-hidden="true"></i><span class="visually-hidden">gtn-theme</span> Theme
	</a>

	<a href="/training-material/user/privacy.html" class="dropdown-item">
		<i class="fas fa-lock" aria-hidden="true"></i><span class="visually-hidden">pref-dataprivate</span> Data Privacy
	</a>

	<div class="dropdown-divider"></div>

	<h6 class="dropdown-header">For Everyone</h6>

        <a class="dropdown-item" href="https://github.com/galaxyproject/training-material/edit/main/topics/statistics/tutorials/fruit_360/tutorial.md">
          <i class="fab fa-github" aria-hidden="true"></i><span class="visually-hidden">github</span> Propose a change or correction
        </a>

	<h6 class="dropdown-header">Instructor Utilities</h6>

        <a class="dropdown-item" href="/training-material/stats.html">
            <i class="fas fa-chart-column" aria-hidden="true"></i><span class="visually-hidden">galaxy-barchart</span> GTN statistics
        </a>

        <a class="dropdown-item" href="https://plausible.galaxyproject.eu/training.galaxyproject.org?period=12mo&page=/training-material/topics/statistics/tutorials/fruit_360/tutorial.html">
            <i class="fas fa-chart-column" aria-hidden="true"></i><span class="visually-hidden">galaxy-barchart</span> Page View Metrics
        </a>

        <!-- link to feedback -->
        
            
            
                <a class="dropdown-item" href="/training-material/feedback.html">
                    <i class="fas fa-chart-column" aria-hidden="true"></i><span class="visually-hidden">galaxy-barchart</span> GTN feedback
                </a>
            
        

        <div class="dropdown-item">
            <div>
                <i class="fas fa-history" aria-hidden="true"></i><span class="visually-hidden">galaxy-rulebuilder-history</span> Previous Versions
            </div>

            <div id="archive-selector">
            
                <a class="btn btn-warning" href="https://training.galaxyproject.org/archive/">Older Versions</a>
            </div>

        </div>

    </div>
</li>


                    <!-- Search bar-->
                    <li class="nav-item">
                      <div id="navbarSupportedContent" role="search">
                        <!-- Search form -->
                        <form class="form-inline mr-auto" method="GET" action="/training-material/search2">
                          <i class="fas fa-search nav-link" aria-hidden="true"></i>
                          <div class="md-form mb-2">
                            <input name="query" class="form-control nicer" type="text" placeholder="Search Tutorials" aria-label="Search">
                          </div>
                        </form>
                      </div>
                    </li>
                </ul>
            </div>
        </div>
    </nav>
</header>

        
        <div class="container main-content" role="main">
        














<!-- Gitter -->






<article class="tutorial topic-statistics">
    <h1 data-toc-skip>Image classification in Galaxy with fruit 360 dataset</h1>
    

    <section aria-labelledby="overview-box" id="tutorial-metadata">
    <div markdown="0">

	<div class="contributors-line">
		
<table class="contributions">
	
	<tr>
		<td><abbr title="These people wrote the bulk of the tutorial, they may have done the analysis, built the workflow, and wrote the text themselves.">Author(s)</abbr></td>
		<td>
			
  <a href="/training-material/hall-of-fame/kxk302/" class="contributor-badge contributor-kxk302"><img src="https://avatars.githubusercontent.com/kxk302?s=36" alt="Kaivan Kamali avatar" width="36" class="avatar" />
    Kaivan Kamali</a>
  
		</td>
	</tr>
	

	

	

	

	

	

	

	
	<tr class="reviewers">
		<td><abbr title="These people reviewed this material for accuracy and correctness">Reviewers</abbr></td>
		<td>
			
  <a href="/training-material/hall-of-fame/shiltemann/" class="contributor-badge contributor-shiltemann">
    <img src="https://avatars.githubusercontent.com/shiltemann?s=36" alt="Saskia Hiltemann avatar" width="36" class="avatar" />
  </a>
  
  <a href="/training-material/hall-of-fame/anuprulez/" class="contributor-badge contributor-anuprulez">
    <img src="https://avatars.githubusercontent.com/anuprulez?s=36" alt="Anup Kumar avatar" width="36" class="avatar" />
  </a>
  
  <a href="/training-material/hall-of-fame/kxk302/" class="contributor-badge contributor-kxk302">
    <img src="https://avatars.githubusercontent.com/kxk302?s=36" alt="Kaivan Kamali avatar" width="36" class="avatar" />
  </a>
  
  <a href="/training-material/hall-of-fame/hexylena/" class="contributor-badge contributor-hexylena">
    <img src="https://avatars.githubusercontent.com/hexylena?s=36" alt="Helena Rasche avatar" width="36" class="avatar" />
  </a>
  </td>
	</tr>
	

	

	

</table>


	</div>

</div>


    <blockquote class="overview">
        <div id="overview-box" class="box-title">Overview</div>
        
        <img alt="Creative Commons License: CC-BY" class="float-right" style="border-width:0; display: inline-block; margin:0" src="/training-material/assets/images/cc-by.png" width="88" height="31"/>
        
        <strong><i class="far fa-question-circle" aria-hidden="true"></i> Questions:</strong>
        <ul>
        
        <li><p>How to solve an image classification problem using convolutional neural network (CNN)?</p>
</li>
        
        </ul>

        <strong><i class="fas fa-bullseye" aria-hidden="true"></i> Objectives: </strong>
        <ul>
        
        <li><p>Learn how to create a CNN using Galaxy’s deep learning tools</p>
</li>
        
        <li><p>Solve an image classification problem on fruit 360 dataset using CNN in Galaxy</p>
</li>
        
        </ul>

        
        <strong><i class="fas fa-check-circle" aria-hidden="true"></i> Requirements:</strong>
        <ul>
        
    
        
        
        
        <li>
          <a href="/training-material/topics/introduction">Introduction to Galaxy Analyses</a>
        </li>
        
    


        
    
        
        
        
            
                
                    
                
                    
                
                    
                        
                            <li>
                              <a href="/training-material/topics/statistics/tutorials/CNN/slides.html"><i class="fab fa-slideshare" aria-hidden="true"></i><span class="visually-hidden">slides</span> Slides: Deep Learning (Part 3) - Convolutional neural networks (CNN)</a>
                            </li>
                        
                        
                            
                                <li>
                                  <a href="/training-material/topics/statistics/tutorials/CNN/tutorial.html"><i class="fas fa-laptop" aria-hidden="true"></i><span class="visually-hidden">tutorial</span> Hands-on: Deep Learning (Part 3) - Convolutional neural networks (CNN)</a>
                                </li>
                            
                        
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            
        
    


        </ul>
        

        
        <div><strong><i class="fas fa-hourglass-half" aria-hidden="true"></i> Time estimation:</strong> 2 hours</div>
        

        

        

        

        <div id="supporting-materials"><strong><i class="fa fa-external-link" aria-hidden="true"></i> Supporting Materials:</strong></div>
        <ul class="supporting_material">
            
                <li class="btn btn-default"><a href="/training-material/topics/statistics/tutorials/fruit_360/slides.html" title="Slides for this tutorial">
                    <i class="fab fa-slideshare" aria-hidden="true"></i> Slides
                </a></li>
            

            
                <li class="btn btn-default supporting_material">


<a class="btn btn-default topic-icon" title="Zenodo datasets used in this tutorial" href="https://zenodo.org/record/5702887">
    <i class="far fa-copy" aria-hidden="true"></i>&nbsp;Datasets
</a>

</li>
            

            
                <li class="btn btn-default supporting_material">


    <a class="btn btn-default topic-icon" href="/training-material/topics/statistics/tutorials/fruit_360/workflows/" title="Image classification in Galaxy with fruit 360 dataset workflows">
        <i class="fas fa-share-alt" aria-hidden="true"></i>&nbsp;Workflows
    </a>

</li>
            

            

            

            

            

            
            
            
                <li class="btn btn-default supporting_material">    <a class="topic-icon" href="/training-material/topics/statistics/tutorials/fruit_360/faqs/" title="Frequently Asked Questions">
        <i class="far fa-question-circle" aria-hidden="true"></i> FAQs
    </a>
</li>
            

            <!-- Check the GTN Video Library for recordings of this tutorial or associated slides -->
            











<li class="btn btn-default supporting_material">


  <!-- dropdown with all recordings -->
  <a href="/training-material/topics/statistics/tutorials/fruit_360/recordings/" class="btn btn-default dropdown-toggle topic-icon" data-toggle="dropdown" aria-expanded="false" title="Latest recordings of this material in the GTN Video Library">
        <i class="fas fa-video" aria-hidden="true"></i><span class="visually-hidden">video</span>&nbsp;Recordings
  </a>


  <ul class="dropdown-menu">
    

    
      
      
      
    <li><a class="dropdown-item" href="/training-material/topics/statistics/tutorials/fruit_360/recordings/index.html#tutorial-recording-19-january-2022" title="View the recording for this tutorial">
                <i class="fas fa-video" aria-hidden="true"></i><span class="visually-hidden">video</span> Tutorial (January 2022) - 1h</a>
    </li>
      
    
    <li><a class="dropdown-item" href="/training-material/topics/statistics/tutorials/fruit_360/recordings/" title="View all recordings for this tutorial">
                <i class="fas fa-video" aria-hidden="true"></i><span class="visually-hidden">video</span> View All</a>
    </li>

  </ul>

  
</li>
  





            
                
                <li class="btn btn-default supporting_material">






    <a href="#" class="dropdown-toggle" data-toggle="dropdown" aria-expanded="false" title="Where to run the tutorial">
        <i class="fas fa-globe" aria-hidden="true"></i><span class="visually-hidden">instances</span>&nbsp;Available on these Galaxies 
    </a>
    <ul class="dropdown-menu">
        
	<li class="dropdown-header">
		<b>Known Working</b>
	</li>
	
	

    
	<li>
		<a class="dropdown-item" href="https://usegalaxy.eu" title="">
			UseGalaxy.eu <abbr title="This instance supports the precise tool versions used in this tutorial">✅</abbr> <abbr title="This is a UseGalaxy.* server which meets minimum requirements for a public Galaxy">⭐️</abbr>
		</a>
	</li>
    
	<li>
		<a class="dropdown-item" href="https://usegalaxy.org" title="">
			UseGalaxy.org (Main) <abbr title="This instance supports the precise tool versions used in this tutorial">✅</abbr> <abbr title="This is a UseGalaxy.* server which meets minimum requirements for a public Galaxy">⭐️</abbr>
		</a>
	</li>
    
	<li>
		<a class="dropdown-item" href="https://usegalaxy.org.au" title="">
			UseGalaxy.org.au <abbr title="This instance supports the precise tool versions used in this tutorial">✅</abbr> <abbr title="This is a UseGalaxy.* server which meets minimum requirements for a public Galaxy">⭐️</abbr>
		</a>
	</li>
    
    
	<li>
		<a class="dropdown-item" href="https://usegalaxy.cz/" title="">
			UseGalaxy.cz <abbr title="This instance supports the precise tool versions used in this tutorial">✅</abbr>
		</a>
	</li>
    
    
    

    </ul>

</li>
                
            
        </ul>

        <div><strong><i class="far fa-calendar" aria-hidden="true"></i> Published:</strong> Dec 1, 2021 </div>
        <div><strong><i class="far fa-calendar" aria-hidden="true"></i> Last modification:</strong> Jul 9, 2024 </div>
        <div><strong><i class="fas fa-balance-scale" aria-hidden="true"></i> License:</strong>
		
            Tutorial Content is licensed under
            
              <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
            
            The GTN Framework is licensed under <a rel="license" href="https://github.com/galaxyproject/training-material/blob/main/LICENSE.md">MIT</a>
        </div>
        
        <div><strong><i class="fas fa-fingerprint" aria-hidden="true"></i><span class="visually-hidden">purl</span> <abbr title="Persistent URL">PURL</abbr>:</strong> <a href="https://gxy.io/GTN:T00265">https://gxy.io/GTN:T00265</a> </div>
        

	
	

	
	
	<div><strong><i class="far fa-star" aria-hidden="true"></i><span class="visually-hidden">rating</span> Rating:</strong> <a href="#feedback-responses">5.0</a> (0 recent ratings, 2 all time)</div>
	
	<div><strong><i class="fas fa-code-commit" aria-hidden="true"></i><span class="visually-hidden">version</span> Revision:</strong> 8 </div>

    </blockquote>
    </section>

    <div class="container">
        <div class="row">
            <!-- sidebar, which will move to the top on a small screen -->
            <div class="col-sm-2 hide-when-printing">
                <nav id="toc" data-toggle="toc" class="sticky-top" aria-label="Table of Contents"></nav>
            </div>
            <div class="col-sm-10">
                 

                <section aria-label="Tutorial Content" id="tutorial-content">
                <p>The classification of fruits and vegetables offers many useful applications such as
automated harvesting by robots, building up stocks for supermarkets, effective detection
of specific defects, and determining fruit ripeness (<span class="citation"><a href="#Duong2020">Duong <i>et al.</i> 2020</a></span>,
<span class="citation"><a href="#NaranjoTorres2020">Naranjo-Torres <i>et al.</i> 2020</a></span>, <span class="citation"><a href="#Iswari2017">Iswari <i>et al.</i> 2017</a></span>). Machine Learning (ML) techniques
such as Deep Learning (DL) are commonly used for image classification problems in various
domains, including in agriculture (<span class="citation"><a href="#Kamilaris2018">Kamilaris and Prenafeta-Boldú 2018</a></span>). DL is a technique inspired
by how a human brain operates. Due to the increased availability of compute capacity and
training data, DL techniques have become very popular in recent years. In this tutorial,
we will use Galaxy’s ML toolkit to build a DL model to classify fruit and vegetable
images. Our DL model is trained and evaluated on Fruit 360 dataset (<span class="citation"><a href="#Murean2018">Mureşan and Oltean 2018</a></span>)</p>

<blockquote class="agenda">
  <div class="box-title agenda-title" id="agenda">Agenda</div>

  <p>In this tutorial, we will cover:</p>

<ol id="markdown-toc">
  <li><a href="#overview-of-convolutional-neural-networks-cnn" id="markdown-toc-overview-of-convolutional-neural-networks-cnn">Overview of convolutional neural networks (CNN)</a></li>
  <li><a href="#architecture-of-cnn" id="markdown-toc-architecture-of-cnn">Architecture of CNN</a></li>
  <li><a href="#fruit-360-dataset" id="markdown-toc-fruit-360-dataset">Fruit 360 dataset</a></li>
  <li><a href="#get-data" id="markdown-toc-get-data">Get data</a></li>
  <li><a href="#classification-of-fruit-360-dataset-images-with-cnn" id="markdown-toc-classification-of-fruit-360-dataset-images-with-cnn">Classification of fruit 360 dataset images with CNN</a></li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
</ol>

</blockquote>
<h2 id="overview-of-convolutional-neural-networks-cnn">Overview of convolutional neural networks (CNN)</h2>

<p>Here we provide a brief overview of CNN. For a more in depth discussion, please refer to the CNN tutorial cited in the requirements
section. CNN were inspired by how the visual cortex of the brain processes visual information (<span class="citation"><a href="#HubelWiesel">Hubel and Wiesel 1959</a></span>). There are
two types of cells in our visual cortex: <strong>simple cells</strong> detect objects at certain angles/locations, and <strong>complex</strong> cells,
which receive inputs from multiple simple cells, and detect movement. In 1980, inspired by hierarchical structure of complex and
simple cells, Fukushima proposed <em>Neocognitron</em> (<span class="citation"><a href="#Fukishima">Fukushima 1988</a></span>), a hierarchical neural network used for handwritten Japanese
character recognition. In 1989, LeCun et. al. (<span class="citation"><a href="#LeCunEtAl">LeCun <i>et al.</i> 1989</a></span>) proposed a CNN that could be trained by backpropagation
algorithm. CNN gained immense popularity when they outperformed other models at ImageNet Challenge, a competition in object
classification and detection on hundreds of object categories and millions of images.</p>

<h2 id="architecture-of-cnn">Architecture of CNN</h2>

<p>A typical CNN has the following 4 layers (<span class="citation"><a href="#OSheaEtAl">O’Shea and Nash 2015</a></span>)</p>

<ol>
  <li>Input layer</li>
  <li>Convolution layer</li>
  <li>Pooling layer</li>
  <li>Fully connected layer</li>
</ol>

<p>Please note that we will explain a 2 dimensional (2D) CNN here. But the same concepts apply to a 1 (or 3) dimensional CNN as well.</p>

<h3 id="input-layer">Input layer</h3>

<p>The input layer represents the input to the CNN. An example input, could be a 28 by 28 pixel grayscale image. We do not
“flatten” the input to a 1D vector. This makes capturing spatial relationships easier.</p>

<h3 id="convolution-layer">Convolution layer</h3>

<p>The convolution layer is composed of multiple <strong>filters</strong> (also called <strong>kernels</strong>). Filters for a 2D image are also 2D. Suppose
we have a 28 by 28 pixel grayscale image. Each pixel is represented by a number between 0 and 255, where 0 represents the color
black, 255 represents the color white, and the values in between represent different shades of gray. Suppose we have a 3 by 3
filter (9 values in total), and the values are randomly set to 0 or 1. Convolution is the process of placing the 3 by 3 filter
on the top left corner of the image, multiplying filter values by the pixel values and adding the results, moving the filter to
the right one pixel at a time and repeating this process (Figure 1). When we get to the top right corner of the image, we simply
move the filter down one pixel and restart from the left. This process ends when we get to the bottom right corner of the image.</p>

<figure id="figure-1" style="max-width: 90%;"><img src="../../images/Conv_no_padding_no_strides.gif" alt="A 3 by 3 filter applied to a 4 by 4 image, resulting in a 2 by 2 image. " width="244" height="259" loading="lazy" /><a target="_blank" href="../../images/Conv_no_padding_no_strides.gif" rel="noopener noreferrer"><small>Open image in new tab</small></a><br /><br /><figcaption><span class="figcaption-prefix"><strong>Figure 1</strong>:</span> A 3 by 3 filter applied to a 4 by 4 image, resulting in a 2 by 2 image (<span class="citation"><a href="#DumoulinVisin">Dumoulin and Visin 2016</a></span>)</figcaption></figure>

<p>Covolution operator has several parameters.</p>

<ol>
  <li>Filter size</li>
  <li>Padding</li>
  <li>Stride</li>
  <li>Dilation</li>
  <li>Activation function</li>
</ol>

<p>Filter size can be 5 by 5, 3 by 3, and so on. Larger filter sizes should be avoided as more weights need to be learned (more
compute capacity, more training time, more chance of overfitting). Also, odd sized filters are preferred to even sized filters,
due to the nice geometric property of all the input pixels being around the output pixel.</p>

<p>If you look at Figure 1 you see that after applying a 3 by 3 filter to a 4 by 4 image, we end up with a 2 by 2 image – the
size of the image has gone down. If we want to keep the image size the same, we can use <em>padding</em> (Figure 2). We pad the input
in every direction with 0’s before applying the filter. If the padding is 1 by 1, then we add 1 zero in evey direction. If its
2 by 2, then we add 2 zeros in every direction, and so on.</p>

<figure id="figure-2" style="max-width: 90%;"><img src="../../images/Conv_same_padding_no_strides.gif" alt="A 3 by 3 filter applied to a 5 by 5 image, with padding of 1, resulting in a 5 by 5 image. " width="395" height="449" loading="lazy" /><a target="_blank" href="../../images/Conv_same_padding_no_strides.gif" rel="noopener noreferrer"><small>Open image in new tab</small></a><br /><br /><figcaption><span class="figcaption-prefix"><strong>Figure 2</strong>:</span> A 3 by 3 filter applied to a 5 by 5 image, with padding of 1, resulting in a 5 by 5 image (<span class="citation"><a href="#DumoulinVisin">Dumoulin and Visin 2016</a></span>)</figcaption></figure>

<p>As mentioned before, we start the convolution by placing the filter on the top left corner of the image, and after multiplying
filter and image values (and adding them), we move the filter to the right and repeat the process. How many pixels we move to
the right (or down) is the <em>stride</em>. In figure 1 and 2, the stride of the filter is 1. We move the filter one pixel to the right
(or down). But we could use a different stride. Figure 3 shows an example of using stride of 2.</p>

<figure id="figure-3" style="max-width: 90%;"><img src="../../images/Conv_no_padding_strides.gif" alt="A 3 by 3 filter applied to a 5 by 5 image, with stride of 2, resulting in a 2 by 2 image. " width="294" height="288" loading="lazy" /><a target="_blank" href="../../images/Conv_no_padding_strides.gif" rel="noopener noreferrer"><small>Open image in new tab</small></a><br /><br /><figcaption><span class="figcaption-prefix"><strong>Figure 3</strong>:</span> A 3 by 3 filter applied to a 5 by 5 image, with stride of 2, resulting in a 2 by 2 image (<span class="citation"><a href="#DumoulinVisin">Dumoulin and Visin 2016</a></span>)</figcaption></figure>

<p>When we apply a, say 3 by 3, filter to an image, our filter’s output is affected by pixels in a 3 by 3 subset of the image. If we
like to have a larger <em>receptive field</em> (portion of image that affect filter’s output), we could use <em>dilation</em>. If we set the
dilation to 2 (Figure 4), instead of a contiguous 3 by 3 subset of the image, every other pixel of a 5 by 5 subset of the image
affects the filter’s output.</p>

<figure id="figure-4" style="max-width: 90%;"><img src="../../images/Conv_dilation.gif" alt="A 3 by 3 filter applied to a 7 by 7 image, with dilation of 2, resulting in a 3 by 3 image. " width="395" height="381" loading="lazy" /><a target="_blank" href="../../images/Conv_dilation.gif" rel="noopener noreferrer"><small>Open image in new tab</small></a><br /><br /><figcaption><span class="figcaption-prefix"><strong>Figure 4</strong>:</span> A 3 by 3 filter applied to a 7 by 7 image, with dilation of 2, resulting in a 3 by 3 image (<span class="citation"><a href="#DumoulinVisin">Dumoulin and Visin 2016</a></span>)</figcaption></figure>

<p>After the filter scans the whole image, we apply an activation function to filter output to introduce non-linearlity. The preferred
activation function used in CNN is ReLU (<span class="citation"><a href="#NwankpaEtAl">Nwankpa <i>et al.</i> 2018</a></span>). ReLU leaves pixels with positive values in filter output as is,
and replaces negative values with 0. Figure 5 shows the results of applying ReLU activation function to a filter output.</p>

<figure id="figure-5" style="max-width: 90%;"><img src="../../images/Conv_ReLU.png" alt="Two matrices representing filter output before and after ReLU activation function is applied. " width="406" height="159" loading="lazy" /><a target="_blank" href="../../images/Conv_ReLU.png" rel="noopener noreferrer"><small>Open image in new tab</small></a><br /><br /><figcaption><span class="figcaption-prefix"><strong>Figure 5</strong>:</span> Applying ReLU activation function to filter output</figcaption></figure>

<figure id="figure-6" style="max-width: 90%;"><img src="../../images/Conv_single_input_channel.png" alt="One matrix representing an input and another matrix representing a filter, along with calculation for single input channel two dimensional convolution operation. " width="564" height="254" loading="lazy" /><a target="_blank" href="../../images/Conv_single_input_channel.png" rel="noopener noreferrer"><small>Open image in new tab</small></a><br /><br /><figcaption><span class="figcaption-prefix"><strong>Figure 6</strong>:</span> Illustration of single input channel two dimensional convolution</figcaption></figure>

<p>Figure 6 illustrates the calculations for a convolution operation, via a 3 by 3 filter on a single channel 5 by 5 input
(5 x 5 x 1). Figure 7 illustrates the calculations when the input has 3 channels. To show this in 2 dimensions, we are
displaying each channel in input and filter separately. Figure 9 shows a sample multi-channel 2D convolution in 3 dimensions.</p>

<figure id="figure-7" style="max-width: 90%;"><img src="../../images/Conv_multiple_input_channel.png" alt="Three matrices representing an input and another three matrices representing a filter, along with calculation for multiple input channel two dimensional convolution operation . " width="643" height="600" loading="lazy" /><a target="_blank" href="../../images/Conv_multiple_input_channel.png" rel="noopener noreferrer"><small>Open image in new tab</small></a><br /><br /><figcaption><span class="figcaption-prefix"><strong>Figure 7</strong>:</span> Illustration of multiple input channel two dimensional convolution</figcaption></figure>

<p>As Figures 7 and 8 show the output of a multi-channel 2D filter is a single channel 2D image. Applying <em>multiple</em> filters to the
input image results in a multi-channel 2D image for the output. For example, if the input image is 28 by 28 by 3
(rows x columns x channels), and we apply a 3 by 3 filter with 1 by 1 padding, we would get a 28 by 28 by 1 image. If we apply 15
filters to the input image, our output would be 28 by 28 by 15. Hence, the number of filters in a convolution layer allows us to
increase or decrease the channel size.</p>

<figure id="figure-8" style="max-width: 90%;"><img src="../../images/Conv_multiple_channel_3d.gif" alt="Multiple cubes representing input, filter, and output in a 3 channel 2 dimensional convolution operation. " width="948" height="548" loading="lazy" /><a target="_blank" href="../../images/Conv_multiple_channel_3d.gif" rel="noopener noreferrer"><small>Open image in new tab</small></a><br /><br /><figcaption><span class="figcaption-prefix"><strong>Figure 8</strong>:</span> Three dimensional illustration of multiple input channel two dimensional convolution (Source: https://thomelane.github.io/convolutions/2DConvRGB.html)</figcaption></figure>

<h3 id="pooling-layer">Pooling layer</h3>

<p>The pooling layer performs down sampling to reduce the spatial dimensionality of the input. This decreases the number of parameters,
which in turn reduces the learning time and computation, and the likelihood of overfitting. The most popular type of pooling is
<em>max pooling</em>. Its usually a 2 by 2 filter with a stride of 2 that returns the maximum value as it slides over the input data,
similar to convolution filters.</p>

<h3 id="fully-connected-layer">Fully connected layer</h3>

<p>The last layer in a CNN is a fully connected layer. We connect all the nodes from the previous layer to this fully connected layer,
which is responsible for classification of the image.</p>

<figure id="figure-9" style="max-width: 90%;"><img src="../../images/Conv_CNN.png" alt="A convolutional neural network with 3 convolution layers followed by 3 pooling layers. " width="842" height="265" loading="lazy" /><a target="_blank" href="../../images/Conv_CNN.png" rel="noopener noreferrer"><small>Open image in new tab</small></a><br /><br /><figcaption><span class="figcaption-prefix"><strong>Figure 9</strong>:</span> A convolutional neural network with 3 convolution layers followed by 3 pooling layers (<span class="citation"><a href="#OSheaEtAl">O’Shea and Nash 2015</a></span>)</figcaption></figure>

<p>As shown in Figure 9, a typical CNN usually has more than one convolution plus pooling layer. Each convolution plus pooling layer
is responsible for feature extraction at a different level of abstraction. For example, the filters in the first layer could detect
horizontal, vertical, and diagonal edges. The filters in the next layer could detect shapes, and the filters in the last layer could
detect collection of shapes. Filter values are randomly initialized and are learned by the learning algorithm. This makes CNN very
powerful as they not only do classification, but can also automatically do feature extraction. This distinguishes CNN from other classification techniques (like Support Vector Machines), which cannot do feature extraction.</p>

<h2 id="fruit-360-dataset">Fruit 360 dataset</h2>

<p>Fruit 360 is a dataset with 90380 images of 131 fruits and vegetables
(https://www.kaggle.com/moltean/fruits). Images are 100 pixel by 100 pixel and are color
(RGB) images (Hence, 3 values for each pixel). There are 67,692 images in the training
dataset and 22,688 images in the test dataset. The dataset we use for this tutorial is a
subset of fruit 360 dataset, containing only 10 fruits/vegetables (Strawberry,
Apple_Red_Delicious, Pepper_Green, Corn, Banana, Tomato_1, Potato_White, Pineapple,
Orange, and Peach). We selected a subset of fruits/vegetables, so the dataset size is
smaller and the neural network can be trained faster. Our training dataset has 5,015 images
and our testing dataset has 1,679 images.</p>

<p>The utilities used to create the subset dataset, along with step by step instructions, can
be found here: https://github.com/kxk302/fruit_dataset_utilities. First, we created feature
vectors for each image. Images are 100 pixel by 100 pixel and are color (RGB) images
(3 values for each pixel). Hence, each image can be represented by 30,000 values
(100 X 100 X 3). Second, we selected a subset of 10 fruit/vegetable images. Training and
testing dataset sizes go from 7 GB and 2.5 GB for 131 fruits/vegetables to 500 MB and
177 MB for 10 fruits/vegetables, respectively. Third, we created separate files for feature
vectors and labels. Finally, we mapped the labels for the 10 selected fruits/vegetables to
a range of 0 to 9. Full dataset labels are in the 0 to 130 range, as the full dataset
includes 131 fruits/vegetables. The 10 labels for out dataset are as follows: Strawberry:0,
Apple_Red_Delicious:1, Pepper_Green:2, Corn:3, Banana:4, Tomato_1:5, Potato_White:6, Pineapple:7,
Orange:8, Peach:9.</p>

<h2 id="get-data">Get data</h2>

<blockquote class="notranslate hands_on">
  <div class="box-title hands-on-title" id="hands-on-data-upload"><i class="fas fa-pencil-alt" aria-hidden="true" ></i> Hands-on: Data upload</div>

  <ol>
    <li>
      <p>Make sure you have an empty analysis history.</p>

      <!--SNIPPET-->
      <blockquote class="tip">   <div class="box-title tip-title" id="tip-creating-a-new-history"><button class="gtn-boxify-button tip" type="button" aria-controls="tip-creating-a-new-history" aria-expanded="true"><i class="far fa-lightbulb" aria-hidden="true"></i> <span>Tip: Creating a new history</span><span class="fold-unfold fa fa-minus-square"></span></button></div>   <p>To create a new history simply click the <i class="fas fa-plus" aria-hidden="true"></i><span class="visually-hidden">new-history</span> icon at the top of the history panel:</p>   <p><img src="/training-material/shared/images/history_create_new.svg" alt="UI for creating new history" /></p>   <!-- the original drawing can be found here https://docs.google.com/drawings/d/1cCBrLAo4kDGic5QyB70rRiWJAKTenTU8STsKDaLcVU8/edit?usp=sharing --> </blockquote>
      <p><!--END_SNIPPET--></p>
    </li>
    <li>
      <p><strong>Rename your history</strong> to make it easy to recognize</p>

      <blockquote class="tip">
        <div class="box-title tip-title" id="tip-rename-a-history"><button class="gtn-boxify-button tip" type="button" aria-controls="tip-rename-a-history" aria-expanded="true"><i class="far fa-lightbulb" aria-hidden="true" ></i> <span>Tip: Rename a history</span><span class="fold-unfold fa fa-minus-square"></span></button></div>

        <ul>
          <li>
            <p>Click on the title of the history (by default the title is <code class="language-plaintext highlighter-rouge">Unnamed history</code>)</p>

            <p><a href="/training-material/shared/images/rename_history.png" rel="noopener noreferrer"><img src="/training-material/shared/images/rename_history.png" alt="Renaming history. " width="308" height="340" loading="lazy" /></a></p>
          </li>
          <li>Type <code class="language-plaintext highlighter-rouge">Galaxy Introduction</code> as the name</li>
          <li>Press <kbd>Enter</kbd></li>
        </ul>

      </blockquote>
    </li>
    <li>
      <p>Import the files from <a href="https://zenodo.org/record/5702887">Zenodo</a></p>

      <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>https://zenodo.org/record/5702887/files/train_X_10.tsv
https://zenodo.org/record/5702887/files/train_y_10.tsv
https://zenodo.org/record/5702887/files/test_X_10.tsv
https://zenodo.org/record/5702887/files/test_y_10.tsv
</code></pre></div>      </div>

      <!--SNIPPET-->
      <blockquote class="tip">   <div class="box-title tip-title" id="tip-importing-via-links"><button class="gtn-boxify-button tip" type="button" aria-controls="tip-importing-via-links" aria-expanded="true"><i class="far fa-lightbulb" aria-hidden="true"></i> <span>Tip: Importing via links</span><span class="fold-unfold fa fa-minus-square"></span></button></div>   <ul>   <li>Copy the link location</li>   <li>     <p>Click <i class="fas fa-upload" aria-hidden="true"></i><span class="visually-hidden">galaxy-upload</span> <strong>Upload Data</strong> at the top of the tool panel</p>   </li>   <li>Select <i class="fa fa-edit" aria-hidden="true"></i><span class="visually-hidden">galaxy-wf-edit</span> <strong>Paste/Fetch Data</strong></li>   <li>     <p>Paste the link(s) into the text field</p>   </li>   <li>     <p>Press <strong>Start</strong></p>   </li>   <li><strong>Close</strong> the window</li> </ul> </blockquote>
      <p><!--END_SNIPPET--></p>
    </li>
    <li>
      <p>Rename the datasets as <code class="language-plaintext highlighter-rouge">train_X_10</code>, <code class="language-plaintext highlighter-rouge">train_y_10</code>, <code class="language-plaintext highlighter-rouge">test_X_10</code>, and <code class="language-plaintext highlighter-rouge">test_y_10</code> respectively.</p>

      <!--SNIPPET-->
      <blockquote class="tip">   <div class="box-title tip-title" id="tip-renaming-a-dataset"><button class="gtn-boxify-button tip" type="button" aria-controls="tip-renaming-a-dataset" aria-expanded="true"><i class="far fa-lightbulb" aria-hidden="true"></i> <span>Tip: Renaming a dataset</span><span class="fold-unfold fa fa-minus-square"></span></button></div>   <ul>   <li>Click on the <i class="fas fa-pencil-alt" aria-hidden="true"></i><span class="visually-hidden">galaxy-pencil</span> <strong>pencil icon</strong> for the dataset to edit its attributes</li>   <li>In the central panel, change the <strong>Name</strong> field</li>   <li>Click the <strong>Save</strong> button</li> </ul> </blockquote>
      <p><!--END_SNIPPET--></p>
    </li>
    <li>
      <p>Check that the datatype of all the three datasets is <code class="language-plaintext highlighter-rouge">tabular</code>.</p>

      <!--SNIPPET-->
      <blockquote class="tip">   <div class="box-title tip-title" id="tip-changing-the-datatype"><button class="gtn-boxify-button tip" type="button" aria-controls="tip-changing-the-datatype" aria-expanded="true"><i class="far fa-lightbulb" aria-hidden="true"></i> <span>Tip: Changing the datatype</span><span class="fold-unfold fa fa-minus-square"></span></button></div>   <ul>   <li>Click on the <i class="fas fa-pencil-alt" aria-hidden="true"></i><span class="visually-hidden">galaxy-pencil</span> <strong>pencil icon</strong> for the dataset to edit its attributes</li>   <li>In the central panel, click <i class="fas fa-database" aria-hidden="true"></i><span class="visually-hidden">galaxy-chart-select-data</span> <strong>Datatypes</strong> tab on the top</li>   <li>In the <i class="fas fa-database" aria-hidden="true"></i><span class="visually-hidden">galaxy-chart-select-data</span> <strong>Assign Datatype</strong>, select <code class="language-plaintext highlighter-rouge">datatypes</code> from “<em>New type</em>” dropdown     <ul>       <li>Tip: you can start typing the datatype into the field to filter the dropdown menu</li>     </ul>   </li>   <li>Click the <strong>Save</strong> button</li> </ul> </blockquote>
      <p><!--END_SNIPPET--></p>
    </li>
  </ol>

</blockquote>

<h2 id="classification-of-fruit-360-dataset-images-with-cnn">Classification of fruit 360 dataset images with CNN</h2>

<p>In this section, we define a CNN and train it using fruit 360 dataset training data. The
goal is to learn a model such that given an image of a fruit/vegetable, we can predict
what fruit/vegetable it is (Labels are in the range of 0 to 9). We then evaluate the trained
CNN on the test dataset and plot the confusion matrix.</p>

<p>In order to train the CNN, we must have the One-Hot Encoding (OHE) representation of the training
labels. This is needed to calculate the categorical cross entropy loss function. OHE encodes labels
as a <strong>one-hot</strong> numeric array, where only one element is 1 and the rest are 0’s. For example, if
we had 3 fruits (apple, orange, banana) and their labels were 1, 2, and 3, the OHE
represntation of apple would be (1,0,0), the OHE representation of orange would be (0,1,0), and the
OHE representation of banana would be (0,0,1). For apple with label 1, the first element of array
is 1 (and the rest are 0’s); For Orange with label 2, the second element of the array is 1 (and the
rest are 0’s); And for Banana with label 3, the third element of the array is 1 (and the rest are 0’s).
We have 10 fruits/vegetables in our dataset and we would just have an array of size 10, where only one
element is 1, corresponding to fruit/vegetable label, and the rest are 0’s.</p>

<p>In order to calculate the OHE of labels, we must first extract the labels column from train_y_10 file.
train_y_10 file has 3 columns: Label_name (string representation of label), file_name (name of fruit/vegetable
image file), and Label (integer representation of label). We extract Label from train_y_10, and then calcuate
its OHE representation.</p>

<h3 id="extract-the-label-column-from-train_y_10">Extract the Label column from train_y_10</h3>

<blockquote class="notranslate hands_on">
  <div class="box-title hands-on-title" id="hands-on-advanced-cut"><i class="fas fa-pencil-alt" aria-hidden="true" ></i> Hands-on: Advanced Cut</div>

  <ul>
    <li><span class="tool" data-tool="toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cut_tool/1.1.0" title="Advanced Cut tool" aria-role="button"><i class="fas fa-wrench" aria-hidden="true"></i> <strong>Advanced Cut</strong> (<i class="fas fa-cubes" aria-hidden="true"></i> Galaxy version 1.1.0)</span>
      <ul>
        <li><em>“File to cut”</em> : Select <code class="language-plaintext highlighter-rouge">train_y_10</code></li>
        <li><em>“Operation”</em> : Select <code class="language-plaintext highlighter-rouge">Keep</code></li>
        <li><em>“Delimited by”</em>: Select <code class="language-plaintext highlighter-rouge">Tab</code></li>
        <li><em>“Cut by”</em>: Select <code class="language-plaintext highlighter-rouge">fields</code></li>
        <li><em>“List of fields”</em>: Select <code class="language-plaintext highlighter-rouge">Column: 3</code></li>
        <li>Click <em>“Run Tool”</em></li>
      </ul>
    </li>
  </ul>

</blockquote>

<h3 id="create-one-hot-encoding-ohe-representation-of-training-labels">Create One-Hot Encoding (OHE) representation of training labels</h3>

<blockquote class="notranslate hands_on">
  <div class="box-title hands-on-title" id="hands-on-one-hot-encoding"><i class="fas fa-pencil-alt" aria-hidden="true" ></i> Hands-on: One-Hot Encoding</div>

  <ul>
    <li><span class="tool" data-tool="toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_to_categorical/sklearn_to_categorical/1.0.8.3" title="To categorical tool" aria-role="button"><i class="fas fa-wrench" aria-hidden="true"></i> <strong>To categorical</strong> (<i class="fas fa-cubes" aria-hidden="true"></i> Galaxy version 1.0.8.3)</span>
      <ul>
        <li><em>“Input file”</em> : Select the output of the previous step.</li>
        <li><em>“Does the dataset contain header?”</em> : Select <code class="language-plaintext highlighter-rouge">Yes</code></li>
        <li><em>“Total number of classes”</em>: Select <code class="language-plaintext highlighter-rouge">10</code></li>
        <li>Click <em>“Run Tool”</em></li>
      </ul>
    </li>
  </ul>

</blockquote>

<h3 id="create-a-deep-learning-model-architecture">Create a deep learning model architecture</h3>

<blockquote class="notranslate hands_on">
  <div class="box-title hands-on-title" id="hands-on-model-config"><i class="fas fa-pencil-alt" aria-hidden="true" ></i> Hands-on: Model config</div>

  <ul>
    <li><span class="tool" data-tool="toolshed.g2.bx.psu.edu/repos/bgruening/keras_model_config/keras_model_config/0.5.0" title="Create a deep learning model architecture tool" aria-role="button"><i class="fas fa-wrench" aria-hidden="true"></i> <strong>Create a deep learning model architecture</strong> (<i class="fas fa-cubes" aria-hidden="true"></i> Galaxy version 0.5.0)</span>
      <ul>
        <li><em>“Select keras model type”</em>: <code class="language-plaintext highlighter-rouge">sequential</code></li>
        <li><em>“input_shape”</em>: <code class="language-plaintext highlighter-rouge">(30000,)</code></li>
        <li>In <em>“LAYER”</em>:
          <ul>
            <li><i class="far fa-plus-square" aria-hidden="true"></i><span class="visually-hidden">param-repeat</span> <em>“1: LAYER”</em>:
              <ul>
                <li><em>“Choose the type of layer”</em>: <code class="language-plaintext highlighter-rouge">Core -- Reshape</code>
                  <ul>
                    <li><em>“target_shape”</em>: <code class="language-plaintext highlighter-rouge">(100,100,3)</code></li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><i class="far fa-plus-square" aria-hidden="true"></i><span class="visually-hidden">param-repeat</span> <em>“2: LAYER”</em>:
              <ul>
                <li><em>“Choose the type of layer”</em>: <code class="language-plaintext highlighter-rouge">Convolutional -- Conv2D</code>
                  <ul>
                    <li><em>“filters”</em>: <code class="language-plaintext highlighter-rouge">16</code></li>
                    <li><em>“kernel_size”</em>: <code class="language-plaintext highlighter-rouge">5</code></li>
                    <li><em>“Activation function”</em>: <code class="language-plaintext highlighter-rouge">relu</code></li>
                    <li><em>“Type in key words arguments if different from the default”</em>: <code class="language-plaintext highlighter-rouge">input_shape=(100, 100, 3)</code></li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><i class="far fa-plus-square" aria-hidden="true"></i><span class="visually-hidden">param-repeat</span> <em>“3: LAYER”</em>:
              <ul>
                <li><em>“Choose the type of layer”</em>: <code class="language-plaintext highlighter-rouge">Pooling -- MaxPooling2D</code>
                  <ul>
                    <li><em>“pool_size”</em>: <code class="language-plaintext highlighter-rouge">(2,2)</code></li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><i class="far fa-plus-square" aria-hidden="true"></i><span class="visually-hidden">param-repeat</span> <em>“4: LAYER”</em>:
              <ul>
                <li><em>“Choose the type of layer”</em>: <code class="language-plaintext highlighter-rouge">Convolutional -- Conv2D</code>
                  <ul>
                    <li><em>“filters”</em>: <code class="language-plaintext highlighter-rouge">32</code></li>
                    <li><em>“kernel_size”</em>: <code class="language-plaintext highlighter-rouge">5</code></li>
                    <li><em>“Activation function”</em>: <code class="language-plaintext highlighter-rouge">relu</code></li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><i class="far fa-plus-square" aria-hidden="true"></i><span class="visually-hidden">param-repeat</span> <em>“5: LAYER”</em>:
              <ul>
                <li><em>“Choose the type of layer”</em>: <code class="language-plaintext highlighter-rouge">Pooling -- MaxPooling2D</code>
                  <ul>
                    <li><em>“pool_size”</em>: <code class="language-plaintext highlighter-rouge">(2,2)</code></li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><i class="far fa-plus-square" aria-hidden="true"></i><span class="visually-hidden">param-repeat</span> <em>“6: LAYER”</em>:
              <ul>
                <li><em>“Choose the type of layer”</em>: <code class="language-plaintext highlighter-rouge">Convolutional -- Conv2D</code>
                  <ul>
                    <li><em>“filters”</em>: <code class="language-plaintext highlighter-rouge">64</code></li>
                    <li><em>“kernel_size”</em>: <code class="language-plaintext highlighter-rouge">5</code></li>
                    <li><em>“Activation function”</em>: <code class="language-plaintext highlighter-rouge">relu</code></li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><i class="far fa-plus-square" aria-hidden="true"></i><span class="visually-hidden">param-repeat</span> <em>“7: LAYER”</em>:
              <ul>
                <li><em>“Choose the type of layer”</em>: <code class="language-plaintext highlighter-rouge">Pooling -- MaxPooling2D</code>
                  <ul>
                    <li><em>“pool_size”</em>: <code class="language-plaintext highlighter-rouge">(2,2)</code></li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><i class="far fa-plus-square" aria-hidden="true"></i><span class="visually-hidden">param-repeat</span> <em>“8: LAYER”</em>:
              <ul>
                <li><em>“Choose the type of layer”</em>: <code class="language-plaintext highlighter-rouge">Core -- Flatten</code></li>
              </ul>
            </li>
            <li><i class="far fa-plus-square" aria-hidden="true"></i><span class="visually-hidden">param-repeat</span> <em>“9: LAYER”</em>:
              <ul>
                <li><em>“Choose the type of layer”</em>: <code class="language-plaintext highlighter-rouge">Core -- Dense</code>
                  <ul>
                    <li><em>“units”</em>”: <code class="language-plaintext highlighter-rouge">256</code></li>
                    <li><em>“Activation function”</em>: <code class="language-plaintext highlighter-rouge">relu</code></li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><i class="far fa-plus-square" aria-hidden="true"></i><span class="visually-hidden">param-repeat</span> <em>“10: LAYER”</em>:
              <ul>
                <li><em>“Choose the type of layer”</em>: <code class="language-plaintext highlighter-rouge">Core -- Dense</code>
                  <ul>
                    <li><em>“units”</em>”: <code class="language-plaintext highlighter-rouge">10</code></li>
                    <li><em>“Activation function”</em>: <code class="language-plaintext highlighter-rouge">softmax</code></li>
                  </ul>
                </li>
              </ul>
            </li>
          </ul>
        </li>
        <li>Click <em>“Run Tool”</em></li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>Each image is passed in as a vector of size 30,000 (100 x 100 X 3 = 30,000). The reshape
layer reshapes it into (100, 100, 3) dimensions – 100 rows (image height), 100 columns
(image width), and 3 channels. Channel size is 3 since the image is color (RGB) and each
color pixel can be represented by 3 integers, representing the Red, Green, and Blue
primary colors. Our CNN then has 3 convolution + pooling layers. The first convolution layer
has 16 filters (output channel size would be 16), and filter size is 5 x 5. The second convolutional
layer has 32 filters (output channel size would be 32), and filter size is 5 x 5. The third
convolutional layer has 64 filters (output channel size would be 64), and filter size is 5 x 5. All
3 pooling layers are MaxPool layers with pool size of 2 x 2. Afterwards, we flatten the previous layer’s
output (every row/column/channel would be an individual node), then add a fully connected layer with 256
nodes and relu activation function. Finally, we add a fully connected layers with 10 nodes, and use
softmax activation function to get the probability of each fruit/vegetable. Fruit/vegetable with the
highest probability is predicted by CNN. The model config can be downloaded as a JSON file.</p>

<h3 id="create-a-deep-learning-model">Create a deep learning model</h3>

<blockquote class="notranslate hands_on">
  <div class="box-title hands-on-title" id="hands-on-model-builder-optimizer-loss-function-and-fit-parameters"><i class="fas fa-pencil-alt" aria-hidden="true" ></i> Hands-on: Model builder (Optimizer, loss function, and fit parameters)</div>

  <ul>
    <li><span class="tool" data-tool="toolshed.g2.bx.psu.edu/repos/bgruening/keras_model_builder/keras_model_builder/0.5.0" title="Create deep learning model tool" aria-role="button"><i class="fas fa-wrench" aria-hidden="true"></i> <strong>Create deep learning model</strong> (<i class="fas fa-cubes" aria-hidden="true"></i> Galaxy version 0.5.0)</span>
      <ul>
        <li><em>“Choose a building mode”</em>: <code class="language-plaintext highlighter-rouge">Build a training model</code></li>
        <li><em>“Select the dataset containing model configuration”</em>: Select the <em>Keras Model Config</em> from the previous step.</li>
        <li><em>“Do classification or regression?”</em>: <code class="language-plaintext highlighter-rouge">KerasGClassifier</code></li>
        <li>In <em>“Compile Parameters”</em>:
          <ul>
            <li><em>“Select a loss function”</em>: <code class="language-plaintext highlighter-rouge">categorical_crossentropy</code></li>
            <li><em>“Select an optimizer”</em>: <code class="language-plaintext highlighter-rouge">Adam - Adam optimizer </code></li>
            <li><em>“Select metrics”</em>: <code class="language-plaintext highlighter-rouge">acc/accuracy</code></li>
          </ul>
        </li>
        <li>In <em>“Fit Parameters”</em>:
          <ul>
            <li><em>“epochs”</em>: <code class="language-plaintext highlighter-rouge">40</code></li>
            <li><em>“batch_size”</em>: <code class="language-plaintext highlighter-rouge">50</code></li>
          </ul>
        </li>
        <li>Click <em>“Run Tool”</em></li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>A loss function measures how different the predicted output is from the expected output. For multi-class classification problems,
we use <em>categorical cross entropy</em> as loss function. Epochs is the number of times the whole training data is used to train the
model. Setting <em>epochs</em> to 40 means each training example in our dataset is used 40 times to train our model. If we update network
weights/biases after all the training data is feed to the network, the training will be very slow (as we have 5014 training examples
in our dataset). To speed up the training, we present only a subset of the training examples to the network, after which we update
the weights/biases. <em>batch_size</em> decides the size of this subset. The model builder can be downloaded as a zip file.</p>

<h3 id="deep-learning-training-and-evaluation">Deep learning training and evaluation</h3>

<blockquote class="notranslate hands_on">
  <div class="box-title hands-on-title" id="hands-on-training-the-model"><i class="fas fa-pencil-alt" aria-hidden="true" ></i> Hands-on: Training the model</div>

  <ul>
    <li><span class="tool" data-tool="toolshed.g2.bx.psu.edu/repos/bgruening/keras_train_and_eval/keras_train_and_eval/1.0.8.2" title="Deep learning training and evaluation tool" aria-role="button"><i class="fas fa-wrench" aria-hidden="true"></i> <strong>Deep learning training and evaluation</strong> (<i class="fas fa-cubes" aria-hidden="true"></i> Galaxy version 1.0.8.2)</span>
      <ul>
        <li><em>“Select a scheme”</em>: <code class="language-plaintext highlighter-rouge">Train and Validate</code></li>
        <li><em>“Choose the dataset containing pipeline/estimator object”</em>: Select the <em>Keras Model Builder</em> from the previous step.</li>
        <li><em>“Select input type:”</em>: <code class="language-plaintext highlighter-rouge">tabular data</code>
          <ul>
            <li><em>“Training samples dataset”</em>: Select <code class="language-plaintext highlighter-rouge">train_X_10</code> dataset</li>
            <li><em>“Choose how to select data by column:”</em>: <code class="language-plaintext highlighter-rouge">All columns</code></li>
            <li><em>“Dataset containing class labels or target values”</em>: Select the OHE representation of <code class="language-plaintext highlighter-rouge">train_y_10</code> dataset</li>
            <li><em>“Choose how to select data by column:”</em>: <code class="language-plaintext highlighter-rouge">All columns</code></li>
          </ul>
        </li>
        <li>Click <em>“Run Tool”</em></li>
      </ul>
    </li>
  </ul>

</blockquote>

<p>The training step generates 3 datasets. 1) accuracy of the trained model, 2) the trained model, downloadable as a zip file, and
3) the trained model weights, downloadable as an hdf5 file. These files are needed for prediction in the next step.</p>

<h3 id="model-prediction">Model Prediction</h3>

<blockquote class="notranslate hands_on">
  <div class="box-title hands-on-title" id="hands-on-testing-the-model"><i class="fas fa-pencil-alt" aria-hidden="true" ></i> Hands-on: Testing the model</div>

  <ul>
    <li><span class="tool" data-tool="toolshed.g2.bx.psu.edu/repos/bgruening/model_prediction/model_prediction/1.0.8.2" title="Model Prediction tool" aria-role="button"><i class="fas fa-wrench" aria-hidden="true"></i> <strong>Model Prediction</strong> (<i class="fas fa-cubes" aria-hidden="true"></i> Galaxy version 1.0.8.2)</span>
      <ul>
        <li><em>“Choose the dataset containing pipeline/estimator object”</em> : Select the trained model from the previous step.</li>
        <li><em>“Choose the dataset containing weights for the estimator above”</em> : Select the trained model weights from the previous step.</li>
        <li><em>“Select invocation method”</em>: <code class="language-plaintext highlighter-rouge">predict</code></li>
        <li><em>“Select input data type for prediction”</em>: <code class="language-plaintext highlighter-rouge">tabular data</code></li>
        <li><em>“Training samples dataset”</em>: Select <code class="language-plaintext highlighter-rouge">test_X_10</code> dataset</li>
        <li><em>“Choose how to select data by column:”</em>: <code class="language-plaintext highlighter-rouge">All columns</code></li>
        <li>Click <em>“Run Tool”</em></li>
      </ul>
    </li>
  </ul>

</blockquote>

<p>The prediction step generates 1 dataset. It’s a file that has predictions (0 to 9 for the predicted fruit/vegetable) for every image
in the test dataset.</p>

<h3 id="machine-learning-visualization-extension">Machine Learning Visualization Extension</h3>

<blockquote class="notranslate hands_on">
  <div class="box-title hands-on-title" id="hands-on-creating-the-confusion-matrix"><i class="fas fa-pencil-alt" aria-hidden="true" ></i> Hands-on: Creating the confusion matrix</div>

  <ul>
    <li><span class="tool" data-tool="toolshed.g2.bx.psu.edu/repos/bgruening/ml_visualization_ex/ml_visualization_ex/1.0.8.2" title="Machine Learning Visualization Extension tool" aria-role="button"><i class="fas fa-wrench" aria-hidden="true"></i> <strong>Machine Learning Visualization Extension</strong> (<i class="fas fa-cubes" aria-hidden="true"></i> Galaxy version 1.0.8.2)</span>
      <ul>
        <li><em>“Select a plotting type”</em>: <code class="language-plaintext highlighter-rouge">Confusion matrix for classes</code></li>
        <li><em>“Select dataset containing the true labels”</em>”: <code class="language-plaintext highlighter-rouge">test_y_10</code></li>
        <li><em>“Does the dataset contain header:”</em>: <code class="language-plaintext highlighter-rouge">Yes</code></li>
        <li><em>“Choose how to select data by column:”</em>: <code class="language-plaintext highlighter-rouge">Select columns by column header name(s)</code>
 	- <em>“Type header name(s):”</em>: <code class="language-plaintext highlighter-rouge">Label</code></li>
        <li><em>“Select dataset containing the predicted labels”</em>”: Select <code class="language-plaintext highlighter-rouge">Model Prediction</code> from the previous step</li>
        <li><em>“Does the dataset contain header:”</em>: <code class="language-plaintext highlighter-rouge">Yes</code></li>
        <li>Click <em>“Run Tool”</em></li>
      </ul>
    </li>
  </ul>

</blockquote>

<p><strong>Confusion Matrix</strong> is a table that describes the performance of a classification model. It lists the number of examples that were
correctly classified by the model, True positives (TP) and true negatives (TN). It also lists the number of examples that were
classified as positive that were actually negative (False positive, FP, or Type I error), and the number of examples that were
classified as negative that were actually positive (False negative, FN, or Type 2 error). Given the confusion matrix, we can
calculate <strong>precision</strong> and <strong>recall</strong> <span class="citation"><a href="#TatbulEtAl">Tatbul <i>et al.</i> 2018</a></span>. Precision is the fraction of predicted positives that are true
positives (Precision = TP / (TP + FP)). Recall is the fraction of true positives that are predicted (Recall = TP / (TP + FN)).
One way to describe the confusion matrix with just one value is to use the <strong>F score</strong>, which is the harmonic mean of precision
and recall</p>

\[Precision = \frac{\text{True positives}}{\text{True positives + False positives}}\]

\[Recall = \frac{\text{True positives}}{\text{True positives + False negatives}}\]

\[F score = \frac{2 * \text{Precision * Recall}}{\text{Precision + Recall}}\]

<figure id="figure-10" style="max-width: 90%;"><img src="../../images/Fruit_confusion_matrix.png" alt="Confusion matrix for fruit 360 image classification problem. " width="922" height="973" loading="lazy" /><a target="_blank" href="../../images/Fruit_confusion_matrix.png" rel="noopener noreferrer"><small>Open image in new tab</small></a><br /><br /><figcaption><span class="figcaption-prefix"><strong>Figure 10</strong>:</span> Fruits/vegetables image classification confusion matrix</figcaption></figure>

<p>Figure 1 is the resultant confusion matrix for our image classification problem. The first row in the table represents the <em>true</em>
fruit/vegetable with 0 as class label, which is strawberry (we have 164 strawberry images with 0 as class label). The second row
represents the <em>true</em> fruit/vegetable with 1 as class label, which is Apple_Red_Delicious (We have 166 Apple_Red_Delicious images
with 1 as class label). Similarly, you can count the true class labels for fruits/vegetables with class label of 2 to 9 by adding
up the numbers in the corresponding row. The first column from the left represents the <em>predicted</em> fruit/vegetable with 0 as class
label, which is strawberry (Our CNN predicted 164 images as being strawberry, and having class label 0). The second column from
the left represents the <em>predicted</em> fruit/vegetable with 1 as class label, which is Apple_Red_Delicious (Our CNN predicted 166
images as being Apple_Red_Delicious, and having class label 1). Similarly, you can count the predicted class labels for
fruits/vegetables with class labels 2 to 9 by adding up the numbers in the corresponding column.</p>

<p>For label 3, which is corn, looking at the green cell in the 4th row and 4th column, we see that our CNN has correctly
predicted 118 images as being a corn image (True positives). Adding the numbers in the other rows in column 4, we see that
our CNN has incorrectly predicted 16 images as being corn (False positives). Adding the numbers on the 4th row besides the
True positives, we see that our CNN has incorrectly predicted 32 corn images as being label Potato_White (False negatives).
Given these numbers we can calculate Precision, Recall, and the F score for digit 0 as follows:</p>

\[Precision = \frac{\text{True positives}}{\text{True positives + False positives}} = \frac{118}{118 + 16} = 0.88\]

\[Recall = \frac{\text{True positives}}{\text{True positives + False negatives}} = \frac{118}{118 + 32} = 0.78\]

\[F score = \frac{2 * \text{Precision * Recall}}{\text{Precision + Recall}} = \frac{2 * 0.88 * 0.78}{0.88 + 0.78} = 0.82\]

<p>You can calculate the Precision, Recall, and F score for other digits in a similar manner.</p>

<h1 id="conclusion">Conclusion</h1>

<p>In this tutorial, we briefly described convolutional neural networks (CNN) and their application to image classification problems.
We then used Galaxy’s ML toolkit to solve an image classification problem using CNN on fruit 360 dataset.</p>

                </section>

                <section aria-label="Tutorial Footer, Feedback, Citation" id="tutorial-footer">
                        <h3>You've Finished the Tutorial</h3>
                        <button id="tutorial-finish-button" class="btn btn-primary" onclick="tutorial_finish()">I finished this tutorial 👍</button>
                        <p style="display: none" id="tutorial-finish-text">Please also consider filling out the <a href="#gtn-feedback">Feedback Form</a> as well!</p>
                        <script>
                        function tutorial_finish() {
                          if(typeof plausible !== 'undefined'){
                            // Plausible may be undefined (script blocked)
                            // or it may be defined, but opted-out (select box/DNT),
                            // which means `plausible()` will work but not send data, *nor* execute the callback.
                            plausible('TutorialComplete', {props: {path: document.location.pathname}})
                          }
                          // since the callback is completely cosmetic, we'll just issue it optimistically.
                          tutorial_finish_finish();
                        }
                        function tutorial_finish_finish() {
                          document.getElementById("tutorial-finish-button").innerText = 'Congrats! Thanks for letting us know! 🎉'
                          document.getElementById("tutorial-finish-button").disabled = true
                          document.getElementById("tutorial-finish-button").disabled = true
                          document.getElementById("tutorial-finish-text").style.display = 'block'
                        }
                        </script>

                

                <h1>Frequently Asked Questions</h1>
                Have questions about this tutorial? Check out the <a href="faqs/">tutorial FAQ page</a> or the  <a href="/training-material/topics/statistics/faqs/">FAQ page for the Statistics and machine learning topic</a> to see if your question is listed there.
                If not, please ask your question on the <a href="https://gitter.im/Galaxy-Training-Network/Lobby">GTN Gitter Channel</a> or the
                <a href="https://help.galaxyproject.org">Galaxy Help Forum</a>

                

                


                
                <h1 id="bibliography">References</h1>
                <ol class="bibliography"><li id="HubelWiesel">Hubel, D. H., and T. N. Wiesel, 1959 <b>Receptive fields of single neurones in the cat’s striate cortex</b>. The Journal of physiology 148: 574–591. <a href="https://doi.org/10.1113/jphysiol.1959.sp006308">10.1113/jphysiol.1959.sp006308</a></li>
<li id="Fukishima">Fukushima, K., 1988 <b>Neocognitron: A hierarchical neural network capable of visual pattern recognition</b>. Neural Networks 1: 119–130. <a href="https://doi.org/https://doi.org/10.1016/0893-6080(88)90014-7">https://doi.org/10.1016/0893-6080(88)90014-7</a> <a href="https://www.sciencedirect.com/science/article/pii/0893608088900147">https://www.sciencedirect.com/science/article/pii/0893608088900147</a></li>
<li id="LeCunEtAl">LeCun, Y., B. Boser, J. S. Denker, D. Henderson, R. E. Howard <i>et al.</i>, 1989 <b>Backpropagation Applied to Handwritten Zip Code Recognition</b>. Neural Computation 1: 541–551. <a href="https://doi.org/10.1162/neco.1989.1.4.541">10.1162/neco.1989.1.4.541</a></li>
<li id="OSheaEtAl">O’Shea, K., and R. Nash, 2015 <b>An Introduction to Convolutional Neural Networks</b>. CoRR abs/1511.08458: <a href="http://arxiv.org/abs/1511.08458">http://arxiv.org/abs/1511.08458</a></li>
<li id="DumoulinVisin">Dumoulin, V., and F. Visin, 2016 <b>A guide to convolution arithmetic for deep learning</b>. <a href="https://doi.org/1603.07285">1603.07285</a></li>
<li id="Iswari2017">Iswari, N. M. S., Wella, and Ranny, 2017 <b>Fruitylicious: Mobile application for fruit ripeness determination based on fruit image</b>, <span style="font-style: normal">in</span> <i>2017 10th International Conference on Human System Interactions (HSI)</i>, <i>IEEE</i>. <a href="https://doi.org/10.1109/hsi.2017.8005025">10.1109/hsi.2017.8005025</a></li>
<li id="Kamilaris2018">Kamilaris, A., and F. X. Prenafeta-Boldú, 2018 <b>Deep learning in agriculture: A survey</b>. Computers and Electronics in Agriculture 147: 70–90. <a href="https://doi.org/10.1016/j.compag.2018.02.016">10.1016/j.compag.2018.02.016</a></li>
<li id="Murean2018">Mureşan, H., and M. Oltean, 2018 <b>Fruit recognition from images using deep learning</b>. Acta Universitatis Sapientiae,  Informatica 10: 26–42. <a href="https://doi.org/10.2478/ausi-2018-0002">10.2478/ausi-2018-0002</a></li>
<li id="NwankpaEtAl">Nwankpa, C., W. Ijomah, A. Gachagan, and S. Marshall, 2018 <b>Activation Functions: Comparison of trends in Practice and Research for Deep Learning</b>. CoRR abs/1811.03378: <a href="http://arxiv.org/abs/1811.03378">http://arxiv.org/abs/1811.03378</a></li>
<li id="TatbulEtAl">Tatbul, N., T. J. Lee, S. Zdonik, M. Alam, and J. Gottschlich, 2018 <b>Precision and Recall for Time Series</b>. Advances in Neural Information Processing Systems 31: 1920–1930. <a href="https://proceedings.neurips.cc/paper/2018/file/8f468c873a32bb0619eaeb2050ba45d1-Paper.pdf">https://proceedings.neurips.cc/paper/2018/file/8f468c873a32bb0619eaeb2050ba45d1-Paper.pdf</a></li>
<li id="Duong2020">Duong, L. T., P. T. Nguyen, C. D. Sipio, and D. D. Ruscio, 2020 <b>Automated fruit recognition using EfficientNet and MixNet</b>. Computers and Electronics in Agriculture 171: 105326. <a href="https://doi.org/10.1016/j.compag.2020.105326">10.1016/j.compag.2020.105326</a></li>
<li id="NaranjoTorres2020">Naranjo-Torres, J., M. Mora, Hernández-Garcı́a Ruber, R. J. Barrientos, C. Fredes <i>et al.</i>, 2020 <b>A Review of Convolutional Neural Network Applied to Fruit Image Processing</b>. Applied Sciences 10: 3443. <a href="https://doi.org/10.3390/app10103443">10.3390/app10103443</a></li></ol>
                

                

                <h1 id="gtn-feedback">Feedback</h1>
                <p class="text-muted">Did you use this material as an instructor? Feel free to give us feedback on <a href="https://github.com/galaxyproject/training-material/issues/1452">how it went</a>.
                <br>Did you use this material as a learner or student? Click the form below to leave feedback.<i class="fas fa-hand-point-down"></i>
                </p>

                <iframe id="feedback-google" class="google-form" src="https://docs.google.com/forms/d/e/1FAIpQLSd4VZptFTQ03kHkMz0JyW9b6_S8geU5KjNE_tLM0dixT3ZQmA/viewform?embedded=true&entry.1235803833=statistics/fruit_360"><a href="https://docs.google.com/forms/d/e/1FAIpQLSd4VZptFTQ03kHkMz0JyW9b6_S8geU5KjNE_tLM0dixT3ZQmA/viewform?embedded=true&entry.1235803833=statistics/fruit_360">Feedback Form</a></iframe>

                <h1>Citing this Tutorial</h1>
                <p>
                    <ol>
                        <li id="citation-text">
                            Kaivan Kamali,  <b>Image classification in Galaxy with fruit 360 dataset (Galaxy Training Materials)</b>. <a href="https://training.galaxyproject.org/training-material/topics/statistics/tutorials/fruit_360/tutorial.html">https://training.galaxyproject.org/training-material/topics/statistics/tutorials/fruit_360/tutorial.html</a> Online; accessed TODAY
                        </li>
                        <li>
                        Hiltemann, Saskia, Rasche, Helena et al., 2023 <b>Galaxy Training: A Powerful Framework for Teaching!</b> PLOS Computational Biology <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010752">10.1371/journal.pcbi.1010752</a>
                        </li>
                        <li>
                        Batut et al., 2018 <b>Community-Driven Data Analysis Training for Biology</b> Cell Systems <a href="https://doi.org/10.1016%2Fj.cels.2018.05.012">10.1016/j.cels.2018.05.012</a>
                        </li>
                    </ol>
                </p>

                <!-- collapsible boxcontaining the BibTeX-formatted citation -->
                <blockquote class="details">

                  <div id="citation-bibtex" class="box-title">
                    <button type="button" aria-controls="citation-bibtex" aria-expanded="false">
                      <i class="fas fa-info-circle" aria-hidden="true"></i>
                      <span class="visually-hidden"></span> BibTeX<span role="button" class="fold-unfold fa fa-minus-square" aria-hidden="true"></span>
                    </button>
                   </div>
                   <p style="display: none;">

                   <div class="highlighter-rouge"><div class="highlight"><pre class="highlight">


<code id="citation-code">@misc{statistics-fruit_360,
author = "Kaivan Kamali",
	title = "Image classification in Galaxy with fruit 360 dataset (Galaxy Training Materials)",
	year = "",
	month = "",
	day = "",
	url = "\url{https://training.galaxyproject.org/training-material/topics/statistics/tutorials/fruit_360/tutorial.html}",
	note = "[Online; accessed TODAY]"
}
@article{Hiltemann_2023,
	doi = {10.1371/journal.pcbi.1010752},
	url = {https://doi.org/10.1371%2Fjournal.pcbi.1010752},
	year = 2023,
	month = {jan},
	publisher = {Public Library of Science ({PLoS})},
	volume = {19},
	number = {1},
	pages = {e1010752},
	author = {Saskia Hiltemann and Helena Rasche and Simon Gladman and Hans-Rudolf Hotz and Delphine Larivi{\`{e}}re and Daniel Blankenberg and Pratik D. Jagtap and Thomas Wollmann and Anthony Bretaudeau and Nadia Gou{\'{e}} and Timothy J. Griffin and Coline Royaux and Yvan Le Bras and Subina Mehta and Anna Syme and Frederik Coppens and Bert Droesbeke and Nicola Soranzo and Wendi Bacon and Fotis Psomopoulos and Crist{\'{o}}bal Gallardo-Alba and John Davis and Melanie Christine Föll and Matthias Fahrner and Maria A. Doyle and Beatriz Serrano-Solano and Anne Claire Fouilloux and Peter van Heusden and Wolfgang Maier and Dave Clements and Florian Heyl and Björn Grüning and B{\'{e}}r{\'{e}}nice Batut and},
	editor = {Francis Ouellette},
	title = {Galaxy Training: A powerful framework for teaching!},
	journal = {PLoS Comput Biol}
}
</code>
                   </pre></div></div>
                   </p>
                </blockquote>

        


<script>
// update the date on load, or leave fallback of 'today'
const citationTodaysDate = new Date();
document.getElementById("citation-code").innerHTML = document.getElementById("citation-code").innerHTML.replace("TODAY", citationTodaysDate.toDateString());
document.getElementById("citation-text").innerHTML = document.getElementById("citation-text").innerHTML.replace("TODAY", citationTodaysDate.toDateString());
</script>

                <i class="far fa-thumbs-up" aria-hidden="true"></i> Congratulations on successfully completing this tutorial!

                

                

		
                <blockquote class="details follow-up" id="admins-install-missing-tools">
                  <div id="admin-missing-tools" class="box-title">
                    <button type="button" aria-controls="admin-missing-tools" aria-expanded="false">
                      <i class="fas fa-info-circle" aria-hidden="true"></i>
                      <span class="visually-hidden"></span> Galaxy Administrators: Install the missing tools<span role="button" class="fold-unfold fa fa-minus-square" aria-hidden="true"></span>
                    </button>
                   </div>


			<p>You can use Ephemeris's <code>shed-tools install</code> command to install the tools used in this tutorial.</p>
<div class="highlight"><pre class="highlight"><code>shed-tools install [-g GALAXY] [-a API_KEY] -t &lt;(curl https://training.galaxyproject.org/training-material/api/topics/statistics/tutorials/fruit_360/tutorial.json | jq .admin_install_yaml -r)</code></pre></div>
<p>Alternatively you can copy and paste the following YAML</p>
<div class="highlight"><pre class="highlight"><code>---
install_tool_dependencies: true
install_repository_dependencies: true
install_resolver_dependencies: true
tools:
- name: keras_model_builder
  owner: bgruening
  revisions: 772e0e89fc68
  tool_panel_section_label: Machine Learning
  tool_shed_url: https://toolshed.g2.bx.psu.edu/
- name: keras_model_config
  owner: bgruening
  revisions: 8a794e6d3388
  tool_panel_section_label: Machine Learning
  tool_shed_url: https://toolshed.g2.bx.psu.edu/
- name: keras_train_and_eval
  owner: bgruening
  revisions: f1b9a42d6809
  tool_panel_section_label: Machine Learning
  tool_shed_url: https://toolshed.g2.bx.psu.edu/
- name: keras_train_and_eval
  owner: bgruening
  revisions: b6ef6d0cb6b7
  tool_panel_section_label: Machine Learning
  tool_shed_url: https://toolshed.g2.bx.psu.edu/
- name: ml_visualization_ex
  owner: bgruening
  revisions: 05143043ca13
  tool_panel_section_label: Machine Learning
  tool_shed_url: https://toolshed.g2.bx.psu.edu/
- name: ml_visualization_ex
  owner: bgruening
  revisions: 947c2eacccb8
  tool_panel_section_label: Machine Learning
  tool_shed_url: https://toolshed.g2.bx.psu.edu/
- name: model_prediction
  owner: bgruening
  revisions: 83228baae3c5
  tool_panel_section_label: Machine Learning
  tool_shed_url: https://toolshed.g2.bx.psu.edu/
- name: model_prediction
  owner: bgruening
  revisions: 1552b8cb4a94
  tool_panel_section_label: Machine Learning
  tool_shed_url: https://toolshed.g2.bx.psu.edu/
- name: sklearn_to_categorical
  owner: bgruening
  revisions: ec69cbe34b73
  tool_panel_section_label: Machine Learning
  tool_shed_url: https://toolshed.g2.bx.psu.edu/
- name: text_processing
  owner: bgruening
  revisions: d698c222f354
  tool_panel_section_label: Text Manipulation
  tool_shed_url: https://toolshed.g2.bx.psu.edu/
</code></pre></div>
                </blockquote>
		

		<blockquote class="details hide-when-printing" id="feedback-responses">
                  <div id="feedback-response-c" class="box-title">
                    <button type="button" aria-controls="feedback-response-c" aria-expanded="false">
                      <i class="fas fa-info-circle" aria-hidden="true"></i>
                      <span class="visually-hidden"></span> Feedback<span role="button" class="fold-unfold fa fa-minus-square" aria-hidden="true"></span>
                    </button>
                   </div>

		   <p>
		   
		   <table class="charts-css bar show-labels" style="--labels-size: 8rem; overflow-y: hidden">
		   
			<tr>
				<th scope="row"><span class="sr-only">5 stars</span><i class="fa fa-star" aria-hidden="true"></i><i class="fa fa-star" aria-hidden="true"></i><i class="fa fa-star" aria-hidden="true"></i><i class="fa fa-star" aria-hidden="true"></i><i class="fa fa-star" aria-hidden="true"></i></th>
				<td style="--size: 1.0">2</td>
			</tr>
		   
		   </table>
		   </p>
		
        
        
		    
		    <b>March 2022</b>
		    <ul>
				
				<li>
					<span class="sr-only">5 stars</span><i class="fa fa-star" aria-hidden="true"></i><i class="fa fa-star" aria-hidden="true"></i><i class="fa fa-star" aria-hidden="true"></i><i class="fa fa-star" aria-hidden="true"></i><i class="fa fa-star" aria-hidden="true"></i>:
					<b>Liked</b>: great tutorial overall
					
				</li>
				
		    </ul>
		    
		    <b>December 2021</b>
		    <ul>
				
				<li>
					<span class="sr-only">5 stars</span><i class="fa fa-star" aria-hidden="true"></i><i class="fa fa-star" aria-hidden="true"></i><i class="fa fa-star" aria-hidden="true"></i><i class="fa fa-star" aria-hidden="true"></i><i class="fa fa-star" aria-hidden="true"></i>:
					<b>Liked</b>: this tutorial explains CNN for image analysis completely and simply.
					
				</li>
				
		    </ul>
		    
        
		</blockquote>




		
		
		

                </section>

            </div>
        </div>
    </div>
</article>
<br/>
<br/>
<br/>

        </div>
        <footer>
	<hr />
	<div class="container">
		<div class="row">
			<div class="col-sm-3">
				<span style="font-size: 2em">GTN</span>
				<p>
					The GTN provides learners with a free, open repository of online training
					materials, with a focus on hands-on training that aims to be directly applicable for learners.
					We aim to connect researchers and learners with local trainers, and events worldwide.
				</p>
				<p>
					We promote FAIR and Open Science practices worldwide, are committed to the accessibility of this platform and training for everyone.
				</p>
			</div>
			<div class="col-sm-3">
				<span style="font-size: 1.3em">About Us</span>
				<ul class="no-bullets">
					<li><a href="/training-material/about.html">About</a></li>
					<li><a rel="code-of-conduct" href="https://galaxyproject.org/community/coc/">Code of Conduct</a></li>
					<li><a href="/training-material/accessibility.html">Accessibility</a></li>
					<li><a href="/training-material/faqs/gtn/fair_training.html">100% FAIR Training</a></li>
					<li><a href="/training-material/faqs/gtn/collaborative_development.html">Collaborative Development</a></li>
				</ul>
				<span style="font-size: 1.3em">Page</span>
				<ul class="no-bullets">
					
					<li><i class="fas fa-fingerprint" aria-hidden="true"></i><span class="visually-hidden">purl</span><abbr title="Persistent URL">PURL</abbr>: <a href="https://gxy.io/GTN:T00265">gxy.io/GTN:T00265</a></li>
					

					

					<li>
						<a rel="license" href="https://spdx.org/licenses/CC-BY-4.0">
							Content licensed under Creative Commons Attribution 4.0 International License
						</a>
					</li>
					<li>
						<a href="https://github.com/galaxyproject/training-material/edit/main/topics/statistics/tutorials/fruit_360/tutorial.md">
						<i class="fab fa-github" aria-hidden="true"></i><span class="visually-hidden">github</span> Edit on GitHub
						</a>
					</li>
					<li>
						<a href="https://github.com/galaxyproject/training-material/commits/main/topics/statistics/tutorials/fruit_360/tutorial.md">
						<i class="fab fa-github" aria-hidden="true"></i><span class="visually-hidden">github</span> View Changes on GitHub
						</a>
					</li>
				</ul>
			</div>
			<div class="col-sm-3">
				<span style="font-size: 1.3em">Support</span>
				<ul class="no-bullets">
					<li><a rel="me" href="/training-material/faqs/galaxy/">Galaxy FAQs</a></li>
					<li><a rel="me" href="https://help.galaxyproject.org">Galaxy Help Forum</a></li>
					<li><a rel="me" href="http://gxy.io/gtn-slack">GTN Slack Chat</a></li>
					<li><a rel="me" href="https://matrix.to/#/%23Galaxy-Training-Network_Lobby%3Agitter.im">GTN Matrix Chat</a></li>
					<li><a rel="me" href="https://matrix.to/#/#galaxyproject_Lobby:gitter.im">Galaxy Matrix Chat</a></li>
				</ul>
				<span style="font-size: 1.3em">Framework</span>
				<ul class="no-bullets">
					<li>Revision <a href="https://github.com/galaxyproject/training-material/commit/066679f25030e7855a8739c65b9c688a61009b66">066679f</a></li>
					<li><a rel="license" href="https://github.com/galaxyproject/training-material/blob/main/LICENSE.md">MIT</a> Licensed</li>
					<li><a href="https://jekyllrb.com/">Jekyll(4.3.2 | production)</a></li>
				</ul>
			</div>
			<div class="col-sm-3">
				<span style="font-size: 1.3em">Follow Us!</span>
				<ul class="no-bullets">
					<li><span style="fill: var(--hyperlink);"><svg   width="1em"   height="1em"   viewBox="0 0 8.4937906 9.1084023"   version="1.1"   id="svg356"   xmlns="http://www.w3.org/2000/svg"   xmlns:svg="http://www.w3.org/2000/svg">  <g     id="layer1"     transform="translate(-70.566217,-144.26757)">    <path       style="fill-opacity:1;stroke:none;stroke-width:0.0179182"       d="m 76.39081,152.24155 c -0.737138,0.20763 -1.554999,0.29101 -2.311453,0.14333 -0.475335,-0.0928 -0.891898,-0.32923 -1.031589,-0.82423 -0.04356,-0.15434 -0.06132,-0.32388 -0.06142,-0.48378 0.353724,0.0457 0.702251,0.1304 1.057176,0.17407 0.701338,0.0864 1.394702,0.0784 2.096434,0.008 0.744056,-0.0745 1.433711,-0.21546 2.060598,-0.64854 0.243974,-0.16855 0.474672,-0.39133 0.603487,-0.66252 0.181421,-0.38195 0.175886,-0.89336 0.204447,-1.30803 0.0923,-1.34029 0.20588,-2.98599 -1.076708,-3.846 -0.499561,-0.33497 -1.208891,-0.39913 -1.791824,-0.45742 -0.987026,-0.0987 -1.971078,-0.0946 -2.956509,0.0338 -0.841146,0.10961 -1.595223,0.31468 -2.1065,1.0443 -0.493296,0.70396 -0.509564,1.52563 -0.509564,2.34729 0,1.37831 -0.05534,2.87744 0.595934,4.13911 0.504703,0.97774 1.498709,1.29589 2.52184,1.41832 0.473239,0.0566 0.96049,0.0849 1.434158,0.0172 0.328853,-0.0471 0.650325,-0.0999 0.966886,-0.20511 0.08957,-0.0298 0.266911,-0.0614 0.322027,-0.14486 0.04089,-0.0618 0.0099,-0.15812 0.0035,-0.22545 -0.01611,-0.16924 -0.02094,-0.34967 -0.02096,-0.51963 m -1.594723,-5.48298 c 0.214822,-0.25951 0.315898,-0.56088 0.60922,-0.75705 0.687899,-0.46006 1.692038,-0.11202 1.992096,0.63161 0.214571,0.5317 0.140174,1.15913 0.140174,1.72017 v 1.03925 c 0,0.0911 0.04009,0.30954 -0.01842,0.38339 -0.04193,0.053 -0.173018,0.0287 -0.232436,0.0287 h -0.698809 v -1.88142 c 0,-0.28413 0.04813,-0.63823 -0.09912,-0.89591 -0.234746,-0.4108 -0.875019,-0.36105 -1.092116,0.0358 -0.123368,0.22555 -0.116792,0.50369 -0.116792,0.75257 v 1.0751 h -0.931726 v -1.05718 c 0,-0.2555 0.0024,-0.53932 -0.121773,-0.77049 -0.21432,-0.39919 -0.857782,-0.44403 -1.090217,-0.0358 -0.147324,0.25871 -0.09604,0.61056 -0.09604,0.89591 v 1.88142 H 72.09042 v -1.98893 c 0,-0.4711 -0.01604,-0.95902 0.233201,-1.3797 0.585269,-0.98786 2.133584,-0.74836 2.472454,0.32253 z"       id="path2318" />  </g></svg></span> <a rel="me" href="https://mstdn.science/@gtn">Mastodon</a></li>
					<li><span style="fill: var(--hyperlink);"><svg  viewBox="0 0 64 57" width="1em" ><path style="fill-opacity:1;stroke:none;stroke-width:0.0179182" d="M13.873 3.805C21.21 9.332 29.103 20.537 32 26.55v15.882c0-.338-.13.044-.41.867-1.512 4.456-7.418 21.847-20.923 7.944-7.111-7.32-3.819-14.64 9.125-16.85-7.405 1.264-15.73-.825-18.014-9.015C1.12 23.022 0 8.51 0 6.55 0-3.268 8.579-.182 13.873 3.805ZM50.127 3.805C42.79 9.332 34.897 20.537 32 26.55v15.882c0-.338.13.044.41.867 1.512 4.456 7.418 21.847 20.923 7.944 7.111-7.32 3.819-14.64-9.125-16.85 7.405 1.264 15.73-.825 18.014-9.015C62.88 23.022 64 8.51 64 6.55c0-9.818-8.578-6.732-13.873-2.745Z"></path></svg></span><a rel="me" href="https://bsky.app/profile/galaxytraining.bsky.social"> Bluesky</a></li>
				</ul>

				<span style="font-size: 1.3em">Publications</span>
				<ul class="no-bullets">
					<li><a href="https://doi.org/10.1371/journal.pcbi.1010752">Hiltemann et al. 2023</a></li>
					<li><a href="https://doi.org/10.1016/j.cels.2018.05.012"> Batut et al. 2018</a></li>
					<li><a href="/training-material/faqs/gtn/gtn_citing.html">Citing Us</a></li>
				</ul>
			</div>
		</div>
	</div>
</footer>


        <script  async defer src='/training-material/assets/js/bundle.main.40d4e218.js'></script>

	
	<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
	
	

    </body>
</html>